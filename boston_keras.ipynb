{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "boston keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsgvnfYNAMHD",
        "outputId": "ee7e2fdc-d2d1-4a47-c94a-f387d6a5cb9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "#X, y = load_boston(return_X_y=True)\n",
        "from sklearn.datasets import load_boston\n",
        "boston_dataset = load_boston()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "y = raw_df.values[1::2, 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "aGrENc87A234",
        "outputId": "b9b724bb-15f8-4b9c-f40e-e2ddb9e6e34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fef6471a3d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://lib.stat.cmu.edu/datasets/boston\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mraw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\s+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mraw_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 506 and the array at index 1 has size 505"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features=boston_dataset.feature_names\n",
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZhSOY8xBse8",
        "outputId": "6e6b3465-ebf1-48b2-d6a6-6350cbecbec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
              "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X,columns = features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "-GHxALbSBZvf",
        "outputId": "f8576524-5879-4ad6-bc70-b6d2f338ea37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ae421e18-7dd7-4477-b17b-01e2efbc6fc9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>0.06263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.593</td>\n",
              "      <td>69.1</td>\n",
              "      <td>2.4786</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>391.99</td>\n",
              "      <td>9.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>0.04527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.120</td>\n",
              "      <td>76.7</td>\n",
              "      <td>2.2875</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>0.06076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.976</td>\n",
              "      <td>91.0</td>\n",
              "      <td>2.1675</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>0.10959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.794</td>\n",
              "      <td>89.3</td>\n",
              "      <td>2.3889</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>393.45</td>\n",
              "      <td>6.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.573</td>\n",
              "      <td>6.030</td>\n",
              "      <td>80.8</td>\n",
              "      <td>2.5050</td>\n",
              "      <td>1.0</td>\n",
              "      <td>273.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>7.88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae421e18-7dd7-4477-b17b-01e2efbc6fc9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae421e18-7dd7-4477-b17b-01e2efbc6fc9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae421e18-7dd7-4477-b17b-01e2efbc6fc9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
              "0    0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.90   4.98\n",
              "1    0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.90   9.14\n",
              "2    0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03\n",
              "3    0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94\n",
              "4    0.06905   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  396.90   5.33\n",
              "..       ...   ...    ...   ...    ...  ...  ...    ...      ...     ...    ...\n",
              "501  0.06263   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  391.99   9.67\n",
              "502  0.04527   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  396.90   9.08\n",
              "503  0.06076   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  396.90   5.64\n",
              "504  0.10959   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  393.45   6.48\n",
              "505  0.04741   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  396.90   7.88\n",
              "\n",
              "[506 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from numba import jit"
      ],
      "metadata": {
        "id": "kVBFadUeB2If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
      ],
      "metadata": {
        "id": "5gkuJTS6CNFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "  print(\"nb d'epoch enregistrés :\",len(history.history['mse']))\n",
        "# list all data in history\n",
        "  #print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.figure(figsize=(20,6))\n",
        "  plt.subplot(122)\n",
        "  #plt.vlines(x = len(history.history['accuracy']))\n",
        "\n",
        "  plt.plot(history.history['mse'])\n",
        "  plt.plot(history.history['val_mse'])\n",
        "  plt.title('model mse')\n",
        "  #plt.ylim(0, 1)\n",
        "  plt.ylabel('mse')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  # summarize history for loss\n",
        "  plt.subplot(121)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "OkOvdTh0CYn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define base model\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = tf.keras.Sequential()\n",
        "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
        "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "duf4t-C1Cwu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train , y_train_cat\n",
        "input_shape = (13)\n",
        "def initialize_model():\n",
        "  model =  tf.keras.Sequential([\n",
        "                             tf.keras.Input(shape=input_shape), \n",
        "                             layers.Dense(32,\n",
        "                                          #kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
        "                                          #bias_regularizer=tf.keras.regularizers.l2(1e-4),\n",
        "                                          #activity_regularizer=tf.keras.regularizers.l2(1e-5),\n",
        "                                          activation=\"relu\"),       \n",
        "\n",
        "                             layers.Dense(16,\n",
        "                                          #kernel_regularizer=tf.keras.regularizers.l1(0.001),\n",
        "                                          #bias_regularizer=tf.keras.regularizers.l2(1e-4),\n",
        "                                          #activity_regularizer=tf.keras.regularizers.l2(1e-5),\n",
        "                                          activation=\"relu\"),\n",
        "                              layers.Dropout(0.5),\n",
        "                             layers.Dense(1,)\n",
        "                             ])    \n",
        "    ### Model compilation\n",
        "  model.compile(\n",
        "      loss='mean_squared_error',\n",
        "      optimizer=\"adam\",\n",
        "      metrics=[\"mse\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "dtqV-Pz7Dm1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = initialize_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhP-E2i1DQXQ",
        "outputId": "22b586ec-9acf-4b0d-db75-27eaac047eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_13 (Dense)            (None, 32)                448       \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 993\n",
            "Trainable params: 993\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                            patience=100,\n",
        "                                            restore_best_weights=True\n",
        "                                            )\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=16,\n",
        "          epochs=10000,\n",
        "          validation_split=0.2,\n",
        "          callbacks=[callback]\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC2Ce9jOEWfH",
        "outputId": "084f1ccf-cd70-4f79-b5b8-a0f7a2960570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000\n",
            "23/23 [==============================] - 1s 10ms/step - loss: 37173.2656 - mse: 37173.2656 - val_loss: 1460.1334 - val_mse: 1460.1334\n",
            "Epoch 2/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 17615.7344 - mse: 17615.7344 - val_loss: 707.6991 - val_mse: 707.6991\n",
            "Epoch 3/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 7467.2856 - mse: 7467.2856 - val_loss: 478.1243 - val_mse: 478.1243\n",
            "Epoch 4/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 4892.2217 - mse: 4892.2217 - val_loss: 334.2678 - val_mse: 334.2678\n",
            "Epoch 5/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 2354.5244 - mse: 2354.5244 - val_loss: 208.2650 - val_mse: 208.2650\n",
            "Epoch 6/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 1415.8903 - mse: 1415.8903 - val_loss: 169.4312 - val_mse: 169.4312\n",
            "Epoch 7/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 775.5152 - mse: 775.5152 - val_loss: 153.5727 - val_mse: 153.5727\n",
            "Epoch 8/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 695.1838 - mse: 695.1838 - val_loss: 127.7674 - val_mse: 127.7674\n",
            "Epoch 9/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 439.7806 - mse: 439.7806 - val_loss: 113.4396 - val_mse: 113.4396\n",
            "Epoch 10/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 378.0900 - mse: 378.0900 - val_loss: 119.6318 - val_mse: 119.6318\n",
            "Epoch 11/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 280.4729 - mse: 280.4729 - val_loss: 125.2487 - val_mse: 125.2487\n",
            "Epoch 12/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 265.7837 - mse: 265.7837 - val_loss: 97.5579 - val_mse: 97.5579\n",
            "Epoch 13/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 260.0309 - mse: 260.0309 - val_loss: 108.8266 - val_mse: 108.8266\n",
            "Epoch 14/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 244.4389 - mse: 244.4389 - val_loss: 92.2502 - val_mse: 92.2502\n",
            "Epoch 15/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 212.3228 - mse: 212.3228 - val_loss: 96.7216 - val_mse: 96.7216\n",
            "Epoch 16/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 193.1022 - mse: 193.1022 - val_loss: 91.7316 - val_mse: 91.7316\n",
            "Epoch 17/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 208.0236 - mse: 208.0236 - val_loss: 96.1684 - val_mse: 96.1684\n",
            "Epoch 18/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 218.7405 - mse: 218.7405 - val_loss: 85.2367 - val_mse: 85.2367\n",
            "Epoch 19/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 203.4644 - mse: 203.4644 - val_loss: 91.0241 - val_mse: 91.0241\n",
            "Epoch 20/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 170.7707 - mse: 170.7707 - val_loss: 96.3484 - val_mse: 96.3484\n",
            "Epoch 21/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 196.3680 - mse: 196.3680 - val_loss: 79.8522 - val_mse: 79.8522\n",
            "Epoch 22/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 182.3174 - mse: 182.3174 - val_loss: 77.4640 - val_mse: 77.4640\n",
            "Epoch 23/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 170.3011 - mse: 170.3011 - val_loss: 85.0949 - val_mse: 85.0949\n",
            "Epoch 24/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 179.1329 - mse: 179.1329 - val_loss: 75.2790 - val_mse: 75.2790\n",
            "Epoch 25/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 160.2694 - mse: 160.2694 - val_loss: 79.3364 - val_mse: 79.3364\n",
            "Epoch 26/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 161.5453 - mse: 161.5453 - val_loss: 76.8343 - val_mse: 76.8343\n",
            "Epoch 27/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 182.7314 - mse: 182.7314 - val_loss: 77.1863 - val_mse: 77.1863\n",
            "Epoch 28/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 149.5803 - mse: 149.5803 - val_loss: 68.7191 - val_mse: 68.7191\n",
            "Epoch 29/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 154.7812 - mse: 154.7812 - val_loss: 71.2140 - val_mse: 71.2140\n",
            "Epoch 30/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 183.6118 - mse: 183.6118 - val_loss: 63.6328 - val_mse: 63.6328\n",
            "Epoch 31/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 164.6416 - mse: 164.6416 - val_loss: 84.8235 - val_mse: 84.8235\n",
            "Epoch 32/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 176.9093 - mse: 176.9093 - val_loss: 74.8214 - val_mse: 74.8214\n",
            "Epoch 33/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 144.5487 - mse: 144.5487 - val_loss: 57.5108 - val_mse: 57.5108\n",
            "Epoch 34/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 178.0962 - mse: 178.0962 - val_loss: 62.8032 - val_mse: 62.8032\n",
            "Epoch 35/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 161.3196 - mse: 161.3196 - val_loss: 68.9957 - val_mse: 68.9957\n",
            "Epoch 36/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 155.8396 - mse: 155.8396 - val_loss: 58.9152 - val_mse: 58.9152\n",
            "Epoch 37/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 163.5655 - mse: 163.5655 - val_loss: 62.8326 - val_mse: 62.8326\n",
            "Epoch 38/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 163.9405 - mse: 163.9405 - val_loss: 64.7124 - val_mse: 64.7124\n",
            "Epoch 39/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 157.3728 - mse: 157.3728 - val_loss: 60.0907 - val_mse: 60.0907\n",
            "Epoch 40/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 164.8561 - mse: 164.8561 - val_loss: 57.4706 - val_mse: 57.4706\n",
            "Epoch 41/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 178.7104 - mse: 178.7104 - val_loss: 63.8564 - val_mse: 63.8564\n",
            "Epoch 42/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 155.7805 - mse: 155.7805 - val_loss: 53.5831 - val_mse: 53.5831\n",
            "Epoch 43/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 148.8852 - mse: 148.8852 - val_loss: 61.7258 - val_mse: 61.7258\n",
            "Epoch 44/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 164.2309 - mse: 164.2309 - val_loss: 56.4064 - val_mse: 56.4064\n",
            "Epoch 45/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 161.2535 - mse: 161.2535 - val_loss: 60.3817 - val_mse: 60.3817\n",
            "Epoch 46/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 137.3997 - mse: 137.3997 - val_loss: 54.1784 - val_mse: 54.1784\n",
            "Epoch 47/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 144.5576 - mse: 144.5576 - val_loss: 66.4195 - val_mse: 66.4195\n",
            "Epoch 48/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 164.9245 - mse: 164.9245 - val_loss: 54.2096 - val_mse: 54.2096\n",
            "Epoch 49/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 128.8961 - mse: 128.8961 - val_loss: 48.3281 - val_mse: 48.3281\n",
            "Epoch 50/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 137.7672 - mse: 137.7672 - val_loss: 45.1675 - val_mse: 45.1675\n",
            "Epoch 51/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 131.5477 - mse: 131.5477 - val_loss: 56.4886 - val_mse: 56.4886\n",
            "Epoch 52/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 149.1768 - mse: 149.1768 - val_loss: 61.0539 - val_mse: 61.0539\n",
            "Epoch 53/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 136.7184 - mse: 136.7184 - val_loss: 58.5263 - val_mse: 58.5263\n",
            "Epoch 54/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 135.8204 - mse: 135.8204 - val_loss: 46.9811 - val_mse: 46.9811\n",
            "Epoch 55/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 126.8249 - mse: 126.8249 - val_loss: 54.4812 - val_mse: 54.4812\n",
            "Epoch 56/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 147.6091 - mse: 147.6091 - val_loss: 52.5976 - val_mse: 52.5976\n",
            "Epoch 57/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 131.9982 - mse: 131.9982 - val_loss: 51.4809 - val_mse: 51.4809\n",
            "Epoch 58/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 137.9764 - mse: 137.9764 - val_loss: 49.0248 - val_mse: 49.0248\n",
            "Epoch 59/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 148.3434 - mse: 148.3434 - val_loss: 64.7120 - val_mse: 64.7120\n",
            "Epoch 60/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 136.8317 - mse: 136.8317 - val_loss: 48.2283 - val_mse: 48.2283\n",
            "Epoch 61/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 148.7384 - mse: 148.7384 - val_loss: 45.0363 - val_mse: 45.0363\n",
            "Epoch 62/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 125.2025 - mse: 125.2025 - val_loss: 47.3721 - val_mse: 47.3721\n",
            "Epoch 63/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 144.2748 - mse: 144.2748 - val_loss: 62.4156 - val_mse: 62.4156\n",
            "Epoch 64/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 157.2371 - mse: 157.2371 - val_loss: 43.1006 - val_mse: 43.1006\n",
            "Epoch 65/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 132.0609 - mse: 132.0609 - val_loss: 40.6204 - val_mse: 40.6204\n",
            "Epoch 66/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 147.5083 - mse: 147.5083 - val_loss: 56.1957 - val_mse: 56.1957\n",
            "Epoch 67/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 128.8644 - mse: 128.8644 - val_loss: 43.9961 - val_mse: 43.9961\n",
            "Epoch 68/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 142.1637 - mse: 142.1637 - val_loss: 51.8662 - val_mse: 51.8662\n",
            "Epoch 69/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 131.3247 - mse: 131.3247 - val_loss: 44.7023 - val_mse: 44.7023\n",
            "Epoch 70/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 129.4957 - mse: 129.4957 - val_loss: 48.9009 - val_mse: 48.9009\n",
            "Epoch 71/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 156.2541 - mse: 156.2541 - val_loss: 51.6300 - val_mse: 51.6300\n",
            "Epoch 72/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 135.3222 - mse: 135.3222 - val_loss: 42.3161 - val_mse: 42.3161\n",
            "Epoch 73/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 125.3245 - mse: 125.3245 - val_loss: 45.6643 - val_mse: 45.6643\n",
            "Epoch 74/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 123.0608 - mse: 123.0608 - val_loss: 60.4594 - val_mse: 60.4594\n",
            "Epoch 75/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 139.5479 - mse: 139.5479 - val_loss: 53.5753 - val_mse: 53.5753\n",
            "Epoch 76/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 140.0573 - mse: 140.0573 - val_loss: 56.3075 - val_mse: 56.3075\n",
            "Epoch 77/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 121.1927 - mse: 121.1927 - val_loss: 45.5663 - val_mse: 45.5663\n",
            "Epoch 78/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 119.3176 - mse: 119.3176 - val_loss: 49.3857 - val_mse: 49.3857\n",
            "Epoch 79/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 115.6840 - mse: 115.6840 - val_loss: 44.5677 - val_mse: 44.5677\n",
            "Epoch 80/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 138.8579 - mse: 138.8579 - val_loss: 39.9957 - val_mse: 39.9957\n",
            "Epoch 81/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 121.9376 - mse: 121.9376 - val_loss: 43.8089 - val_mse: 43.8089\n",
            "Epoch 82/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 111.7272 - mse: 111.7272 - val_loss: 46.6522 - val_mse: 46.6522\n",
            "Epoch 83/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 121.7753 - mse: 121.7753 - val_loss: 68.6018 - val_mse: 68.6018\n",
            "Epoch 84/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 130.4350 - mse: 130.4350 - val_loss: 67.7958 - val_mse: 67.7958\n",
            "Epoch 85/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 105.5280 - mse: 105.5280 - val_loss: 54.9489 - val_mse: 54.9489\n",
            "Epoch 86/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 130.3676 - mse: 130.3676 - val_loss: 71.2293 - val_mse: 71.2293\n",
            "Epoch 87/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 114.1475 - mse: 114.1475 - val_loss: 50.3755 - val_mse: 50.3755\n",
            "Epoch 88/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 112.1774 - mse: 112.1774 - val_loss: 49.7234 - val_mse: 49.7234\n",
            "Epoch 89/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 115.4632 - mse: 115.4632 - val_loss: 37.7421 - val_mse: 37.7421\n",
            "Epoch 90/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 107.9596 - mse: 107.9596 - val_loss: 35.3266 - val_mse: 35.3266\n",
            "Epoch 91/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 119.3974 - mse: 119.3974 - val_loss: 39.8442 - val_mse: 39.8442\n",
            "Epoch 92/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 106.9904 - mse: 106.9904 - val_loss: 47.2469 - val_mse: 47.2469\n",
            "Epoch 93/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 115.6929 - mse: 115.6929 - val_loss: 40.2733 - val_mse: 40.2733\n",
            "Epoch 94/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 118.7308 - mse: 118.7308 - val_loss: 34.3623 - val_mse: 34.3623\n",
            "Epoch 95/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 111.8958 - mse: 111.8958 - val_loss: 53.6539 - val_mse: 53.6539\n",
            "Epoch 96/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 110.9989 - mse: 110.9989 - val_loss: 38.2726 - val_mse: 38.2726\n",
            "Epoch 97/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 101.8501 - mse: 101.8501 - val_loss: 44.6756 - val_mse: 44.6756\n",
            "Epoch 98/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 115.2950 - mse: 115.2950 - val_loss: 35.4746 - val_mse: 35.4746\n",
            "Epoch 99/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 125.0071 - mse: 125.0071 - val_loss: 34.7628 - val_mse: 34.7628\n",
            "Epoch 100/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 116.4637 - mse: 116.4637 - val_loss: 36.7621 - val_mse: 36.7621\n",
            "Epoch 101/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 124.3109 - mse: 124.3109 - val_loss: 35.5909 - val_mse: 35.5909\n",
            "Epoch 102/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 97.3010 - mse: 97.3010 - val_loss: 52.2217 - val_mse: 52.2217\n",
            "Epoch 103/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 112.2990 - mse: 112.2990 - val_loss: 42.8975 - val_mse: 42.8975\n",
            "Epoch 104/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 106.9361 - mse: 106.9361 - val_loss: 66.9886 - val_mse: 66.9886\n",
            "Epoch 105/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 108.2111 - mse: 108.2111 - val_loss: 37.2728 - val_mse: 37.2728\n",
            "Epoch 106/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 136.3502 - mse: 136.3502 - val_loss: 34.2209 - val_mse: 34.2209\n",
            "Epoch 107/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 106.4106 - mse: 106.4106 - val_loss: 35.3675 - val_mse: 35.3675\n",
            "Epoch 108/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 99.6507 - mse: 99.6507 - val_loss: 35.1996 - val_mse: 35.1996\n",
            "Epoch 109/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 110.1466 - mse: 110.1466 - val_loss: 35.5565 - val_mse: 35.5565\n",
            "Epoch 110/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 105.7707 - mse: 105.7707 - val_loss: 34.0668 - val_mse: 34.0668\n",
            "Epoch 111/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 102.6159 - mse: 102.6159 - val_loss: 41.1761 - val_mse: 41.1761\n",
            "Epoch 112/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 107.9349 - mse: 107.9349 - val_loss: 53.8286 - val_mse: 53.8286\n",
            "Epoch 113/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 121.7347 - mse: 121.7347 - val_loss: 46.7834 - val_mse: 46.7834\n",
            "Epoch 114/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 112.8438 - mse: 112.8438 - val_loss: 32.9614 - val_mse: 32.9614\n",
            "Epoch 115/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 111.2387 - mse: 111.2387 - val_loss: 30.7963 - val_mse: 30.7963\n",
            "Epoch 116/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 106.7747 - mse: 106.7747 - val_loss: 30.9861 - val_mse: 30.9861\n",
            "Epoch 117/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 108.4192 - mse: 108.4192 - val_loss: 37.6476 - val_mse: 37.6476\n",
            "Epoch 118/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 108.3055 - mse: 108.3055 - val_loss: 37.7176 - val_mse: 37.7176\n",
            "Epoch 119/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 101.8174 - mse: 101.8174 - val_loss: 40.9668 - val_mse: 40.9668\n",
            "Epoch 120/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 103.3490 - mse: 103.3490 - val_loss: 41.3322 - val_mse: 41.3322\n",
            "Epoch 121/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 102.8955 - mse: 102.8955 - val_loss: 37.1868 - val_mse: 37.1868\n",
            "Epoch 122/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 95.6333 - mse: 95.6333 - val_loss: 30.5498 - val_mse: 30.5498\n",
            "Epoch 123/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 106.4321 - mse: 106.4321 - val_loss: 40.5000 - val_mse: 40.5000\n",
            "Epoch 124/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 109.3813 - mse: 109.3813 - val_loss: 43.5538 - val_mse: 43.5538\n",
            "Epoch 125/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 111.1332 - mse: 111.1332 - val_loss: 31.4931 - val_mse: 31.4931\n",
            "Epoch 126/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 109.5123 - mse: 109.5123 - val_loss: 42.4970 - val_mse: 42.4970\n",
            "Epoch 127/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 109.8762 - mse: 109.8762 - val_loss: 34.6805 - val_mse: 34.6805\n",
            "Epoch 128/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 107.9779 - mse: 107.9779 - val_loss: 32.7187 - val_mse: 32.7187\n",
            "Epoch 129/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 102.6130 - mse: 102.6130 - val_loss: 32.4172 - val_mse: 32.4172\n",
            "Epoch 130/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 119.0845 - mse: 119.0845 - val_loss: 31.9969 - val_mse: 31.9969\n",
            "Epoch 131/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 88.1009 - mse: 88.1009 - val_loss: 30.0196 - val_mse: 30.0196\n",
            "Epoch 132/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 101.1759 - mse: 101.1759 - val_loss: 40.5193 - val_mse: 40.5193\n",
            "Epoch 133/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 103.7371 - mse: 103.7371 - val_loss: 30.8173 - val_mse: 30.8173\n",
            "Epoch 134/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 100.7335 - mse: 100.7335 - val_loss: 32.5958 - val_mse: 32.5958\n",
            "Epoch 135/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 101.5326 - mse: 101.5326 - val_loss: 31.2065 - val_mse: 31.2065\n",
            "Epoch 136/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 97.0803 - mse: 97.0803 - val_loss: 44.5911 - val_mse: 44.5911\n",
            "Epoch 137/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 102.7994 - mse: 102.7994 - val_loss: 41.9768 - val_mse: 41.9768\n",
            "Epoch 138/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 100.7753 - mse: 100.7753 - val_loss: 33.3038 - val_mse: 33.3038\n",
            "Epoch 139/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 114.5781 - mse: 114.5781 - val_loss: 41.2363 - val_mse: 41.2363\n",
            "Epoch 140/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 95.1094 - mse: 95.1094 - val_loss: 30.8127 - val_mse: 30.8127\n",
            "Epoch 141/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 101.3518 - mse: 101.3518 - val_loss: 32.4098 - val_mse: 32.4098\n",
            "Epoch 142/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 120.3180 - mse: 120.3180 - val_loss: 47.9963 - val_mse: 47.9963\n",
            "Epoch 143/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 106.2142 - mse: 106.2142 - val_loss: 32.3550 - val_mse: 32.3550\n",
            "Epoch 144/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 101.1189 - mse: 101.1189 - val_loss: 39.3139 - val_mse: 39.3139\n",
            "Epoch 145/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 95.3886 - mse: 95.3886 - val_loss: 29.5596 - val_mse: 29.5596\n",
            "Epoch 146/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 114.3387 - mse: 114.3387 - val_loss: 45.8349 - val_mse: 45.8349\n",
            "Epoch 147/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 98.7786 - mse: 98.7786 - val_loss: 30.2935 - val_mse: 30.2935\n",
            "Epoch 148/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 100.6850 - mse: 100.6850 - val_loss: 31.4163 - val_mse: 31.4163\n",
            "Epoch 149/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 106.7152 - mse: 106.7152 - val_loss: 30.0000 - val_mse: 30.0000\n",
            "Epoch 150/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 99.6665 - mse: 99.6665 - val_loss: 31.2404 - val_mse: 31.2404\n",
            "Epoch 151/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 96.5094 - mse: 96.5094 - val_loss: 48.9119 - val_mse: 48.9119\n",
            "Epoch 152/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 116.9760 - mse: 116.9760 - val_loss: 29.4671 - val_mse: 29.4671\n",
            "Epoch 153/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 97.6042 - mse: 97.6042 - val_loss: 29.4328 - val_mse: 29.4328\n",
            "Epoch 154/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 108.2629 - mse: 108.2629 - val_loss: 30.2941 - val_mse: 30.2941\n",
            "Epoch 155/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 109.7762 - mse: 109.7762 - val_loss: 36.0827 - val_mse: 36.0827\n",
            "Epoch 156/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 112.0032 - mse: 112.0032 - val_loss: 37.9073 - val_mse: 37.9073\n",
            "Epoch 157/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 112.2676 - mse: 112.2676 - val_loss: 31.9866 - val_mse: 31.9866\n",
            "Epoch 158/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 86.5519 - mse: 86.5519 - val_loss: 30.9851 - val_mse: 30.9851\n",
            "Epoch 159/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 95.6073 - mse: 95.6073 - val_loss: 35.9024 - val_mse: 35.9024\n",
            "Epoch 160/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 91.5221 - mse: 91.5221 - val_loss: 33.7406 - val_mse: 33.7406\n",
            "Epoch 161/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 108.8984 - mse: 108.8984 - val_loss: 37.6896 - val_mse: 37.6896\n",
            "Epoch 162/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 103.5896 - mse: 103.5896 - val_loss: 30.0300 - val_mse: 30.0300\n",
            "Epoch 163/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 98.3945 - mse: 98.3945 - val_loss: 43.9685 - val_mse: 43.9685\n",
            "Epoch 164/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 90.4167 - mse: 90.4167 - val_loss: 29.3992 - val_mse: 29.3992\n",
            "Epoch 165/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 92.7342 - mse: 92.7342 - val_loss: 38.0594 - val_mse: 38.0594\n",
            "Epoch 166/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 94.5454 - mse: 94.5454 - val_loss: 29.1780 - val_mse: 29.1780\n",
            "Epoch 167/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 98.7416 - mse: 98.7416 - val_loss: 30.8905 - val_mse: 30.8905\n",
            "Epoch 168/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 97.7702 - mse: 97.7702 - val_loss: 49.4853 - val_mse: 49.4853\n",
            "Epoch 169/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 102.8959 - mse: 102.8959 - val_loss: 30.6205 - val_mse: 30.6205\n",
            "Epoch 170/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 97.3095 - mse: 97.3095 - val_loss: 31.5380 - val_mse: 31.5380\n",
            "Epoch 171/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 98.4110 - mse: 98.4110 - val_loss: 34.6283 - val_mse: 34.6283\n",
            "Epoch 172/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 99.6866 - mse: 99.6866 - val_loss: 56.3681 - val_mse: 56.3681\n",
            "Epoch 173/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 92.8875 - mse: 92.8875 - val_loss: 34.2968 - val_mse: 34.2968\n",
            "Epoch 174/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 99.4556 - mse: 99.4556 - val_loss: 29.7995 - val_mse: 29.7995\n",
            "Epoch 175/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 102.6897 - mse: 102.6897 - val_loss: 37.4710 - val_mse: 37.4710\n",
            "Epoch 176/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 100.8323 - mse: 100.8323 - val_loss: 32.4485 - val_mse: 32.4485\n",
            "Epoch 177/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 91.2338 - mse: 91.2338 - val_loss: 33.1243 - val_mse: 33.1243\n",
            "Epoch 178/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 87.8858 - mse: 87.8858 - val_loss: 28.3510 - val_mse: 28.3510\n",
            "Epoch 179/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 106.3395 - mse: 106.3395 - val_loss: 29.8368 - val_mse: 29.8368\n",
            "Epoch 180/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 90.9326 - mse: 90.9326 - val_loss: 34.9148 - val_mse: 34.9148\n",
            "Epoch 181/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 92.7197 - mse: 92.7197 - val_loss: 28.0668 - val_mse: 28.0668\n",
            "Epoch 182/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 110.1572 - mse: 110.1572 - val_loss: 32.2983 - val_mse: 32.2983\n",
            "Epoch 183/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 98.5132 - mse: 98.5132 - val_loss: 27.9210 - val_mse: 27.9210\n",
            "Epoch 184/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 96.1250 - mse: 96.1250 - val_loss: 28.1963 - val_mse: 28.1963\n",
            "Epoch 185/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 93.8593 - mse: 93.8593 - val_loss: 30.3380 - val_mse: 30.3380\n",
            "Epoch 186/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 96.9709 - mse: 96.9709 - val_loss: 29.5894 - val_mse: 29.5894\n",
            "Epoch 187/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 76.2427 - mse: 76.2427 - val_loss: 32.0100 - val_mse: 32.0100\n",
            "Epoch 188/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 98.0188 - mse: 98.0188 - val_loss: 28.1815 - val_mse: 28.1815\n",
            "Epoch 189/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 105.3126 - mse: 105.3126 - val_loss: 48.5700 - val_mse: 48.5700\n",
            "Epoch 190/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 102.2125 - mse: 102.2125 - val_loss: 35.1660 - val_mse: 35.1660\n",
            "Epoch 191/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 94.2563 - mse: 94.2563 - val_loss: 29.4204 - val_mse: 29.4204\n",
            "Epoch 192/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 93.5364 - mse: 93.5364 - val_loss: 29.8139 - val_mse: 29.8139\n",
            "Epoch 193/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 84.4439 - mse: 84.4439 - val_loss: 33.0799 - val_mse: 33.0799\n",
            "Epoch 194/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 81.8559 - mse: 81.8559 - val_loss: 43.2325 - val_mse: 43.2325\n",
            "Epoch 195/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 85.6158 - mse: 85.6158 - val_loss: 31.9883 - val_mse: 31.9883\n",
            "Epoch 196/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 101.1930 - mse: 101.1930 - val_loss: 27.5458 - val_mse: 27.5458\n",
            "Epoch 197/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 101.3332 - mse: 101.3332 - val_loss: 29.2373 - val_mse: 29.2373\n",
            "Epoch 198/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 95.4413 - mse: 95.4413 - val_loss: 51.4724 - val_mse: 51.4724\n",
            "Epoch 199/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 97.6224 - mse: 97.6224 - val_loss: 33.7421 - val_mse: 33.7421\n",
            "Epoch 200/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 98.1750 - mse: 98.1750 - val_loss: 29.7023 - val_mse: 29.7023\n",
            "Epoch 201/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 93.9574 - mse: 93.9574 - val_loss: 51.9433 - val_mse: 51.9433\n",
            "Epoch 202/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 83.3512 - mse: 83.3512 - val_loss: 36.6515 - val_mse: 36.6515\n",
            "Epoch 203/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 105.3575 - mse: 105.3575 - val_loss: 32.0139 - val_mse: 32.0139\n",
            "Epoch 204/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 99.1961 - mse: 99.1961 - val_loss: 34.7954 - val_mse: 34.7954\n",
            "Epoch 205/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 81.0813 - mse: 81.0813 - val_loss: 39.6071 - val_mse: 39.6071\n",
            "Epoch 206/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 89.4355 - mse: 89.4355 - val_loss: 35.3134 - val_mse: 35.3134\n",
            "Epoch 207/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 85.3985 - mse: 85.3985 - val_loss: 32.8501 - val_mse: 32.8501\n",
            "Epoch 208/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 105.4433 - mse: 105.4433 - val_loss: 35.6434 - val_mse: 35.6434\n",
            "Epoch 209/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 99.6953 - mse: 99.6953 - val_loss: 51.8665 - val_mse: 51.8665\n",
            "Epoch 210/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 86.8016 - mse: 86.8016 - val_loss: 35.5390 - val_mse: 35.5390\n",
            "Epoch 211/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 83.9142 - mse: 83.9142 - val_loss: 35.3913 - val_mse: 35.3913\n",
            "Epoch 212/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 90.3634 - mse: 90.3634 - val_loss: 33.6059 - val_mse: 33.6059\n",
            "Epoch 213/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 97.8787 - mse: 97.8787 - val_loss: 41.3002 - val_mse: 41.3002\n",
            "Epoch 214/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 105.5386 - mse: 105.5386 - val_loss: 29.5102 - val_mse: 29.5102\n",
            "Epoch 215/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 93.8556 - mse: 93.8556 - val_loss: 29.6850 - val_mse: 29.6850\n",
            "Epoch 216/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 109.9609 - mse: 109.9609 - val_loss: 45.7180 - val_mse: 45.7180\n",
            "Epoch 217/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 89.2317 - mse: 89.2317 - val_loss: 42.0352 - val_mse: 42.0352\n",
            "Epoch 218/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 97.0748 - mse: 97.0748 - val_loss: 27.4015 - val_mse: 27.4015\n",
            "Epoch 219/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 85.3515 - mse: 85.3515 - val_loss: 32.7183 - val_mse: 32.7183\n",
            "Epoch 220/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 94.5999 - mse: 94.5999 - val_loss: 27.6100 - val_mse: 27.6100\n",
            "Epoch 221/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 102.7027 - mse: 102.7027 - val_loss: 56.8286 - val_mse: 56.8286\n",
            "Epoch 222/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 105.4181 - mse: 105.4181 - val_loss: 31.2977 - val_mse: 31.2977\n",
            "Epoch 223/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 93.6466 - mse: 93.6466 - val_loss: 38.4984 - val_mse: 38.4984\n",
            "Epoch 224/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 95.2792 - mse: 95.2792 - val_loss: 35.7259 - val_mse: 35.7259\n",
            "Epoch 225/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 84.8988 - mse: 84.8988 - val_loss: 28.2346 - val_mse: 28.2346\n",
            "Epoch 226/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 108.9506 - mse: 108.9506 - val_loss: 35.1587 - val_mse: 35.1587\n",
            "Epoch 227/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 74.4440 - mse: 74.4440 - val_loss: 43.1750 - val_mse: 43.1750\n",
            "Epoch 228/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 90.0654 - mse: 90.0654 - val_loss: 26.4823 - val_mse: 26.4823\n",
            "Epoch 229/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 86.1075 - mse: 86.1075 - val_loss: 27.9873 - val_mse: 27.9873\n",
            "Epoch 230/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 84.0900 - mse: 84.0900 - val_loss: 26.7583 - val_mse: 26.7583\n",
            "Epoch 231/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 92.7422 - mse: 92.7422 - val_loss: 31.6120 - val_mse: 31.6120\n",
            "Epoch 232/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 93.5790 - mse: 93.5790 - val_loss: 29.6597 - val_mse: 29.6597\n",
            "Epoch 233/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 79.5630 - mse: 79.5630 - val_loss: 52.6920 - val_mse: 52.6920\n",
            "Epoch 234/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 87.9513 - mse: 87.9513 - val_loss: 28.1068 - val_mse: 28.1068\n",
            "Epoch 235/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 82.1987 - mse: 82.1987 - val_loss: 27.7895 - val_mse: 27.7895\n",
            "Epoch 236/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 76.3976 - mse: 76.3976 - val_loss: 26.7372 - val_mse: 26.7372\n",
            "Epoch 237/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 91.4477 - mse: 91.4477 - val_loss: 29.7034 - val_mse: 29.7034\n",
            "Epoch 238/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 94.2921 - mse: 94.2921 - val_loss: 32.7120 - val_mse: 32.7120\n",
            "Epoch 239/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 86.5477 - mse: 86.5477 - val_loss: 32.7079 - val_mse: 32.7079\n",
            "Epoch 240/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 87.9805 - mse: 87.9805 - val_loss: 28.8937 - val_mse: 28.8937\n",
            "Epoch 241/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 99.7440 - mse: 99.7440 - val_loss: 26.2324 - val_mse: 26.2324\n",
            "Epoch 242/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 97.7202 - mse: 97.7202 - val_loss: 38.2694 - val_mse: 38.2694\n",
            "Epoch 243/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 96.0613 - mse: 96.0613 - val_loss: 29.8465 - val_mse: 29.8465\n",
            "Epoch 244/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 105.7208 - mse: 105.7208 - val_loss: 26.7928 - val_mse: 26.7928\n",
            "Epoch 245/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 102.5976 - mse: 102.5976 - val_loss: 34.5108 - val_mse: 34.5108\n",
            "Epoch 246/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 97.0500 - mse: 97.0500 - val_loss: 26.7128 - val_mse: 26.7128\n",
            "Epoch 247/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 99.6734 - mse: 99.6734 - val_loss: 35.1505 - val_mse: 35.1505\n",
            "Epoch 248/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 85.5427 - mse: 85.5427 - val_loss: 31.9684 - val_mse: 31.9684\n",
            "Epoch 249/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 96.1870 - mse: 96.1870 - val_loss: 27.7836 - val_mse: 27.7836\n",
            "Epoch 250/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 78.4798 - mse: 78.4798 - val_loss: 26.4156 - val_mse: 26.4156\n",
            "Epoch 251/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 97.0841 - mse: 97.0841 - val_loss: 26.2768 - val_mse: 26.2768\n",
            "Epoch 252/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 98.2814 - mse: 98.2814 - val_loss: 32.3563 - val_mse: 32.3563\n",
            "Epoch 253/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 94.9689 - mse: 94.9689 - val_loss: 29.5704 - val_mse: 29.5704\n",
            "Epoch 254/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 80.1800 - mse: 80.1800 - val_loss: 33.5343 - val_mse: 33.5343\n",
            "Epoch 255/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 96.5977 - mse: 96.5977 - val_loss: 26.3290 - val_mse: 26.3290\n",
            "Epoch 256/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 94.7919 - mse: 94.7919 - val_loss: 31.0884 - val_mse: 31.0884\n",
            "Epoch 257/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 94.1885 - mse: 94.1885 - val_loss: 50.1998 - val_mse: 50.1998\n",
            "Epoch 258/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 78.6191 - mse: 78.6191 - val_loss: 25.3576 - val_mse: 25.3576\n",
            "Epoch 259/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 96.5355 - mse: 96.5355 - val_loss: 31.8650 - val_mse: 31.8650\n",
            "Epoch 260/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 81.7198 - mse: 81.7198 - val_loss: 51.0695 - val_mse: 51.0695\n",
            "Epoch 261/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 95.1121 - mse: 95.1121 - val_loss: 27.8201 - val_mse: 27.8201\n",
            "Epoch 262/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 81.4480 - mse: 81.4480 - val_loss: 29.4770 - val_mse: 29.4770\n",
            "Epoch 263/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 97.1015 - mse: 97.1015 - val_loss: 33.8359 - val_mse: 33.8359\n",
            "Epoch 264/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 89.0380 - mse: 89.0380 - val_loss: 52.4757 - val_mse: 52.4757\n",
            "Epoch 265/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 95.6080 - mse: 95.6080 - val_loss: 26.0799 - val_mse: 26.0799\n",
            "Epoch 266/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 81.3660 - mse: 81.3660 - val_loss: 32.6113 - val_mse: 32.6113\n",
            "Epoch 267/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 96.8931 - mse: 96.8931 - val_loss: 27.2196 - val_mse: 27.2196\n",
            "Epoch 268/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 84.1225 - mse: 84.1225 - val_loss: 27.1565 - val_mse: 27.1565\n",
            "Epoch 269/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 84.9565 - mse: 84.9565 - val_loss: 31.7891 - val_mse: 31.7891\n",
            "Epoch 270/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 83.7722 - mse: 83.7722 - val_loss: 44.9766 - val_mse: 44.9766\n",
            "Epoch 271/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 85.6939 - mse: 85.6939 - val_loss: 27.0542 - val_mse: 27.0542\n",
            "Epoch 272/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 102.0185 - mse: 102.0185 - val_loss: 32.5619 - val_mse: 32.5619\n",
            "Epoch 273/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 95.7990 - mse: 95.7990 - val_loss: 30.7583 - val_mse: 30.7583\n",
            "Epoch 274/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 78.5489 - mse: 78.5489 - val_loss: 29.6947 - val_mse: 29.6947\n",
            "Epoch 275/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 85.8093 - mse: 85.8093 - val_loss: 49.5490 - val_mse: 49.5490\n",
            "Epoch 276/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 93.0796 - mse: 93.0796 - val_loss: 26.6031 - val_mse: 26.6031\n",
            "Epoch 277/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 73.7082 - mse: 73.7082 - val_loss: 36.5966 - val_mse: 36.5966\n",
            "Epoch 278/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 86.2703 - mse: 86.2703 - val_loss: 26.7427 - val_mse: 26.7427\n",
            "Epoch 279/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 80.6920 - mse: 80.6920 - val_loss: 31.1570 - val_mse: 31.1570\n",
            "Epoch 280/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 79.7471 - mse: 79.7471 - val_loss: 26.5260 - val_mse: 26.5260\n",
            "Epoch 281/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 93.3765 - mse: 93.3765 - val_loss: 24.6129 - val_mse: 24.6129\n",
            "Epoch 282/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 82.9388 - mse: 82.9388 - val_loss: 27.8025 - val_mse: 27.8025\n",
            "Epoch 283/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 88.2109 - mse: 88.2109 - val_loss: 42.6941 - val_mse: 42.6941\n",
            "Epoch 284/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 87.2234 - mse: 87.2234 - val_loss: 28.4532 - val_mse: 28.4532\n",
            "Epoch 285/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 92.1606 - mse: 92.1606 - val_loss: 26.6856 - val_mse: 26.6856\n",
            "Epoch 286/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 80.6790 - mse: 80.6790 - val_loss: 30.9541 - val_mse: 30.9541\n",
            "Epoch 287/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 78.5908 - mse: 78.5908 - val_loss: 25.8515 - val_mse: 25.8515\n",
            "Epoch 288/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 85.1698 - mse: 85.1698 - val_loss: 27.5204 - val_mse: 27.5204\n",
            "Epoch 289/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 80.6368 - mse: 80.6368 - val_loss: 36.6237 - val_mse: 36.6237\n",
            "Epoch 290/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 76.6411 - mse: 76.6411 - val_loss: 31.9930 - val_mse: 31.9930\n",
            "Epoch 291/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 88.2481 - mse: 88.2481 - val_loss: 26.1007 - val_mse: 26.1007\n",
            "Epoch 292/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 90.8482 - mse: 90.8482 - val_loss: 26.9206 - val_mse: 26.9206\n",
            "Epoch 293/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 88.3001 - mse: 88.3001 - val_loss: 28.1269 - val_mse: 28.1269\n",
            "Epoch 294/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 72.3958 - mse: 72.3958 - val_loss: 25.9646 - val_mse: 25.9646\n",
            "Epoch 295/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 94.0751 - mse: 94.0751 - val_loss: 33.8899 - val_mse: 33.8899\n",
            "Epoch 296/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 85.7407 - mse: 85.7407 - val_loss: 26.1163 - val_mse: 26.1163\n",
            "Epoch 297/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 79.4039 - mse: 79.4039 - val_loss: 34.8864 - val_mse: 34.8864\n",
            "Epoch 298/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 84.3402 - mse: 84.3402 - val_loss: 29.3305 - val_mse: 29.3305\n",
            "Epoch 299/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 79.5477 - mse: 79.5477 - val_loss: 33.2011 - val_mse: 33.2011\n",
            "Epoch 300/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 83.4394 - mse: 83.4394 - val_loss: 48.8931 - val_mse: 48.8931\n",
            "Epoch 301/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 86.2706 - mse: 86.2706 - val_loss: 26.2361 - val_mse: 26.2361\n",
            "Epoch 302/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 87.3504 - mse: 87.3504 - val_loss: 26.3901 - val_mse: 26.3901\n",
            "Epoch 303/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 83.7318 - mse: 83.7318 - val_loss: 31.5017 - val_mse: 31.5017\n",
            "Epoch 304/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 81.0959 - mse: 81.0959 - val_loss: 33.7989 - val_mse: 33.7989\n",
            "Epoch 305/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 71.5653 - mse: 71.5653 - val_loss: 27.0220 - val_mse: 27.0220\n",
            "Epoch 306/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 89.1673 - mse: 89.1673 - val_loss: 25.8707 - val_mse: 25.8707\n",
            "Epoch 307/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 74.4149 - mse: 74.4149 - val_loss: 24.8654 - val_mse: 24.8654\n",
            "Epoch 308/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 89.2816 - mse: 89.2816 - val_loss: 24.5443 - val_mse: 24.5443\n",
            "Epoch 309/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 91.4245 - mse: 91.4245 - val_loss: 27.2645 - val_mse: 27.2645\n",
            "Epoch 310/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 79.6022 - mse: 79.6022 - val_loss: 25.3703 - val_mse: 25.3703\n",
            "Epoch 311/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 81.5425 - mse: 81.5425 - val_loss: 26.3022 - val_mse: 26.3022\n",
            "Epoch 312/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 73.0847 - mse: 73.0847 - val_loss: 25.6531 - val_mse: 25.6531\n",
            "Epoch 313/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 76.0252 - mse: 76.0252 - val_loss: 28.6362 - val_mse: 28.6362\n",
            "Epoch 314/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 83.8061 - mse: 83.8061 - val_loss: 47.0380 - val_mse: 47.0380\n",
            "Epoch 315/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 92.8575 - mse: 92.8575 - val_loss: 32.3525 - val_mse: 32.3525\n",
            "Epoch 316/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 90.8663 - mse: 90.8663 - val_loss: 27.5968 - val_mse: 27.5968\n",
            "Epoch 317/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 89.5885 - mse: 89.5885 - val_loss: 25.7790 - val_mse: 25.7790\n",
            "Epoch 318/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 84.2335 - mse: 84.2335 - val_loss: 27.7284 - val_mse: 27.7284\n",
            "Epoch 319/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 85.3494 - mse: 85.3494 - val_loss: 27.1636 - val_mse: 27.1636\n",
            "Epoch 320/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 82.4409 - mse: 82.4409 - val_loss: 30.5426 - val_mse: 30.5426\n",
            "Epoch 321/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 89.4529 - mse: 89.4529 - val_loss: 33.6189 - val_mse: 33.6189\n",
            "Epoch 322/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 85.1885 - mse: 85.1885 - val_loss: 25.7350 - val_mse: 25.7350\n",
            "Epoch 323/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 75.7753 - mse: 75.7753 - val_loss: 29.0362 - val_mse: 29.0362\n",
            "Epoch 324/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 80.4787 - mse: 80.4787 - val_loss: 30.3584 - val_mse: 30.3584\n",
            "Epoch 325/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 79.8610 - mse: 79.8610 - val_loss: 25.3158 - val_mse: 25.3158\n",
            "Epoch 326/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 82.6822 - mse: 82.6822 - val_loss: 51.4354 - val_mse: 51.4354\n",
            "Epoch 327/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 70.2989 - mse: 70.2989 - val_loss: 30.2701 - val_mse: 30.2701\n",
            "Epoch 328/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 90.3925 - mse: 90.3925 - val_loss: 28.3437 - val_mse: 28.3437\n",
            "Epoch 329/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 87.4072 - mse: 87.4072 - val_loss: 24.0373 - val_mse: 24.0373\n",
            "Epoch 330/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 79.5592 - mse: 79.5592 - val_loss: 31.7594 - val_mse: 31.7594\n",
            "Epoch 331/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 81.4207 - mse: 81.4207 - val_loss: 33.0855 - val_mse: 33.0855\n",
            "Epoch 332/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 72.2974 - mse: 72.2974 - val_loss: 27.0009 - val_mse: 27.0009\n",
            "Epoch 333/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 69.5679 - mse: 69.5679 - val_loss: 26.1976 - val_mse: 26.1976\n",
            "Epoch 334/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 90.0480 - mse: 90.0480 - val_loss: 26.0795 - val_mse: 26.0795\n",
            "Epoch 335/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 82.5584 - mse: 82.5584 - val_loss: 28.0345 - val_mse: 28.0345\n",
            "Epoch 336/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 72.5212 - mse: 72.5212 - val_loss: 40.8346 - val_mse: 40.8346\n",
            "Epoch 337/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 77.2847 - mse: 77.2847 - val_loss: 41.1950 - val_mse: 41.1950\n",
            "Epoch 338/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 65.9073 - mse: 65.9073 - val_loss: 25.5187 - val_mse: 25.5187\n",
            "Epoch 339/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 77.5466 - mse: 77.5466 - val_loss: 24.8599 - val_mse: 24.8599\n",
            "Epoch 340/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 69.5990 - mse: 69.5990 - val_loss: 31.6383 - val_mse: 31.6383\n",
            "Epoch 341/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 89.8463 - mse: 89.8463 - val_loss: 33.9137 - val_mse: 33.9137\n",
            "Epoch 342/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 73.9672 - mse: 73.9672 - val_loss: 25.2240 - val_mse: 25.2240\n",
            "Epoch 343/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 86.1015 - mse: 86.1015 - val_loss: 28.4828 - val_mse: 28.4828\n",
            "Epoch 344/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 77.6054 - mse: 77.6054 - val_loss: 23.7203 - val_mse: 23.7203\n",
            "Epoch 345/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 71.4144 - mse: 71.4144 - val_loss: 23.9083 - val_mse: 23.9083\n",
            "Epoch 346/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 72.5713 - mse: 72.5713 - val_loss: 23.5371 - val_mse: 23.5371\n",
            "Epoch 347/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 85.3406 - mse: 85.3406 - val_loss: 38.6420 - val_mse: 38.6420\n",
            "Epoch 348/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 77.1667 - mse: 77.1667 - val_loss: 29.1583 - val_mse: 29.1583\n",
            "Epoch 349/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 87.5032 - mse: 87.5032 - val_loss: 26.3862 - val_mse: 26.3862\n",
            "Epoch 350/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 86.7472 - mse: 86.7472 - val_loss: 28.8381 - val_mse: 28.8381\n",
            "Epoch 351/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 72.4967 - mse: 72.4967 - val_loss: 27.8070 - val_mse: 27.8070\n",
            "Epoch 352/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 84.4281 - mse: 84.4281 - val_loss: 24.4039 - val_mse: 24.4039\n",
            "Epoch 353/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 76.5955 - mse: 76.5955 - val_loss: 24.2360 - val_mse: 24.2360\n",
            "Epoch 354/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 79.2565 - mse: 79.2565 - val_loss: 28.5674 - val_mse: 28.5674\n",
            "Epoch 355/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 79.7834 - mse: 79.7834 - val_loss: 22.3034 - val_mse: 22.3034\n",
            "Epoch 356/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 84.6055 - mse: 84.6055 - val_loss: 24.7555 - val_mse: 24.7555\n",
            "Epoch 357/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 70.2987 - mse: 70.2987 - val_loss: 23.8879 - val_mse: 23.8879\n",
            "Epoch 358/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 88.8407 - mse: 88.8407 - val_loss: 53.9866 - val_mse: 53.9866\n",
            "Epoch 359/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 80.2367 - mse: 80.2367 - val_loss: 24.8425 - val_mse: 24.8425\n",
            "Epoch 360/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 74.6607 - mse: 74.6607 - val_loss: 33.8314 - val_mse: 33.8314\n",
            "Epoch 361/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 82.1258 - mse: 82.1258 - val_loss: 22.2108 - val_mse: 22.2108\n",
            "Epoch 362/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 79.7599 - mse: 79.7599 - val_loss: 23.1502 - val_mse: 23.1502\n",
            "Epoch 363/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 76.6362 - mse: 76.6362 - val_loss: 27.6728 - val_mse: 27.6728\n",
            "Epoch 364/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 79.0036 - mse: 79.0036 - val_loss: 27.9281 - val_mse: 27.9281\n",
            "Epoch 365/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 68.0737 - mse: 68.0737 - val_loss: 28.6214 - val_mse: 28.6214\n",
            "Epoch 366/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 81.2440 - mse: 81.2440 - val_loss: 22.3490 - val_mse: 22.3490\n",
            "Epoch 367/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 73.3703 - mse: 73.3703 - val_loss: 32.8206 - val_mse: 32.8206\n",
            "Epoch 368/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 68.6601 - mse: 68.6601 - val_loss: 33.1739 - val_mse: 33.1739\n",
            "Epoch 369/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 77.9867 - mse: 77.9867 - val_loss: 28.2374 - val_mse: 28.2374\n",
            "Epoch 370/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 76.5718 - mse: 76.5718 - val_loss: 25.2713 - val_mse: 25.2713\n",
            "Epoch 371/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 78.9617 - mse: 78.9617 - val_loss: 28.3436 - val_mse: 28.3436\n",
            "Epoch 372/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 72.1339 - mse: 72.1339 - val_loss: 42.2223 - val_mse: 42.2223\n",
            "Epoch 373/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 75.3519 - mse: 75.3519 - val_loss: 42.8462 - val_mse: 42.8462\n",
            "Epoch 374/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 71.9396 - mse: 71.9396 - val_loss: 30.1509 - val_mse: 30.1509\n",
            "Epoch 375/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 89.3220 - mse: 89.3220 - val_loss: 29.5085 - val_mse: 29.5085\n",
            "Epoch 376/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 83.1898 - mse: 83.1898 - val_loss: 22.7032 - val_mse: 22.7032\n",
            "Epoch 377/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 59.1637 - mse: 59.1637 - val_loss: 29.6702 - val_mse: 29.6702\n",
            "Epoch 378/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 71.4484 - mse: 71.4484 - val_loss: 33.2489 - val_mse: 33.2489\n",
            "Epoch 379/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 72.4389 - mse: 72.4389 - val_loss: 36.3974 - val_mse: 36.3974\n",
            "Epoch 380/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.5347 - mse: 68.5347 - val_loss: 34.1180 - val_mse: 34.1180\n",
            "Epoch 381/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 70.8153 - mse: 70.8153 - val_loss: 24.1177 - val_mse: 24.1177\n",
            "Epoch 382/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 85.6058 - mse: 85.6058 - val_loss: 24.2564 - val_mse: 24.2564\n",
            "Epoch 383/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 78.0899 - mse: 78.0899 - val_loss: 34.7349 - val_mse: 34.7349\n",
            "Epoch 384/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 77.5851 - mse: 77.5851 - val_loss: 25.1260 - val_mse: 25.1260\n",
            "Epoch 385/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 69.2623 - mse: 69.2623 - val_loss: 24.1116 - val_mse: 24.1116\n",
            "Epoch 386/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 75.9683 - mse: 75.9683 - val_loss: 24.8258 - val_mse: 24.8258\n",
            "Epoch 387/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 79.5008 - mse: 79.5008 - val_loss: 51.6151 - val_mse: 51.6151\n",
            "Epoch 388/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 78.5620 - mse: 78.5620 - val_loss: 23.9133 - val_mse: 23.9133\n",
            "Epoch 389/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 71.0925 - mse: 71.0925 - val_loss: 24.2373 - val_mse: 24.2373\n",
            "Epoch 390/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 69.2132 - mse: 69.2132 - val_loss: 24.3618 - val_mse: 24.3618\n",
            "Epoch 391/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 74.0179 - mse: 74.0179 - val_loss: 22.7610 - val_mse: 22.7610\n",
            "Epoch 392/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 85.9516 - mse: 85.9516 - val_loss: 25.1184 - val_mse: 25.1184\n",
            "Epoch 393/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 72.3326 - mse: 72.3326 - val_loss: 30.8035 - val_mse: 30.8035\n",
            "Epoch 394/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 78.9571 - mse: 78.9571 - val_loss: 28.3840 - val_mse: 28.3840\n",
            "Epoch 395/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 83.0053 - mse: 83.0053 - val_loss: 21.9884 - val_mse: 21.9884\n",
            "Epoch 396/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 74.8317 - mse: 74.8317 - val_loss: 26.2848 - val_mse: 26.2848\n",
            "Epoch 397/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 63.4283 - mse: 63.4283 - val_loss: 27.4266 - val_mse: 27.4266\n",
            "Epoch 398/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 78.8461 - mse: 78.8461 - val_loss: 24.3172 - val_mse: 24.3172\n",
            "Epoch 399/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 78.4731 - mse: 78.4731 - val_loss: 23.9544 - val_mse: 23.9544\n",
            "Epoch 400/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 80.5056 - mse: 80.5056 - val_loss: 22.2568 - val_mse: 22.2568\n",
            "Epoch 401/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 69.1404 - mse: 69.1404 - val_loss: 27.5617 - val_mse: 27.5617\n",
            "Epoch 402/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 62.9833 - mse: 62.9833 - val_loss: 25.1719 - val_mse: 25.1719\n",
            "Epoch 403/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 61.8295 - mse: 61.8295 - val_loss: 29.5495 - val_mse: 29.5495\n",
            "Epoch 404/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 66.4381 - mse: 66.4381 - val_loss: 22.9368 - val_mse: 22.9368\n",
            "Epoch 405/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 75.8137 - mse: 75.8137 - val_loss: 22.9032 - val_mse: 22.9032\n",
            "Epoch 406/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 70.2528 - mse: 70.2528 - val_loss: 25.6525 - val_mse: 25.6525\n",
            "Epoch 407/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 71.3085 - mse: 71.3085 - val_loss: 26.0922 - val_mse: 26.0922\n",
            "Epoch 408/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 75.7380 - mse: 75.7380 - val_loss: 34.1888 - val_mse: 34.1888\n",
            "Epoch 409/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 67.9363 - mse: 67.9363 - val_loss: 42.4502 - val_mse: 42.4502\n",
            "Epoch 410/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 86.0811 - mse: 86.0811 - val_loss: 32.7722 - val_mse: 32.7722\n",
            "Epoch 411/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 59.2746 - mse: 59.2746 - val_loss: 26.4844 - val_mse: 26.4844\n",
            "Epoch 412/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 75.8814 - mse: 75.8814 - val_loss: 41.1865 - val_mse: 41.1865\n",
            "Epoch 413/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 75.2226 - mse: 75.2226 - val_loss: 28.4818 - val_mse: 28.4818\n",
            "Epoch 414/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 76.9543 - mse: 76.9543 - val_loss: 22.4648 - val_mse: 22.4648\n",
            "Epoch 415/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.2952 - mse: 68.2952 - val_loss: 22.4143 - val_mse: 22.4143\n",
            "Epoch 416/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 76.4719 - mse: 76.4719 - val_loss: 22.5135 - val_mse: 22.5135\n",
            "Epoch 417/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 62.5091 - mse: 62.5091 - val_loss: 22.1447 - val_mse: 22.1447\n",
            "Epoch 418/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 72.7962 - mse: 72.7962 - val_loss: 31.6825 - val_mse: 31.6825\n",
            "Epoch 419/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 81.8805 - mse: 81.8805 - val_loss: 25.9882 - val_mse: 25.9882\n",
            "Epoch 420/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 64.7270 - mse: 64.7270 - val_loss: 25.1318 - val_mse: 25.1318\n",
            "Epoch 421/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 80.5898 - mse: 80.5898 - val_loss: 29.6772 - val_mse: 29.6772\n",
            "Epoch 422/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 78.0646 - mse: 78.0646 - val_loss: 23.7234 - val_mse: 23.7234\n",
            "Epoch 423/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 79.1998 - mse: 79.1998 - val_loss: 23.1610 - val_mse: 23.1610\n",
            "Epoch 424/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 72.6624 - mse: 72.6624 - val_loss: 30.8610 - val_mse: 30.8610\n",
            "Epoch 425/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 68.9739 - mse: 68.9739 - val_loss: 27.2752 - val_mse: 27.2752\n",
            "Epoch 426/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 63.9642 - mse: 63.9642 - val_loss: 22.9860 - val_mse: 22.9860\n",
            "Epoch 427/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 68.1313 - mse: 68.1313 - val_loss: 24.2551 - val_mse: 24.2551\n",
            "Epoch 428/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 67.8142 - mse: 67.8142 - val_loss: 29.4793 - val_mse: 29.4793\n",
            "Epoch 429/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 59.6813 - mse: 59.6813 - val_loss: 31.2621 - val_mse: 31.2621\n",
            "Epoch 430/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 82.4929 - mse: 82.4929 - val_loss: 25.2577 - val_mse: 25.2577\n",
            "Epoch 431/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 66.4821 - mse: 66.4821 - val_loss: 32.7453 - val_mse: 32.7453\n",
            "Epoch 432/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 70.2503 - mse: 70.2503 - val_loss: 27.7304 - val_mse: 27.7304\n",
            "Epoch 433/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 66.6666 - mse: 66.6666 - val_loss: 23.9599 - val_mse: 23.9599\n",
            "Epoch 434/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 60.9367 - mse: 60.9367 - val_loss: 22.1603 - val_mse: 22.1603\n",
            "Epoch 435/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 75.3314 - mse: 75.3314 - val_loss: 25.8566 - val_mse: 25.8566\n",
            "Epoch 436/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.5507 - mse: 68.5507 - val_loss: 22.0043 - val_mse: 22.0043\n",
            "Epoch 437/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 74.3787 - mse: 74.3787 - val_loss: 27.2422 - val_mse: 27.2422\n",
            "Epoch 438/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 59.4667 - mse: 59.4667 - val_loss: 22.3875 - val_mse: 22.3875\n",
            "Epoch 439/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 72.4597 - mse: 72.4597 - val_loss: 29.5669 - val_mse: 29.5669\n",
            "Epoch 440/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 54.3044 - mse: 54.3044 - val_loss: 28.2017 - val_mse: 28.2017\n",
            "Epoch 441/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 65.4952 - mse: 65.4952 - val_loss: 25.7242 - val_mse: 25.7242\n",
            "Epoch 442/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.8956 - mse: 68.8956 - val_loss: 21.0082 - val_mse: 21.0082\n",
            "Epoch 443/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 91.6227 - mse: 91.6227 - val_loss: 42.5343 - val_mse: 42.5343\n",
            "Epoch 444/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.5809 - mse: 68.5809 - val_loss: 24.3539 - val_mse: 24.3539\n",
            "Epoch 445/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 66.4296 - mse: 66.4296 - val_loss: 23.8378 - val_mse: 23.8378\n",
            "Epoch 446/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.4566 - mse: 68.4566 - val_loss: 41.5703 - val_mse: 41.5703\n",
            "Epoch 447/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 77.9325 - mse: 77.9325 - val_loss: 31.1947 - val_mse: 31.1947\n",
            "Epoch 448/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 67.7765 - mse: 67.7765 - val_loss: 25.4853 - val_mse: 25.4853\n",
            "Epoch 449/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 63.6023 - mse: 63.6023 - val_loss: 25.3910 - val_mse: 25.3910\n",
            "Epoch 450/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 65.4466 - mse: 65.4466 - val_loss: 22.8990 - val_mse: 22.8990\n",
            "Epoch 451/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 63.3237 - mse: 63.3237 - val_loss: 28.8917 - val_mse: 28.8917\n",
            "Epoch 452/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 59.4245 - mse: 59.4245 - val_loss: 23.9883 - val_mse: 23.9883\n",
            "Epoch 453/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 77.2149 - mse: 77.2149 - val_loss: 21.7467 - val_mse: 21.7467\n",
            "Epoch 454/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 64.1721 - mse: 64.1721 - val_loss: 23.8670 - val_mse: 23.8670\n",
            "Epoch 455/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.5381 - mse: 68.5381 - val_loss: 43.8411 - val_mse: 43.8411\n",
            "Epoch 456/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 66.2544 - mse: 66.2544 - val_loss: 22.6759 - val_mse: 22.6759\n",
            "Epoch 457/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 66.2334 - mse: 66.2334 - val_loss: 22.0645 - val_mse: 22.0645\n",
            "Epoch 458/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 65.6225 - mse: 65.6225 - val_loss: 38.2177 - val_mse: 38.2177\n",
            "Epoch 459/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 75.4817 - mse: 75.4817 - val_loss: 32.2724 - val_mse: 32.2724\n",
            "Epoch 460/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 66.8216 - mse: 66.8216 - val_loss: 24.0225 - val_mse: 24.0225\n",
            "Epoch 461/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 65.7376 - mse: 65.7376 - val_loss: 21.0705 - val_mse: 21.0705\n",
            "Epoch 462/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 69.3458 - mse: 69.3458 - val_loss: 21.6205 - val_mse: 21.6205\n",
            "Epoch 463/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 66.6640 - mse: 66.6640 - val_loss: 26.3317 - val_mse: 26.3317\n",
            "Epoch 464/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 71.4277 - mse: 71.4277 - val_loss: 24.8781 - val_mse: 24.8781\n",
            "Epoch 465/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 69.9675 - mse: 69.9675 - val_loss: 29.9752 - val_mse: 29.9752\n",
            "Epoch 466/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.7402 - mse: 68.7402 - val_loss: 21.2945 - val_mse: 21.2945\n",
            "Epoch 467/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 66.6521 - mse: 66.6521 - val_loss: 32.4249 - val_mse: 32.4249\n",
            "Epoch 468/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 50.8819 - mse: 50.8819 - val_loss: 24.1425 - val_mse: 24.1425\n",
            "Epoch 469/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 70.1756 - mse: 70.1756 - val_loss: 28.5150 - val_mse: 28.5150\n",
            "Epoch 470/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 72.9118 - mse: 72.9118 - val_loss: 22.3721 - val_mse: 22.3721\n",
            "Epoch 471/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 56.2916 - mse: 56.2916 - val_loss: 25.6557 - val_mse: 25.6557\n",
            "Epoch 472/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 73.6654 - mse: 73.6654 - val_loss: 22.6473 - val_mse: 22.6473\n",
            "Epoch 473/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 70.1758 - mse: 70.1758 - val_loss: 21.2975 - val_mse: 21.2975\n",
            "Epoch 474/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 70.8138 - mse: 70.8138 - val_loss: 29.0710 - val_mse: 29.0710\n",
            "Epoch 475/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 62.0925 - mse: 62.0925 - val_loss: 21.9137 - val_mse: 21.9137\n",
            "Epoch 476/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 69.7841 - mse: 69.7841 - val_loss: 31.0260 - val_mse: 31.0260\n",
            "Epoch 477/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 72.3740 - mse: 72.3740 - val_loss: 25.1483 - val_mse: 25.1483\n",
            "Epoch 478/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 74.9089 - mse: 74.9089 - val_loss: 21.9961 - val_mse: 21.9961\n",
            "Epoch 479/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 59.4565 - mse: 59.4565 - val_loss: 21.7999 - val_mse: 21.7999\n",
            "Epoch 480/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.5752 - mse: 68.5752 - val_loss: 27.2168 - val_mse: 27.2168\n",
            "Epoch 481/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 65.0357 - mse: 65.0357 - val_loss: 19.9686 - val_mse: 19.9686\n",
            "Epoch 482/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 71.3360 - mse: 71.3360 - val_loss: 20.8619 - val_mse: 20.8619\n",
            "Epoch 483/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 71.8740 - mse: 71.8740 - val_loss: 26.0535 - val_mse: 26.0535\n",
            "Epoch 484/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 62.3570 - mse: 62.3570 - val_loss: 45.3784 - val_mse: 45.3784\n",
            "Epoch 485/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 66.9370 - mse: 66.9370 - val_loss: 30.0697 - val_mse: 30.0697\n",
            "Epoch 486/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 67.5162 - mse: 67.5162 - val_loss: 29.7892 - val_mse: 29.7892\n",
            "Epoch 487/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 70.7960 - mse: 70.7960 - val_loss: 26.6869 - val_mse: 26.6869\n",
            "Epoch 488/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 70.8913 - mse: 70.8913 - val_loss: 29.5180 - val_mse: 29.5180\n",
            "Epoch 489/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.7648 - mse: 57.7648 - val_loss: 22.2888 - val_mse: 22.2888\n",
            "Epoch 490/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.7830 - mse: 68.7830 - val_loss: 22.3564 - val_mse: 22.3564\n",
            "Epoch 491/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 65.9775 - mse: 65.9775 - val_loss: 24.1215 - val_mse: 24.1215\n",
            "Epoch 492/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 59.2754 - mse: 59.2754 - val_loss: 23.1884 - val_mse: 23.1884\n",
            "Epoch 493/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 60.9166 - mse: 60.9166 - val_loss: 26.7288 - val_mse: 26.7288\n",
            "Epoch 494/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 70.4336 - mse: 70.4336 - val_loss: 32.1213 - val_mse: 32.1213\n",
            "Epoch 495/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 71.3236 - mse: 71.3236 - val_loss: 23.8924 - val_mse: 23.8924\n",
            "Epoch 496/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.6418 - mse: 51.6418 - val_loss: 22.3431 - val_mse: 22.3431\n",
            "Epoch 497/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 69.0781 - mse: 69.0781 - val_loss: 23.6939 - val_mse: 23.6939\n",
            "Epoch 498/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 65.1618 - mse: 65.1618 - val_loss: 23.0382 - val_mse: 23.0382\n",
            "Epoch 499/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 61.1139 - mse: 61.1139 - val_loss: 25.5121 - val_mse: 25.5121\n",
            "Epoch 500/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 71.1474 - mse: 71.1474 - val_loss: 24.0657 - val_mse: 24.0657\n",
            "Epoch 501/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.4356 - mse: 68.4356 - val_loss: 22.5622 - val_mse: 22.5622\n",
            "Epoch 502/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 60.6375 - mse: 60.6375 - val_loss: 21.3147 - val_mse: 21.3147\n",
            "Epoch 503/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 58.0911 - mse: 58.0911 - val_loss: 21.1750 - val_mse: 21.1750\n",
            "Epoch 504/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 64.7308 - mse: 64.7308 - val_loss: 47.6731 - val_mse: 47.6731\n",
            "Epoch 505/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 64.9410 - mse: 64.9410 - val_loss: 24.4880 - val_mse: 24.4880\n",
            "Epoch 506/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 63.5834 - mse: 63.5834 - val_loss: 27.0260 - val_mse: 27.0260\n",
            "Epoch 507/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 68.3156 - mse: 68.3156 - val_loss: 28.5645 - val_mse: 28.5645\n",
            "Epoch 508/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 61.6547 - mse: 61.6547 - val_loss: 27.4345 - val_mse: 27.4345\n",
            "Epoch 509/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 64.9427 - mse: 64.9427 - val_loss: 23.5775 - val_mse: 23.5775\n",
            "Epoch 510/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.4430 - mse: 57.4430 - val_loss: 22.5866 - val_mse: 22.5866\n",
            "Epoch 511/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 70.8004 - mse: 70.8004 - val_loss: 21.5641 - val_mse: 21.5641\n",
            "Epoch 512/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 58.5949 - mse: 58.5949 - val_loss: 27.2169 - val_mse: 27.2169\n",
            "Epoch 513/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.7231 - mse: 68.7231 - val_loss: 22.6541 - val_mse: 22.6541\n",
            "Epoch 514/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 67.8539 - mse: 67.8539 - val_loss: 21.3144 - val_mse: 21.3144\n",
            "Epoch 515/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 64.1843 - mse: 64.1843 - val_loss: 21.1548 - val_mse: 21.1548\n",
            "Epoch 516/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 64.1641 - mse: 64.1641 - val_loss: 20.7382 - val_mse: 20.7382\n",
            "Epoch 517/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 52.5146 - mse: 52.5146 - val_loss: 23.9698 - val_mse: 23.9698\n",
            "Epoch 518/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 60.6657 - mse: 60.6657 - val_loss: 33.6526 - val_mse: 33.6526\n",
            "Epoch 519/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 59.3126 - mse: 59.3126 - val_loss: 26.0607 - val_mse: 26.0607\n",
            "Epoch 520/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 64.3123 - mse: 64.3123 - val_loss: 28.6351 - val_mse: 28.6351\n",
            "Epoch 521/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 55.2581 - mse: 55.2581 - val_loss: 27.4208 - val_mse: 27.4208\n",
            "Epoch 522/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 63.8178 - mse: 63.8178 - val_loss: 21.4923 - val_mse: 21.4923\n",
            "Epoch 523/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 56.2804 - mse: 56.2804 - val_loss: 22.7572 - val_mse: 22.7572\n",
            "Epoch 524/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 64.1540 - mse: 64.1540 - val_loss: 21.2465 - val_mse: 21.2465\n",
            "Epoch 525/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 69.7387 - mse: 69.7387 - val_loss: 37.8323 - val_mse: 37.8323\n",
            "Epoch 526/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 73.0915 - mse: 73.0915 - val_loss: 24.5924 - val_mse: 24.5924\n",
            "Epoch 527/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 67.7700 - mse: 67.7700 - val_loss: 44.6463 - val_mse: 44.6463\n",
            "Epoch 528/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 65.4456 - mse: 65.4456 - val_loss: 36.8737 - val_mse: 36.8737\n",
            "Epoch 529/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 56.3775 - mse: 56.3775 - val_loss: 21.9958 - val_mse: 21.9958\n",
            "Epoch 530/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 53.4237 - mse: 53.4237 - val_loss: 20.3197 - val_mse: 20.3197\n",
            "Epoch 531/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 64.7369 - mse: 64.7369 - val_loss: 36.2034 - val_mse: 36.2034\n",
            "Epoch 532/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 72.4789 - mse: 72.4789 - val_loss: 28.1834 - val_mse: 28.1834\n",
            "Epoch 533/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 65.6712 - mse: 65.6712 - val_loss: 33.5816 - val_mse: 33.5816\n",
            "Epoch 534/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 72.1333 - mse: 72.1333 - val_loss: 48.2785 - val_mse: 48.2785\n",
            "Epoch 535/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 57.6517 - mse: 57.6517 - val_loss: 23.8697 - val_mse: 23.8697\n",
            "Epoch 536/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 67.3415 - mse: 67.3415 - val_loss: 21.5933 - val_mse: 21.5933\n",
            "Epoch 537/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 58.7284 - mse: 58.7284 - val_loss: 25.3493 - val_mse: 25.3493\n",
            "Epoch 538/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 62.4310 - mse: 62.4310 - val_loss: 23.9874 - val_mse: 23.9874\n",
            "Epoch 539/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 62.1037 - mse: 62.1037 - val_loss: 20.8480 - val_mse: 20.8480\n",
            "Epoch 540/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 54.7259 - mse: 54.7259 - val_loss: 30.1548 - val_mse: 30.1548\n",
            "Epoch 541/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 54.6823 - mse: 54.6823 - val_loss: 37.8157 - val_mse: 37.8157\n",
            "Epoch 542/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 63.8749 - mse: 63.8749 - val_loss: 31.1640 - val_mse: 31.1640\n",
            "Epoch 543/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 62.3783 - mse: 62.3783 - val_loss: 23.6519 - val_mse: 23.6519\n",
            "Epoch 544/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 56.6342 - mse: 56.6342 - val_loss: 24.3220 - val_mse: 24.3220\n",
            "Epoch 545/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 61.3703 - mse: 61.3703 - val_loss: 23.9763 - val_mse: 23.9763\n",
            "Epoch 546/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 53.0233 - mse: 53.0233 - val_loss: 26.4972 - val_mse: 26.4972\n",
            "Epoch 547/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 58.9374 - mse: 58.9374 - val_loss: 19.9128 - val_mse: 19.9128\n",
            "Epoch 548/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.1647 - mse: 57.1647 - val_loss: 23.7541 - val_mse: 23.7541\n",
            "Epoch 549/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 60.9318 - mse: 60.9318 - val_loss: 24.7287 - val_mse: 24.7287\n",
            "Epoch 550/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 58.4414 - mse: 58.4414 - val_loss: 22.7776 - val_mse: 22.7776\n",
            "Epoch 551/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 57.6502 - mse: 57.6502 - val_loss: 28.9387 - val_mse: 28.9387\n",
            "Epoch 552/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 55.9295 - mse: 55.9295 - val_loss: 32.4242 - val_mse: 32.4242\n",
            "Epoch 553/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 58.9653 - mse: 58.9653 - val_loss: 25.7503 - val_mse: 25.7503\n",
            "Epoch 554/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 58.3730 - mse: 58.3730 - val_loss: 22.1199 - val_mse: 22.1199\n",
            "Epoch 555/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 52.3331 - mse: 52.3331 - val_loss: 23.4791 - val_mse: 23.4791\n",
            "Epoch 556/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.6906 - mse: 57.6906 - val_loss: 19.3493 - val_mse: 19.3493\n",
            "Epoch 557/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 60.5301 - mse: 60.5301 - val_loss: 20.2341 - val_mse: 20.2341\n",
            "Epoch 558/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 50.3324 - mse: 50.3324 - val_loss: 24.2655 - val_mse: 24.2655\n",
            "Epoch 559/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 66.3777 - mse: 66.3777 - val_loss: 21.4935 - val_mse: 21.4935\n",
            "Epoch 560/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 59.7984 - mse: 59.7984 - val_loss: 19.6722 - val_mse: 19.6722\n",
            "Epoch 561/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 54.6926 - mse: 54.6926 - val_loss: 19.6725 - val_mse: 19.6725\n",
            "Epoch 562/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.6242 - mse: 57.6242 - val_loss: 19.4932 - val_mse: 19.4932\n",
            "Epoch 563/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 51.4405 - mse: 51.4405 - val_loss: 19.6779 - val_mse: 19.6779\n",
            "Epoch 564/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 56.1678 - mse: 56.1678 - val_loss: 23.9697 - val_mse: 23.9697\n",
            "Epoch 565/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 58.9879 - mse: 58.9879 - val_loss: 20.5377 - val_mse: 20.5377\n",
            "Epoch 566/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 60.8911 - mse: 60.8911 - val_loss: 21.5380 - val_mse: 21.5380\n",
            "Epoch 567/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 52.4867 - mse: 52.4867 - val_loss: 32.1794 - val_mse: 32.1794\n",
            "Epoch 568/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 61.2373 - mse: 61.2373 - val_loss: 25.5551 - val_mse: 25.5551\n",
            "Epoch 569/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 57.2505 - mse: 57.2505 - val_loss: 20.5313 - val_mse: 20.5313\n",
            "Epoch 570/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 64.0186 - mse: 64.0186 - val_loss: 21.3026 - val_mse: 21.3026\n",
            "Epoch 571/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 57.0740 - mse: 57.0740 - val_loss: 19.9194 - val_mse: 19.9194\n",
            "Epoch 572/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.7548 - mse: 68.7548 - val_loss: 20.6052 - val_mse: 20.6052\n",
            "Epoch 573/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 59.7337 - mse: 59.7337 - val_loss: 19.6326 - val_mse: 19.6326\n",
            "Epoch 574/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 62.0637 - mse: 62.0637 - val_loss: 20.9236 - val_mse: 20.9236\n",
            "Epoch 575/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 64.2134 - mse: 64.2134 - val_loss: 20.0550 - val_mse: 20.0550\n",
            "Epoch 576/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 62.1177 - mse: 62.1177 - val_loss: 25.4768 - val_mse: 25.4768\n",
            "Epoch 577/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 75.4328 - mse: 75.4328 - val_loss: 30.3123 - val_mse: 30.3123\n",
            "Epoch 578/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.6951 - mse: 57.6951 - val_loss: 20.9923 - val_mse: 20.9923\n",
            "Epoch 579/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.6328 - mse: 57.6328 - val_loss: 20.8355 - val_mse: 20.8355\n",
            "Epoch 580/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 68.2087 - mse: 68.2087 - val_loss: 22.2289 - val_mse: 22.2289\n",
            "Epoch 581/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 60.3618 - mse: 60.3618 - val_loss: 20.7315 - val_mse: 20.7315\n",
            "Epoch 582/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.7521 - mse: 57.7521 - val_loss: 23.8216 - val_mse: 23.8216\n",
            "Epoch 583/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 56.4909 - mse: 56.4909 - val_loss: 21.1132 - val_mse: 21.1132\n",
            "Epoch 584/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 59.6300 - mse: 59.6300 - val_loss: 20.4113 - val_mse: 20.4113\n",
            "Epoch 585/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 68.6648 - mse: 68.6648 - val_loss: 20.9982 - val_mse: 20.9982\n",
            "Epoch 586/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 53.3887 - mse: 53.3887 - val_loss: 38.8602 - val_mse: 38.8602\n",
            "Epoch 587/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 59.1897 - mse: 59.1897 - val_loss: 38.5836 - val_mse: 38.5836\n",
            "Epoch 588/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 58.4256 - mse: 58.4256 - val_loss: 25.3539 - val_mse: 25.3539\n",
            "Epoch 589/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 62.1965 - mse: 62.1965 - val_loss: 23.3307 - val_mse: 23.3307\n",
            "Epoch 590/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 55.8289 - mse: 55.8289 - val_loss: 21.9352 - val_mse: 21.9352\n",
            "Epoch 591/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 54.0490 - mse: 54.0490 - val_loss: 24.0198 - val_mse: 24.0198\n",
            "Epoch 592/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 59.5933 - mse: 59.5933 - val_loss: 19.7017 - val_mse: 19.7017\n",
            "Epoch 593/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 55.4989 - mse: 55.4989 - val_loss: 23.8931 - val_mse: 23.8931\n",
            "Epoch 594/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 62.2909 - mse: 62.2909 - val_loss: 23.3891 - val_mse: 23.3891\n",
            "Epoch 595/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.0619 - mse: 51.0619 - val_loss: 27.5223 - val_mse: 27.5223\n",
            "Epoch 596/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 68.2927 - mse: 68.2927 - val_loss: 35.6950 - val_mse: 35.6950\n",
            "Epoch 597/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 69.3061 - mse: 69.3061 - val_loss: 21.2635 - val_mse: 21.2635\n",
            "Epoch 598/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 56.7138 - mse: 56.7138 - val_loss: 19.0741 - val_mse: 19.0741\n",
            "Epoch 599/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 64.4798 - mse: 64.4798 - val_loss: 27.3531 - val_mse: 27.3531\n",
            "Epoch 600/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 54.4374 - mse: 54.4374 - val_loss: 25.1679 - val_mse: 25.1679\n",
            "Epoch 601/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 52.8077 - mse: 52.8077 - val_loss: 20.3821 - val_mse: 20.3821\n",
            "Epoch 602/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 63.2809 - mse: 63.2809 - val_loss: 22.4262 - val_mse: 22.4262\n",
            "Epoch 603/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 61.0272 - mse: 61.0272 - val_loss: 19.8511 - val_mse: 19.8511\n",
            "Epoch 604/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 47.7955 - mse: 47.7955 - val_loss: 21.1474 - val_mse: 21.1474\n",
            "Epoch 605/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 58.8966 - mse: 58.8966 - val_loss: 20.2552 - val_mse: 20.2552\n",
            "Epoch 606/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 57.7934 - mse: 57.7934 - val_loss: 19.9977 - val_mse: 19.9977\n",
            "Epoch 607/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 54.6228 - mse: 54.6228 - val_loss: 24.4014 - val_mse: 24.4014\n",
            "Epoch 608/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 46.8603 - mse: 46.8603 - val_loss: 20.6338 - val_mse: 20.6338\n",
            "Epoch 609/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 47.0045 - mse: 47.0045 - val_loss: 19.5515 - val_mse: 19.5515\n",
            "Epoch 610/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 53.9741 - mse: 53.9741 - val_loss: 23.1096 - val_mse: 23.1096\n",
            "Epoch 611/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 53.7903 - mse: 53.7903 - val_loss: 28.4796 - val_mse: 28.4796\n",
            "Epoch 612/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 53.1435 - mse: 53.1435 - val_loss: 23.7311 - val_mse: 23.7311\n",
            "Epoch 613/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.1139 - mse: 49.1139 - val_loss: 21.0164 - val_mse: 21.0164\n",
            "Epoch 614/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 56.3604 - mse: 56.3604 - val_loss: 22.4773 - val_mse: 22.4773\n",
            "Epoch 615/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 50.4382 - mse: 50.4382 - val_loss: 21.0466 - val_mse: 21.0466\n",
            "Epoch 616/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 52.0968 - mse: 52.0968 - val_loss: 31.5564 - val_mse: 31.5564\n",
            "Epoch 617/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 54.6720 - mse: 54.6720 - val_loss: 21.2872 - val_mse: 21.2872\n",
            "Epoch 618/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 47.1307 - mse: 47.1307 - val_loss: 20.5425 - val_mse: 20.5425\n",
            "Epoch 619/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 54.7921 - mse: 54.7921 - val_loss: 31.0924 - val_mse: 31.0924\n",
            "Epoch 620/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 54.9257 - mse: 54.9257 - val_loss: 22.1166 - val_mse: 22.1166\n",
            "Epoch 621/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 48.8107 - mse: 48.8107 - val_loss: 21.8137 - val_mse: 21.8137\n",
            "Epoch 622/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 56.2425 - mse: 56.2425 - val_loss: 20.5391 - val_mse: 20.5391\n",
            "Epoch 623/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 55.2523 - mse: 55.2523 - val_loss: 25.6980 - val_mse: 25.6980\n",
            "Epoch 624/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 39.5814 - mse: 39.5814 - val_loss: 19.3317 - val_mse: 19.3317\n",
            "Epoch 625/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.1431 - mse: 43.1431 - val_loss: 18.8350 - val_mse: 18.8350\n",
            "Epoch 626/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.2187 - mse: 51.2187 - val_loss: 20.6164 - val_mse: 20.6164\n",
            "Epoch 627/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.5751 - mse: 43.5751 - val_loss: 25.6795 - val_mse: 25.6795\n",
            "Epoch 628/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.1377 - mse: 57.1377 - val_loss: 19.6917 - val_mse: 19.6917\n",
            "Epoch 629/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.2677 - mse: 49.2677 - val_loss: 19.7117 - val_mse: 19.7117\n",
            "Epoch 630/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 55.1028 - mse: 55.1028 - val_loss: 19.4881 - val_mse: 19.4881\n",
            "Epoch 631/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 50.3492 - mse: 50.3492 - val_loss: 19.1201 - val_mse: 19.1201\n",
            "Epoch 632/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 58.0208 - mse: 58.0208 - val_loss: 18.4886 - val_mse: 18.4886\n",
            "Epoch 633/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.2176 - mse: 51.2176 - val_loss: 18.9250 - val_mse: 18.9250\n",
            "Epoch 634/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 52.0213 - mse: 52.0213 - val_loss: 20.1118 - val_mse: 20.1118\n",
            "Epoch 635/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 50.5436 - mse: 50.5436 - val_loss: 26.3809 - val_mse: 26.3809\n",
            "Epoch 636/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 57.6395 - mse: 57.6395 - val_loss: 26.7912 - val_mse: 26.7912\n",
            "Epoch 637/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 48.2992 - mse: 48.2992 - val_loss: 21.4194 - val_mse: 21.4194\n",
            "Epoch 638/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 52.9473 - mse: 52.9473 - val_loss: 18.6440 - val_mse: 18.6440\n",
            "Epoch 639/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 52.6656 - mse: 52.6656 - val_loss: 18.9473 - val_mse: 18.9473\n",
            "Epoch 640/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 52.7507 - mse: 52.7507 - val_loss: 19.7529 - val_mse: 19.7529\n",
            "Epoch 641/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 60.4139 - mse: 60.4139 - val_loss: 25.6427 - val_mse: 25.6427\n",
            "Epoch 642/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.9736 - mse: 51.9736 - val_loss: 20.4545 - val_mse: 20.4545\n",
            "Epoch 643/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.0324 - mse: 46.0324 - val_loss: 25.1524 - val_mse: 25.1524\n",
            "Epoch 644/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.6852 - mse: 57.6852 - val_loss: 21.8098 - val_mse: 21.8098\n",
            "Epoch 645/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 47.6493 - mse: 47.6493 - val_loss: 19.1848 - val_mse: 19.1848\n",
            "Epoch 646/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 56.1056 - mse: 56.1056 - val_loss: 20.2864 - val_mse: 20.2864\n",
            "Epoch 647/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 55.7252 - mse: 55.7252 - val_loss: 25.1946 - val_mse: 25.1946\n",
            "Epoch 648/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 54.1871 - mse: 54.1871 - val_loss: 26.4126 - val_mse: 26.4126\n",
            "Epoch 649/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 52.3748 - mse: 52.3748 - val_loss: 25.4763 - val_mse: 25.4763\n",
            "Epoch 650/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 54.6645 - mse: 54.6645 - val_loss: 22.6436 - val_mse: 22.6436\n",
            "Epoch 651/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 55.2484 - mse: 55.2484 - val_loss: 22.2742 - val_mse: 22.2742\n",
            "Epoch 652/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.4813 - mse: 51.4813 - val_loss: 27.4408 - val_mse: 27.4408\n",
            "Epoch 653/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 52.7288 - mse: 52.7288 - val_loss: 19.8875 - val_mse: 19.8875\n",
            "Epoch 654/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 50.1980 - mse: 50.1980 - val_loss: 20.3461 - val_mse: 20.3461\n",
            "Epoch 655/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 67.7969 - mse: 67.7969 - val_loss: 22.2353 - val_mse: 22.2353\n",
            "Epoch 656/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 54.7844 - mse: 54.7844 - val_loss: 21.3982 - val_mse: 21.3982\n",
            "Epoch 657/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.7238 - mse: 51.7238 - val_loss: 19.3033 - val_mse: 19.3033\n",
            "Epoch 658/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 47.3254 - mse: 47.3254 - val_loss: 20.6290 - val_mse: 20.6290\n",
            "Epoch 659/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 54.3679 - mse: 54.3679 - val_loss: 24.0278 - val_mse: 24.0278\n",
            "Epoch 660/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 55.2540 - mse: 55.2540 - val_loss: 27.8527 - val_mse: 27.8527\n",
            "Epoch 661/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 43.8550 - mse: 43.8550 - val_loss: 21.6747 - val_mse: 21.6747\n",
            "Epoch 662/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 55.6324 - mse: 55.6324 - val_loss: 27.5565 - val_mse: 27.5565\n",
            "Epoch 663/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.4188 - mse: 51.4188 - val_loss: 28.5759 - val_mse: 28.5759\n",
            "Epoch 664/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 58.9622 - mse: 58.9622 - val_loss: 20.4175 - val_mse: 20.4175\n",
            "Epoch 665/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 53.6009 - mse: 53.6009 - val_loss: 19.0840 - val_mse: 19.0840\n",
            "Epoch 666/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 62.4143 - mse: 62.4143 - val_loss: 20.8697 - val_mse: 20.8697\n",
            "Epoch 667/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 52.9227 - mse: 52.9227 - val_loss: 19.0039 - val_mse: 19.0039\n",
            "Epoch 668/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.7019 - mse: 46.7019 - val_loss: 19.1902 - val_mse: 19.1902\n",
            "Epoch 669/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 50.3335 - mse: 50.3335 - val_loss: 21.4163 - val_mse: 21.4163\n",
            "Epoch 670/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 43.8347 - mse: 43.8347 - val_loss: 21.1295 - val_mse: 21.1295\n",
            "Epoch 671/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.9790 - mse: 43.9790 - val_loss: 19.5088 - val_mse: 19.5088\n",
            "Epoch 672/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 61.2716 - mse: 61.2716 - val_loss: 19.5770 - val_mse: 19.5770\n",
            "Epoch 673/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 45.4268 - mse: 45.4268 - val_loss: 18.9136 - val_mse: 18.9136\n",
            "Epoch 674/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 48.7873 - mse: 48.7873 - val_loss: 22.6376 - val_mse: 22.6376\n",
            "Epoch 675/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 42.7192 - mse: 42.7192 - val_loss: 21.9554 - val_mse: 21.9554\n",
            "Epoch 676/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 53.6292 - mse: 53.6292 - val_loss: 23.1096 - val_mse: 23.1096\n",
            "Epoch 677/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 44.5131 - mse: 44.5131 - val_loss: 20.0616 - val_mse: 20.0616\n",
            "Epoch 678/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 56.8149 - mse: 56.8149 - val_loss: 19.1284 - val_mse: 19.1284\n",
            "Epoch 679/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.6094 - mse: 43.6094 - val_loss: 18.8845 - val_mse: 18.8845\n",
            "Epoch 680/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 55.3520 - mse: 55.3520 - val_loss: 22.0428 - val_mse: 22.0428\n",
            "Epoch 681/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.7050 - mse: 51.7050 - val_loss: 21.0190 - val_mse: 21.0190\n",
            "Epoch 682/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 49.5326 - mse: 49.5326 - val_loss: 28.4957 - val_mse: 28.4957\n",
            "Epoch 683/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 50.2155 - mse: 50.2155 - val_loss: 22.3728 - val_mse: 22.3728\n",
            "Epoch 684/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.3951 - mse: 46.3951 - val_loss: 19.9909 - val_mse: 19.9909\n",
            "Epoch 685/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.1514 - mse: 49.1514 - val_loss: 19.1801 - val_mse: 19.1801\n",
            "Epoch 686/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 47.6593 - mse: 47.6593 - val_loss: 18.6817 - val_mse: 18.6817\n",
            "Epoch 687/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.4694 - mse: 57.4694 - val_loss: 20.7577 - val_mse: 20.7577\n",
            "Epoch 688/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.7890 - mse: 46.7890 - val_loss: 19.3593 - val_mse: 19.3593\n",
            "Epoch 689/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 53.3696 - mse: 53.3696 - val_loss: 22.4449 - val_mse: 22.4449\n",
            "Epoch 690/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 49.2903 - mse: 49.2903 - val_loss: 22.1959 - val_mse: 22.1959\n",
            "Epoch 691/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.4206 - mse: 49.4206 - val_loss: 27.3875 - val_mse: 27.3875\n",
            "Epoch 692/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 48.7504 - mse: 48.7504 - val_loss: 25.5542 - val_mse: 25.5542\n",
            "Epoch 693/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 60.0530 - mse: 60.0530 - val_loss: 21.1702 - val_mse: 21.1702\n",
            "Epoch 694/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 49.2497 - mse: 49.2497 - val_loss: 20.2817 - val_mse: 20.2817\n",
            "Epoch 695/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 54.6735 - mse: 54.6735 - val_loss: 18.8047 - val_mse: 18.8047\n",
            "Epoch 696/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 50.7335 - mse: 50.7335 - val_loss: 21.8427 - val_mse: 21.8427\n",
            "Epoch 697/10000\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 50.4559 - mse: 50.4559 - val_loss: 26.4351 - val_mse: 26.4351\n",
            "Epoch 698/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.9889 - mse: 49.9889 - val_loss: 20.7251 - val_mse: 20.7251\n",
            "Epoch 699/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 61.1595 - mse: 61.1595 - val_loss: 21.7113 - val_mse: 21.7113\n",
            "Epoch 700/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 43.1059 - mse: 43.1059 - val_loss: 24.1080 - val_mse: 24.1080\n",
            "Epoch 701/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 48.2141 - mse: 48.2141 - val_loss: 42.2799 - val_mse: 42.2799\n",
            "Epoch 702/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.5484 - mse: 57.5484 - val_loss: 41.0560 - val_mse: 41.0560\n",
            "Epoch 703/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 57.2568 - mse: 57.2568 - val_loss: 21.1027 - val_mse: 21.1027\n",
            "Epoch 704/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 45.5948 - mse: 45.5948 - val_loss: 19.8439 - val_mse: 19.8439\n",
            "Epoch 705/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 45.5024 - mse: 45.5024 - val_loss: 18.9032 - val_mse: 18.9032\n",
            "Epoch 706/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 45.1941 - mse: 45.1941 - val_loss: 20.3612 - val_mse: 20.3612\n",
            "Epoch 707/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 50.9169 - mse: 50.9169 - val_loss: 20.0293 - val_mse: 20.0293\n",
            "Epoch 708/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.5820 - mse: 41.5820 - val_loss: 19.1648 - val_mse: 19.1648\n",
            "Epoch 709/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 50.4242 - mse: 50.4242 - val_loss: 23.6651 - val_mse: 23.6651\n",
            "Epoch 710/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.6849 - mse: 49.6849 - val_loss: 24.5919 - val_mse: 24.5919\n",
            "Epoch 711/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 50.1282 - mse: 50.1282 - val_loss: 21.8347 - val_mse: 21.8347\n",
            "Epoch 712/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 49.4240 - mse: 49.4240 - val_loss: 18.5651 - val_mse: 18.5651\n",
            "Epoch 713/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 44.8526 - mse: 44.8526 - val_loss: 22.7196 - val_mse: 22.7196\n",
            "Epoch 714/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 49.0259 - mse: 49.0259 - val_loss: 19.8700 - val_mse: 19.8700\n",
            "Epoch 715/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.4989 - mse: 43.4989 - val_loss: 19.5982 - val_mse: 19.5982\n",
            "Epoch 716/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.6447 - mse: 51.6447 - val_loss: 23.2892 - val_mse: 23.2892\n",
            "Epoch 717/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.1213 - mse: 38.1213 - val_loss: 19.8033 - val_mse: 19.8033\n",
            "Epoch 718/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 50.9467 - mse: 50.9467 - val_loss: 25.8655 - val_mse: 25.8655\n",
            "Epoch 719/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.3929 - mse: 49.3929 - val_loss: 26.5697 - val_mse: 26.5697\n",
            "Epoch 720/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 47.4337 - mse: 47.4337 - val_loss: 19.9145 - val_mse: 19.9145\n",
            "Epoch 721/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 44.7021 - mse: 44.7021 - val_loss: 21.5892 - val_mse: 21.5892\n",
            "Epoch 722/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.5804 - mse: 46.5804 - val_loss: 20.7068 - val_mse: 20.7068\n",
            "Epoch 723/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 44.7292 - mse: 44.7292 - val_loss: 20.9340 - val_mse: 20.9340\n",
            "Epoch 724/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 52.5086 - mse: 52.5086 - val_loss: 28.0886 - val_mse: 28.0886\n",
            "Epoch 725/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 53.1624 - mse: 53.1624 - val_loss: 20.2063 - val_mse: 20.2063\n",
            "Epoch 726/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 46.7001 - mse: 46.7001 - val_loss: 21.1504 - val_mse: 21.1504\n",
            "Epoch 727/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 57.1944 - mse: 57.1944 - val_loss: 19.2621 - val_mse: 19.2621\n",
            "Epoch 728/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.1636 - mse: 49.1636 - val_loss: 19.7779 - val_mse: 19.7779\n",
            "Epoch 729/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 45.6922 - mse: 45.6922 - val_loss: 19.7598 - val_mse: 19.7598\n",
            "Epoch 730/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 39.7851 - mse: 39.7851 - val_loss: 18.2196 - val_mse: 18.2196\n",
            "Epoch 731/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 44.2448 - mse: 44.2448 - val_loss: 19.1853 - val_mse: 19.1853\n",
            "Epoch 732/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.4252 - mse: 49.4252 - val_loss: 18.3806 - val_mse: 18.3806\n",
            "Epoch 733/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 47.8522 - mse: 47.8522 - val_loss: 20.5943 - val_mse: 20.5943\n",
            "Epoch 734/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.7328 - mse: 41.7328 - val_loss: 18.0816 - val_mse: 18.0816\n",
            "Epoch 735/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 51.3949 - mse: 51.3949 - val_loss: 23.9176 - val_mse: 23.9176\n",
            "Epoch 736/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 53.5477 - mse: 53.5477 - val_loss: 19.5453 - val_mse: 19.5453\n",
            "Epoch 737/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 48.2572 - mse: 48.2572 - val_loss: 18.7882 - val_mse: 18.7882\n",
            "Epoch 738/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.4419 - mse: 43.4419 - val_loss: 18.7816 - val_mse: 18.7816\n",
            "Epoch 739/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 45.1367 - mse: 45.1367 - val_loss: 19.9985 - val_mse: 19.9985\n",
            "Epoch 740/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.4326 - mse: 39.4326 - val_loss: 19.7024 - val_mse: 19.7024\n",
            "Epoch 741/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.0195 - mse: 43.0195 - val_loss: 20.2156 - val_mse: 20.2156\n",
            "Epoch 742/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.7523 - mse: 46.7523 - val_loss: 22.4423 - val_mse: 22.4423\n",
            "Epoch 743/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 53.5752 - mse: 53.5752 - val_loss: 17.6048 - val_mse: 17.6048\n",
            "Epoch 744/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.6642 - mse: 46.6642 - val_loss: 18.1369 - val_mse: 18.1369\n",
            "Epoch 745/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.8676 - mse: 38.8676 - val_loss: 19.0905 - val_mse: 19.0905\n",
            "Epoch 746/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.1255 - mse: 39.1255 - val_loss: 23.9633 - val_mse: 23.9633\n",
            "Epoch 747/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 36.2016 - mse: 36.2016 - val_loss: 17.0409 - val_mse: 17.0409\n",
            "Epoch 748/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 43.5338 - mse: 43.5338 - val_loss: 19.8447 - val_mse: 19.8447\n",
            "Epoch 749/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 51.6364 - mse: 51.6364 - val_loss: 24.1189 - val_mse: 24.1189\n",
            "Epoch 750/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.9904 - mse: 41.9904 - val_loss: 19.6648 - val_mse: 19.6648\n",
            "Epoch 751/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.0045 - mse: 49.0045 - val_loss: 22.8375 - val_mse: 22.8375\n",
            "Epoch 752/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.9306 - mse: 42.9306 - val_loss: 23.5960 - val_mse: 23.5960\n",
            "Epoch 753/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.3069 - mse: 42.3069 - val_loss: 18.0328 - val_mse: 18.0328\n",
            "Epoch 754/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 45.0734 - mse: 45.0734 - val_loss: 18.8758 - val_mse: 18.8758\n",
            "Epoch 755/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 48.5626 - mse: 48.5626 - val_loss: 19.7453 - val_mse: 19.7453\n",
            "Epoch 756/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 48.0526 - mse: 48.0526 - val_loss: 20.7304 - val_mse: 20.7304\n",
            "Epoch 757/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 42.9382 - mse: 42.9382 - val_loss: 25.4236 - val_mse: 25.4236\n",
            "Epoch 758/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 48.6052 - mse: 48.6052 - val_loss: 20.4172 - val_mse: 20.4172\n",
            "Epoch 759/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.8314 - mse: 38.8314 - val_loss: 18.4549 - val_mse: 18.4549\n",
            "Epoch 760/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.3705 - mse: 41.3705 - val_loss: 19.3985 - val_mse: 19.3985\n",
            "Epoch 761/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 40.4234 - mse: 40.4234 - val_loss: 19.0998 - val_mse: 19.0998\n",
            "Epoch 762/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 39.9650 - mse: 39.9650 - val_loss: 24.8393 - val_mse: 24.8393\n",
            "Epoch 763/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 40.8753 - mse: 40.8753 - val_loss: 18.9915 - val_mse: 18.9915\n",
            "Epoch 764/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 45.0639 - mse: 45.0639 - val_loss: 19.3657 - val_mse: 19.3657\n",
            "Epoch 765/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 48.0486 - mse: 48.0486 - val_loss: 25.6176 - val_mse: 25.6176\n",
            "Epoch 766/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 42.9242 - mse: 42.9242 - val_loss: 18.3504 - val_mse: 18.3504\n",
            "Epoch 767/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 45.7739 - mse: 45.7739 - val_loss: 19.0529 - val_mse: 19.0529\n",
            "Epoch 768/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 46.5537 - mse: 46.5537 - val_loss: 19.5054 - val_mse: 19.5054\n",
            "Epoch 769/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 46.3870 - mse: 46.3870 - val_loss: 21.0970 - val_mse: 21.0970\n",
            "Epoch 770/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.6023 - mse: 41.6023 - val_loss: 23.6605 - val_mse: 23.6605\n",
            "Epoch 771/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.7159 - mse: 41.7159 - val_loss: 21.1100 - val_mse: 21.1100\n",
            "Epoch 772/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.9176 - mse: 46.9176 - val_loss: 19.4428 - val_mse: 19.4428\n",
            "Epoch 773/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 37.6816 - mse: 37.6816 - val_loss: 23.0282 - val_mse: 23.0282\n",
            "Epoch 774/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 46.3284 - mse: 46.3284 - val_loss: 21.6835 - val_mse: 21.6835\n",
            "Epoch 775/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.9456 - mse: 38.9456 - val_loss: 20.5506 - val_mse: 20.5506\n",
            "Epoch 776/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 40.2488 - mse: 40.2488 - val_loss: 20.2463 - val_mse: 20.2463\n",
            "Epoch 777/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 38.9568 - mse: 38.9568 - val_loss: 21.2779 - val_mse: 21.2779\n",
            "Epoch 778/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.8894 - mse: 38.8894 - val_loss: 23.4259 - val_mse: 23.4259\n",
            "Epoch 779/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 45.3833 - mse: 45.3833 - val_loss: 20.2199 - val_mse: 20.2199\n",
            "Epoch 780/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 47.4104 - mse: 47.4104 - val_loss: 19.2575 - val_mse: 19.2575\n",
            "Epoch 781/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.0344 - mse: 42.0344 - val_loss: 18.7527 - val_mse: 18.7527\n",
            "Epoch 782/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 50.0748 - mse: 50.0748 - val_loss: 21.8211 - val_mse: 21.8211\n",
            "Epoch 783/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 44.8918 - mse: 44.8918 - val_loss: 21.6263 - val_mse: 21.6263\n",
            "Epoch 784/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.4870 - mse: 41.4870 - val_loss: 17.4597 - val_mse: 17.4597\n",
            "Epoch 785/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 43.1361 - mse: 43.1361 - val_loss: 18.1527 - val_mse: 18.1527\n",
            "Epoch 786/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.6935 - mse: 46.6935 - val_loss: 24.6226 - val_mse: 24.6226\n",
            "Epoch 787/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 48.4451 - mse: 48.4451 - val_loss: 21.7435 - val_mse: 21.7435\n",
            "Epoch 788/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.0969 - mse: 41.0969 - val_loss: 18.1483 - val_mse: 18.1483\n",
            "Epoch 789/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.9149 - mse: 41.9149 - val_loss: 20.7393 - val_mse: 20.7393\n",
            "Epoch 790/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 42.7606 - mse: 42.7606 - val_loss: 23.0636 - val_mse: 23.0636\n",
            "Epoch 791/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 41.2558 - mse: 41.2558 - val_loss: 20.0604 - val_mse: 20.0604\n",
            "Epoch 792/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.6405 - mse: 38.6405 - val_loss: 22.9215 - val_mse: 22.9215\n",
            "Epoch 793/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 47.0880 - mse: 47.0880 - val_loss: 23.5270 - val_mse: 23.5270\n",
            "Epoch 794/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 47.1105 - mse: 47.1105 - val_loss: 18.4474 - val_mse: 18.4474\n",
            "Epoch 795/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 47.3717 - mse: 47.3717 - val_loss: 19.1450 - val_mse: 19.1450\n",
            "Epoch 796/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 43.7651 - mse: 43.7651 - val_loss: 25.8908 - val_mse: 25.8908\n",
            "Epoch 797/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.6123 - mse: 49.6123 - val_loss: 22.1480 - val_mse: 22.1480\n",
            "Epoch 798/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.3431 - mse: 49.3431 - val_loss: 25.6820 - val_mse: 25.6820\n",
            "Epoch 799/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 47.0418 - mse: 47.0418 - val_loss: 19.3083 - val_mse: 19.3083\n",
            "Epoch 800/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 42.2619 - mse: 42.2619 - val_loss: 17.7359 - val_mse: 17.7359\n",
            "Epoch 801/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.0120 - mse: 43.0120 - val_loss: 22.1024 - val_mse: 22.1024\n",
            "Epoch 802/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 43.0077 - mse: 43.0077 - val_loss: 19.0044 - val_mse: 19.0044\n",
            "Epoch 803/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 45.1368 - mse: 45.1368 - val_loss: 18.2860 - val_mse: 18.2860\n",
            "Epoch 804/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 46.5088 - mse: 46.5088 - val_loss: 25.8225 - val_mse: 25.8225\n",
            "Epoch 805/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.8417 - mse: 41.8417 - val_loss: 19.9030 - val_mse: 19.9030\n",
            "Epoch 806/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 44.8376 - mse: 44.8376 - val_loss: 19.3657 - val_mse: 19.3657\n",
            "Epoch 807/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 40.7559 - mse: 40.7559 - val_loss: 20.8568 - val_mse: 20.8568\n",
            "Epoch 808/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 43.3585 - mse: 43.3585 - val_loss: 18.7373 - val_mse: 18.7373\n",
            "Epoch 809/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 47.8327 - mse: 47.8327 - val_loss: 18.9774 - val_mse: 18.9774\n",
            "Epoch 810/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.9961 - mse: 36.9961 - val_loss: 17.0567 - val_mse: 17.0567\n",
            "Epoch 811/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 40.1281 - mse: 40.1281 - val_loss: 20.5675 - val_mse: 20.5675\n",
            "Epoch 812/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 42.8868 - mse: 42.8868 - val_loss: 20.1149 - val_mse: 20.1149\n",
            "Epoch 813/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 45.8984 - mse: 45.8984 - val_loss: 19.7094 - val_mse: 19.7094\n",
            "Epoch 814/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 35.9872 - mse: 35.9872 - val_loss: 18.9507 - val_mse: 18.9507\n",
            "Epoch 815/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 46.5081 - mse: 46.5081 - val_loss: 19.7211 - val_mse: 19.7211\n",
            "Epoch 816/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 43.1100 - mse: 43.1100 - val_loss: 18.3946 - val_mse: 18.3946\n",
            "Epoch 817/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.2363 - mse: 42.2363 - val_loss: 21.4820 - val_mse: 21.4820\n",
            "Epoch 818/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.9312 - mse: 42.9312 - val_loss: 24.6252 - val_mse: 24.6252\n",
            "Epoch 819/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.9560 - mse: 35.9560 - val_loss: 19.3897 - val_mse: 19.3897\n",
            "Epoch 820/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 47.4694 - mse: 47.4694 - val_loss: 20.1833 - val_mse: 20.1833\n",
            "Epoch 821/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 42.3070 - mse: 42.3070 - val_loss: 24.0241 - val_mse: 24.0241\n",
            "Epoch 822/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 36.5227 - mse: 36.5227 - val_loss: 21.4206 - val_mse: 21.4206\n",
            "Epoch 823/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 40.7442 - mse: 40.7442 - val_loss: 17.8491 - val_mse: 17.8491\n",
            "Epoch 824/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 41.9396 - mse: 41.9396 - val_loss: 20.0094 - val_mse: 20.0094\n",
            "Epoch 825/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 40.4695 - mse: 40.4695 - val_loss: 17.9990 - val_mse: 17.9990\n",
            "Epoch 826/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 40.3006 - mse: 40.3006 - val_loss: 17.0162 - val_mse: 17.0162\n",
            "Epoch 827/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 44.5963 - mse: 44.5963 - val_loss: 17.7652 - val_mse: 17.7652\n",
            "Epoch 828/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 45.9546 - mse: 45.9546 - val_loss: 18.9512 - val_mse: 18.9512\n",
            "Epoch 829/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.0240 - mse: 37.0240 - val_loss: 18.4786 - val_mse: 18.4786\n",
            "Epoch 830/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 40.1778 - mse: 40.1778 - val_loss: 22.3762 - val_mse: 22.3762\n",
            "Epoch 831/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.3755 - mse: 41.3755 - val_loss: 18.9069 - val_mse: 18.9069\n",
            "Epoch 832/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.1895 - mse: 46.1895 - val_loss: 27.9062 - val_mse: 27.9062\n",
            "Epoch 833/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.0176 - mse: 38.0176 - val_loss: 20.4337 - val_mse: 20.4337\n",
            "Epoch 834/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 44.9786 - mse: 44.9786 - val_loss: 28.7352 - val_mse: 28.7352\n",
            "Epoch 835/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.6500 - mse: 41.6500 - val_loss: 21.5557 - val_mse: 21.5557\n",
            "Epoch 836/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.6033 - mse: 38.6033 - val_loss: 20.5207 - val_mse: 20.5207\n",
            "Epoch 837/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.3706 - mse: 38.3706 - val_loss: 17.8551 - val_mse: 17.8551\n",
            "Epoch 838/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 43.2692 - mse: 43.2692 - val_loss: 17.9276 - val_mse: 17.9276\n",
            "Epoch 839/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.0626 - mse: 35.0626 - val_loss: 23.7710 - val_mse: 23.7710\n",
            "Epoch 840/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 47.5475 - mse: 47.5475 - val_loss: 25.9625 - val_mse: 25.9625\n",
            "Epoch 841/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 39.7121 - mse: 39.7121 - val_loss: 22.9913 - val_mse: 22.9913\n",
            "Epoch 842/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 40.6555 - mse: 40.6555 - val_loss: 17.6905 - val_mse: 17.6905\n",
            "Epoch 843/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 39.5922 - mse: 39.5922 - val_loss: 17.2936 - val_mse: 17.2936\n",
            "Epoch 844/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 40.6286 - mse: 40.6286 - val_loss: 17.4829 - val_mse: 17.4829\n",
            "Epoch 845/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 44.0076 - mse: 44.0076 - val_loss: 17.9966 - val_mse: 17.9966\n",
            "Epoch 846/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 39.9948 - mse: 39.9948 - val_loss: 18.0822 - val_mse: 18.0822\n",
            "Epoch 847/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.0767 - mse: 39.0767 - val_loss: 17.5295 - val_mse: 17.5295\n",
            "Epoch 848/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.8741 - mse: 34.8741 - val_loss: 21.9693 - val_mse: 21.9693\n",
            "Epoch 849/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.1806 - mse: 42.1806 - val_loss: 18.4106 - val_mse: 18.4106\n",
            "Epoch 850/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 48.8183 - mse: 48.8183 - val_loss: 19.6684 - val_mse: 19.6684\n",
            "Epoch 851/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.3393 - mse: 38.3393 - val_loss: 18.4561 - val_mse: 18.4561\n",
            "Epoch 852/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 44.4861 - mse: 44.4861 - val_loss: 21.3412 - val_mse: 21.3412\n",
            "Epoch 853/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 43.0418 - mse: 43.0418 - val_loss: 19.1405 - val_mse: 19.1405\n",
            "Epoch 854/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 39.1162 - mse: 39.1162 - val_loss: 18.0655 - val_mse: 18.0655\n",
            "Epoch 855/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 37.3543 - mse: 37.3543 - val_loss: 17.7885 - val_mse: 17.7885\n",
            "Epoch 856/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.0619 - mse: 42.0619 - val_loss: 22.3137 - val_mse: 22.3137\n",
            "Epoch 857/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.0015 - mse: 42.0015 - val_loss: 19.6321 - val_mse: 19.6321\n",
            "Epoch 858/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 47.3912 - mse: 47.3912 - val_loss: 20.2045 - val_mse: 20.2045\n",
            "Epoch 859/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 33.2823 - mse: 33.2823 - val_loss: 19.7110 - val_mse: 19.7110\n",
            "Epoch 860/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 43.9059 - mse: 43.9059 - val_loss: 19.4764 - val_mse: 19.4764\n",
            "Epoch 861/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 39.3700 - mse: 39.3700 - val_loss: 23.4604 - val_mse: 23.4604\n",
            "Epoch 862/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 49.6970 - mse: 49.6970 - val_loss: 24.6444 - val_mse: 24.6444\n",
            "Epoch 863/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.4814 - mse: 41.4814 - val_loss: 19.9681 - val_mse: 19.9681\n",
            "Epoch 864/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.2685 - mse: 41.2685 - val_loss: 20.4929 - val_mse: 20.4929\n",
            "Epoch 865/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.4093 - mse: 41.4093 - val_loss: 21.6061 - val_mse: 21.6061\n",
            "Epoch 866/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.4803 - mse: 39.4803 - val_loss: 26.3672 - val_mse: 26.3672\n",
            "Epoch 867/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.0243 - mse: 42.0243 - val_loss: 19.1389 - val_mse: 19.1389\n",
            "Epoch 868/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.7932 - mse: 41.7932 - val_loss: 17.5120 - val_mse: 17.5120\n",
            "Epoch 869/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.9566 - mse: 37.9566 - val_loss: 17.5703 - val_mse: 17.5703\n",
            "Epoch 870/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.7565 - mse: 39.7565 - val_loss: 18.4723 - val_mse: 18.4723\n",
            "Epoch 871/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 35.8126 - mse: 35.8126 - val_loss: 27.0700 - val_mse: 27.0700\n",
            "Epoch 872/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 42.9919 - mse: 42.9919 - val_loss: 17.7843 - val_mse: 17.7843\n",
            "Epoch 873/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.3985 - mse: 37.3985 - val_loss: 20.1385 - val_mse: 20.1385\n",
            "Epoch 874/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 42.8896 - mse: 42.8896 - val_loss: 17.6211 - val_mse: 17.6211\n",
            "Epoch 875/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.7002 - mse: 35.7002 - val_loss: 17.1427 - val_mse: 17.1427\n",
            "Epoch 876/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 50.3568 - mse: 50.3568 - val_loss: 19.2548 - val_mse: 19.2548\n",
            "Epoch 877/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.8873 - mse: 37.8873 - val_loss: 17.8308 - val_mse: 17.8308\n",
            "Epoch 878/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.5744 - mse: 33.5744 - val_loss: 18.5822 - val_mse: 18.5822\n",
            "Epoch 879/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.7431 - mse: 43.7431 - val_loss: 18.3277 - val_mse: 18.3277\n",
            "Epoch 880/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.6283 - mse: 36.6283 - val_loss: 17.1269 - val_mse: 17.1269\n",
            "Epoch 881/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 37.7220 - mse: 37.7220 - val_loss: 18.2728 - val_mse: 18.2728\n",
            "Epoch 882/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.5141 - mse: 38.5141 - val_loss: 16.5444 - val_mse: 16.5444\n",
            "Epoch 883/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.6301 - mse: 37.6301 - val_loss: 30.7464 - val_mse: 30.7464\n",
            "Epoch 884/10000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 39.2412 - mse: 39.2412 - val_loss: 18.4398 - val_mse: 18.4398\n",
            "Epoch 885/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.3901 - mse: 39.3901 - val_loss: 18.1942 - val_mse: 18.1942\n",
            "Epoch 886/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.5975 - mse: 38.5975 - val_loss: 17.2330 - val_mse: 17.2330\n",
            "Epoch 887/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.2505 - mse: 32.2505 - val_loss: 17.9914 - val_mse: 17.9914\n",
            "Epoch 888/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.4963 - mse: 38.4963 - val_loss: 18.9096 - val_mse: 18.9096\n",
            "Epoch 889/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 36.5725 - mse: 36.5725 - val_loss: 17.2991 - val_mse: 17.2991\n",
            "Epoch 890/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.5013 - mse: 35.5013 - val_loss: 17.6372 - val_mse: 17.6372\n",
            "Epoch 891/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 37.8873 - mse: 37.8873 - val_loss: 16.6499 - val_mse: 16.6499\n",
            "Epoch 892/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 36.9737 - mse: 36.9737 - val_loss: 19.7132 - val_mse: 19.7132\n",
            "Epoch 893/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 40.2405 - mse: 40.2405 - val_loss: 32.6864 - val_mse: 32.6864\n",
            "Epoch 894/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 45.1532 - mse: 45.1532 - val_loss: 28.7920 - val_mse: 28.7920\n",
            "Epoch 895/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 40.1927 - mse: 40.1927 - val_loss: 17.7815 - val_mse: 17.7815\n",
            "Epoch 896/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 39.2374 - mse: 39.2374 - val_loss: 19.2276 - val_mse: 19.2276\n",
            "Epoch 897/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.2549 - mse: 37.2549 - val_loss: 18.9109 - val_mse: 18.9109\n",
            "Epoch 898/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.9558 - mse: 38.9558 - val_loss: 16.8517 - val_mse: 16.8517\n",
            "Epoch 899/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.4823 - mse: 37.4823 - val_loss: 18.3018 - val_mse: 18.3018\n",
            "Epoch 900/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 36.7325 - mse: 36.7325 - val_loss: 19.3251 - val_mse: 19.3251\n",
            "Epoch 901/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.2571 - mse: 36.2571 - val_loss: 17.7214 - val_mse: 17.7214\n",
            "Epoch 902/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 44.9458 - mse: 44.9458 - val_loss: 17.7963 - val_mse: 17.7963\n",
            "Epoch 903/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 40.4452 - mse: 40.4452 - val_loss: 18.2455 - val_mse: 18.2455\n",
            "Epoch 904/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 44.4416 - mse: 44.4416 - val_loss: 17.4445 - val_mse: 17.4445\n",
            "Epoch 905/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.1681 - mse: 35.1681 - val_loss: 25.7165 - val_mse: 25.7165\n",
            "Epoch 906/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.2340 - mse: 42.2340 - val_loss: 27.9487 - val_mse: 27.9487\n",
            "Epoch 907/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.3727 - mse: 35.3727 - val_loss: 18.4229 - val_mse: 18.4229\n",
            "Epoch 908/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.5191 - mse: 35.5191 - val_loss: 18.3107 - val_mse: 18.3107\n",
            "Epoch 909/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.1609 - mse: 41.1609 - val_loss: 16.7002 - val_mse: 16.7002\n",
            "Epoch 910/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.1988 - mse: 41.1988 - val_loss: 18.1977 - val_mse: 18.1977\n",
            "Epoch 911/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 34.6834 - mse: 34.6834 - val_loss: 16.7546 - val_mse: 16.7546\n",
            "Epoch 912/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.7807 - mse: 32.7807 - val_loss: 15.9539 - val_mse: 15.9539\n",
            "Epoch 913/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.3264 - mse: 35.3264 - val_loss: 21.4933 - val_mse: 21.4933\n",
            "Epoch 914/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.0482 - mse: 41.0482 - val_loss: 18.5733 - val_mse: 18.5733\n",
            "Epoch 915/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 36.2371 - mse: 36.2371 - val_loss: 23.7889 - val_mse: 23.7889\n",
            "Epoch 916/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.4097 - mse: 34.4097 - val_loss: 19.4294 - val_mse: 19.4294\n",
            "Epoch 917/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.9953 - mse: 34.9953 - val_loss: 24.3684 - val_mse: 24.3684\n",
            "Epoch 918/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.1525 - mse: 39.1525 - val_loss: 23.3902 - val_mse: 23.3902\n",
            "Epoch 919/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.5695 - mse: 41.5695 - val_loss: 17.0891 - val_mse: 17.0891\n",
            "Epoch 920/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.2677 - mse: 32.2677 - val_loss: 16.1166 - val_mse: 16.1166\n",
            "Epoch 921/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.3280 - mse: 41.3280 - val_loss: 18.8086 - val_mse: 18.8086\n",
            "Epoch 922/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.4956 - mse: 37.4956 - val_loss: 18.7674 - val_mse: 18.7674\n",
            "Epoch 923/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 44.6178 - mse: 44.6178 - val_loss: 24.5989 - val_mse: 24.5989\n",
            "Epoch 924/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 49.3400 - mse: 49.3400 - val_loss: 30.6949 - val_mse: 30.6949\n",
            "Epoch 925/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 45.3796 - mse: 45.3796 - val_loss: 19.5643 - val_mse: 19.5643\n",
            "Epoch 926/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 46.1165 - mse: 46.1165 - val_loss: 19.2534 - val_mse: 19.2534\n",
            "Epoch 927/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.1405 - mse: 34.1405 - val_loss: 16.3672 - val_mse: 16.3672\n",
            "Epoch 928/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.9261 - mse: 41.9261 - val_loss: 16.6202 - val_mse: 16.6202\n",
            "Epoch 929/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 42.6317 - mse: 42.6317 - val_loss: 16.5765 - val_mse: 16.5765\n",
            "Epoch 930/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.3736 - mse: 34.3736 - val_loss: 17.9679 - val_mse: 17.9679\n",
            "Epoch 931/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.6916 - mse: 33.6916 - val_loss: 19.1747 - val_mse: 19.1747\n",
            "Epoch 932/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 45.7962 - mse: 45.7962 - val_loss: 18.0305 - val_mse: 18.0305\n",
            "Epoch 933/10000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 39.2009 - mse: 39.2009 - val_loss: 21.4425 - val_mse: 21.4425\n",
            "Epoch 934/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.7385 - mse: 38.7385 - val_loss: 17.8353 - val_mse: 17.8353\n",
            "Epoch 935/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 44.3261 - mse: 44.3261 - val_loss: 17.0922 - val_mse: 17.0922\n",
            "Epoch 936/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.0890 - mse: 43.0890 - val_loss: 18.5211 - val_mse: 18.5211\n",
            "Epoch 937/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.6628 - mse: 39.6628 - val_loss: 17.9488 - val_mse: 17.9488\n",
            "Epoch 938/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.6873 - mse: 37.6873 - val_loss: 17.4681 - val_mse: 17.4681\n",
            "Epoch 939/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.9488 - mse: 41.9488 - val_loss: 15.9558 - val_mse: 15.9558\n",
            "Epoch 940/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.2986 - mse: 34.2986 - val_loss: 18.3999 - val_mse: 18.3999\n",
            "Epoch 941/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.6532 - mse: 37.6532 - val_loss: 16.6314 - val_mse: 16.6314\n",
            "Epoch 942/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 40.3987 - mse: 40.3987 - val_loss: 19.2456 - val_mse: 19.2456\n",
            "Epoch 943/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 37.0520 - mse: 37.0520 - val_loss: 18.5989 - val_mse: 18.5989\n",
            "Epoch 944/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.0850 - mse: 39.0850 - val_loss: 16.3500 - val_mse: 16.3500\n",
            "Epoch 945/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.8854 - mse: 33.8854 - val_loss: 16.2965 - val_mse: 16.2965\n",
            "Epoch 946/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 37.4783 - mse: 37.4783 - val_loss: 18.1292 - val_mse: 18.1292\n",
            "Epoch 947/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.1758 - mse: 34.1758 - val_loss: 21.2451 - val_mse: 21.2451\n",
            "Epoch 948/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.6325 - mse: 36.6325 - val_loss: 19.3868 - val_mse: 19.3868\n",
            "Epoch 949/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.8641 - mse: 35.8641 - val_loss: 16.6969 - val_mse: 16.6969\n",
            "Epoch 950/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.3741 - mse: 36.3741 - val_loss: 19.2853 - val_mse: 19.2853\n",
            "Epoch 951/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.9602 - mse: 35.9602 - val_loss: 16.2221 - val_mse: 16.2221\n",
            "Epoch 952/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 36.3902 - mse: 36.3902 - val_loss: 18.7622 - val_mse: 18.7622\n",
            "Epoch 953/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 39.4548 - mse: 39.4548 - val_loss: 16.6575 - val_mse: 16.6575\n",
            "Epoch 954/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 40.5833 - mse: 40.5833 - val_loss: 17.4160 - val_mse: 17.4160\n",
            "Epoch 955/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.2500 - mse: 39.2500 - val_loss: 18.1551 - val_mse: 18.1551\n",
            "Epoch 956/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.5996 - mse: 30.5996 - val_loss: 19.9215 - val_mse: 19.9215\n",
            "Epoch 957/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.0260 - mse: 35.0260 - val_loss: 19.1375 - val_mse: 19.1375\n",
            "Epoch 958/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.1697 - mse: 33.1697 - val_loss: 16.8829 - val_mse: 16.8829\n",
            "Epoch 959/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.9987 - mse: 34.9987 - val_loss: 15.8448 - val_mse: 15.8448\n",
            "Epoch 960/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.9958 - mse: 43.9958 - val_loss: 19.7421 - val_mse: 19.7421\n",
            "Epoch 961/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.3794 - mse: 43.3794 - val_loss: 21.6647 - val_mse: 21.6647\n",
            "Epoch 962/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.6601 - mse: 30.6601 - val_loss: 21.5970 - val_mse: 21.5970\n",
            "Epoch 963/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.6799 - mse: 41.6799 - val_loss: 24.4001 - val_mse: 24.4001\n",
            "Epoch 964/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.9569 - mse: 38.9569 - val_loss: 23.0663 - val_mse: 23.0663\n",
            "Epoch 965/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.5383 - mse: 37.5383 - val_loss: 17.9897 - val_mse: 17.9897\n",
            "Epoch 966/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.8331 - mse: 37.8331 - val_loss: 18.3981 - val_mse: 18.3981\n",
            "Epoch 967/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.5283 - mse: 35.5283 - val_loss: 19.2952 - val_mse: 19.2952\n",
            "Epoch 968/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.4482 - mse: 36.4482 - val_loss: 16.9449 - val_mse: 16.9449\n",
            "Epoch 969/10000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 39.3251 - mse: 39.3251 - val_loss: 18.0781 - val_mse: 18.0781\n",
            "Epoch 970/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 46.7256 - mse: 46.7256 - val_loss: 19.5386 - val_mse: 19.5386\n",
            "Epoch 971/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.3036 - mse: 31.3036 - val_loss: 18.6714 - val_mse: 18.6714\n",
            "Epoch 972/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.3609 - mse: 38.3609 - val_loss: 18.7783 - val_mse: 18.7783\n",
            "Epoch 973/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.0252 - mse: 38.0252 - val_loss: 20.1821 - val_mse: 20.1821\n",
            "Epoch 974/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 43.8379 - mse: 43.8379 - val_loss: 22.7911 - val_mse: 22.7911\n",
            "Epoch 975/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.8070 - mse: 33.8070 - val_loss: 19.5905 - val_mse: 19.5905\n",
            "Epoch 976/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.0813 - mse: 34.0813 - val_loss: 16.9548 - val_mse: 16.9548\n",
            "Epoch 977/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.0136 - mse: 32.0136 - val_loss: 17.6991 - val_mse: 17.6991\n",
            "Epoch 978/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.5363 - mse: 41.5363 - val_loss: 18.1159 - val_mse: 18.1159\n",
            "Epoch 979/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.7455 - mse: 37.7455 - val_loss: 17.0959 - val_mse: 17.0959\n",
            "Epoch 980/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.6840 - mse: 32.6840 - val_loss: 17.3277 - val_mse: 17.3277\n",
            "Epoch 981/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.9795 - mse: 34.9795 - val_loss: 16.8983 - val_mse: 16.8983\n",
            "Epoch 982/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 36.1933 - mse: 36.1933 - val_loss: 17.2768 - val_mse: 17.2768\n",
            "Epoch 983/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.1274 - mse: 39.1274 - val_loss: 17.1082 - val_mse: 17.1082\n",
            "Epoch 984/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.1992 - mse: 36.1992 - val_loss: 16.5323 - val_mse: 16.5323\n",
            "Epoch 985/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 49.3221 - mse: 49.3221 - val_loss: 20.4944 - val_mse: 20.4944\n",
            "Epoch 986/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.3628 - mse: 32.3628 - val_loss: 18.2767 - val_mse: 18.2767\n",
            "Epoch 987/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.1505 - mse: 41.1505 - val_loss: 15.7104 - val_mse: 15.7104\n",
            "Epoch 988/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.2357 - mse: 39.2357 - val_loss: 18.2798 - val_mse: 18.2798\n",
            "Epoch 989/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 36.0751 - mse: 36.0751 - val_loss: 17.3964 - val_mse: 17.3964\n",
            "Epoch 990/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.3691 - mse: 31.3691 - val_loss: 16.0999 - val_mse: 16.0999\n",
            "Epoch 991/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 41.3205 - mse: 41.3205 - val_loss: 17.9588 - val_mse: 17.9588\n",
            "Epoch 992/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.9489 - mse: 28.9489 - val_loss: 20.1090 - val_mse: 20.1090\n",
            "Epoch 993/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.1326 - mse: 32.1326 - val_loss: 18.3869 - val_mse: 18.3869\n",
            "Epoch 994/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.6223 - mse: 39.6223 - val_loss: 20.2638 - val_mse: 20.2638\n",
            "Epoch 995/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 37.7055 - mse: 37.7055 - val_loss: 17.4567 - val_mse: 17.4567\n",
            "Epoch 996/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.9609 - mse: 35.9609 - val_loss: 17.8254 - val_mse: 17.8254\n",
            "Epoch 997/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 39.1455 - mse: 39.1455 - val_loss: 19.6199 - val_mse: 19.6199\n",
            "Epoch 998/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.8827 - mse: 33.8827 - val_loss: 17.1402 - val_mse: 17.1402\n",
            "Epoch 999/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 36.6849 - mse: 36.6849 - val_loss: 19.2179 - val_mse: 19.2179\n",
            "Epoch 1000/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 42.1059 - mse: 42.1059 - val_loss: 18.0064 - val_mse: 18.0064\n",
            "Epoch 1001/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 28.7769 - mse: 28.7769 - val_loss: 17.9740 - val_mse: 17.9740\n",
            "Epoch 1002/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.5380 - mse: 34.5380 - val_loss: 17.4558 - val_mse: 17.4558\n",
            "Epoch 1003/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 40.5469 - mse: 40.5469 - val_loss: 19.1559 - val_mse: 19.1559\n",
            "Epoch 1004/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.6468 - mse: 37.6468 - val_loss: 18.9330 - val_mse: 18.9330\n",
            "Epoch 1005/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.3354 - mse: 29.3354 - val_loss: 18.7746 - val_mse: 18.7746\n",
            "Epoch 1006/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.3114 - mse: 36.3114 - val_loss: 20.5561 - val_mse: 20.5561\n",
            "Epoch 1007/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.1206 - mse: 33.1206 - val_loss: 18.5547 - val_mse: 18.5547\n",
            "Epoch 1008/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.9128 - mse: 32.9128 - val_loss: 16.8187 - val_mse: 16.8187\n",
            "Epoch 1009/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.1275 - mse: 37.1275 - val_loss: 26.4229 - val_mse: 26.4229\n",
            "Epoch 1010/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.6773 - mse: 38.6773 - val_loss: 18.4303 - val_mse: 18.4303\n",
            "Epoch 1011/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.8103 - mse: 29.8103 - val_loss: 15.9567 - val_mse: 15.9567\n",
            "Epoch 1012/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 39.8870 - mse: 39.8870 - val_loss: 17.3054 - val_mse: 17.3054\n",
            "Epoch 1013/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.8804 - mse: 32.8804 - val_loss: 19.3334 - val_mse: 19.3334\n",
            "Epoch 1014/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.6761 - mse: 34.6761 - val_loss: 16.7960 - val_mse: 16.7960\n",
            "Epoch 1015/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.5790 - mse: 34.5790 - val_loss: 17.6358 - val_mse: 17.6358\n",
            "Epoch 1016/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.6062 - mse: 30.6062 - val_loss: 18.3475 - val_mse: 18.3475\n",
            "Epoch 1017/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 39.0058 - mse: 39.0058 - val_loss: 17.2834 - val_mse: 17.2834\n",
            "Epoch 1018/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.2589 - mse: 36.2589 - val_loss: 17.2111 - val_mse: 17.2111\n",
            "Epoch 1019/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.8270 - mse: 35.8270 - val_loss: 17.0089 - val_mse: 17.0089\n",
            "Epoch 1020/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.1641 - mse: 38.1641 - val_loss: 16.8275 - val_mse: 16.8275\n",
            "Epoch 1021/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.2898 - mse: 30.2898 - val_loss: 21.4780 - val_mse: 21.4780\n",
            "Epoch 1022/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.9191 - mse: 32.9191 - val_loss: 25.5804 - val_mse: 25.5804\n",
            "Epoch 1023/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.2627 - mse: 36.2627 - val_loss: 18.9735 - val_mse: 18.9735\n",
            "Epoch 1024/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.7617 - mse: 33.7617 - val_loss: 19.3131 - val_mse: 19.3131\n",
            "Epoch 1025/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.9194 - mse: 32.9194 - val_loss: 16.9823 - val_mse: 16.9823\n",
            "Epoch 1026/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.1520 - mse: 33.1520 - val_loss: 20.1254 - val_mse: 20.1254\n",
            "Epoch 1027/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.6787 - mse: 26.6787 - val_loss: 17.8125 - val_mse: 17.8125\n",
            "Epoch 1028/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.4250 - mse: 33.4250 - val_loss: 18.1788 - val_mse: 18.1788\n",
            "Epoch 1029/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.7122 - mse: 35.7122 - val_loss: 18.3274 - val_mse: 18.3274\n",
            "Epoch 1030/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.5225 - mse: 36.5225 - val_loss: 19.0426 - val_mse: 19.0426\n",
            "Epoch 1031/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.2237 - mse: 31.2237 - val_loss: 16.4268 - val_mse: 16.4268\n",
            "Epoch 1032/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 37.5691 - mse: 37.5691 - val_loss: 19.7220 - val_mse: 19.7220\n",
            "Epoch 1033/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.6537 - mse: 33.6537 - val_loss: 19.5809 - val_mse: 19.5809\n",
            "Epoch 1034/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.9954 - mse: 27.9954 - val_loss: 18.8800 - val_mse: 18.8800\n",
            "Epoch 1035/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.0641 - mse: 30.0641 - val_loss: 18.1022 - val_mse: 18.1022\n",
            "Epoch 1036/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.3186 - mse: 41.3186 - val_loss: 20.4844 - val_mse: 20.4844\n",
            "Epoch 1037/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.2948 - mse: 32.2948 - val_loss: 16.6794 - val_mse: 16.6794\n",
            "Epoch 1038/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.3943 - mse: 35.3943 - val_loss: 17.4809 - val_mse: 17.4809\n",
            "Epoch 1039/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.6326 - mse: 36.6326 - val_loss: 20.2505 - val_mse: 20.2505\n",
            "Epoch 1040/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.7440 - mse: 33.7440 - val_loss: 19.3194 - val_mse: 19.3194\n",
            "Epoch 1041/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.0896 - mse: 33.0896 - val_loss: 21.1798 - val_mse: 21.1798\n",
            "Epoch 1042/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.0421 - mse: 36.0421 - val_loss: 17.1872 - val_mse: 17.1872\n",
            "Epoch 1043/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.8189 - mse: 30.8189 - val_loss: 19.0165 - val_mse: 19.0165\n",
            "Epoch 1044/10000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 34.1730 - mse: 34.1730 - val_loss: 17.0871 - val_mse: 17.0871\n",
            "Epoch 1045/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.9544 - mse: 38.9544 - val_loss: 22.3516 - val_mse: 22.3516\n",
            "Epoch 1046/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 43.7960 - mse: 43.7960 - val_loss: 31.1266 - val_mse: 31.1266\n",
            "Epoch 1047/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.3442 - mse: 34.3442 - val_loss: 23.7745 - val_mse: 23.7745\n",
            "Epoch 1048/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.3113 - mse: 30.3113 - val_loss: 19.0349 - val_mse: 19.0349\n",
            "Epoch 1049/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.4331 - mse: 31.4331 - val_loss: 15.0465 - val_mse: 15.0465\n",
            "Epoch 1050/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.0295 - mse: 41.0295 - val_loss: 19.9979 - val_mse: 19.9979\n",
            "Epoch 1051/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.2490 - mse: 38.2490 - val_loss: 20.9919 - val_mse: 20.9919\n",
            "Epoch 1052/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.2544 - mse: 39.2544 - val_loss: 15.4518 - val_mse: 15.4518\n",
            "Epoch 1053/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.0044 - mse: 36.0044 - val_loss: 15.7316 - val_mse: 15.7316\n",
            "Epoch 1054/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 41.8179 - mse: 41.8179 - val_loss: 17.1781 - val_mse: 17.1781\n",
            "Epoch 1055/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.6668 - mse: 34.6668 - val_loss: 17.7455 - val_mse: 17.7455\n",
            "Epoch 1056/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.8849 - mse: 31.8849 - val_loss: 20.9758 - val_mse: 20.9758\n",
            "Epoch 1057/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.5509 - mse: 27.5509 - val_loss: 18.8982 - val_mse: 18.8982\n",
            "Epoch 1058/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 39.0848 - mse: 39.0848 - val_loss: 21.9637 - val_mse: 21.9637\n",
            "Epoch 1059/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.0436 - mse: 34.0436 - val_loss: 21.0434 - val_mse: 21.0434\n",
            "Epoch 1060/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.9929 - mse: 35.9929 - val_loss: 18.1735 - val_mse: 18.1735\n",
            "Epoch 1061/10000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 38.8829 - mse: 38.8829 - val_loss: 21.1746 - val_mse: 21.1746\n",
            "Epoch 1062/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.8223 - mse: 34.8223 - val_loss: 21.3609 - val_mse: 21.3609\n",
            "Epoch 1063/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.2980 - mse: 28.2980 - val_loss: 17.0896 - val_mse: 17.0896\n",
            "Epoch 1064/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.9160 - mse: 32.9160 - val_loss: 16.9502 - val_mse: 16.9502\n",
            "Epoch 1065/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.3389 - mse: 30.3389 - val_loss: 15.7284 - val_mse: 15.7284\n",
            "Epoch 1066/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.6984 - mse: 30.6984 - val_loss: 18.3971 - val_mse: 18.3971\n",
            "Epoch 1067/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.4123 - mse: 27.4123 - val_loss: 21.5643 - val_mse: 21.5643\n",
            "Epoch 1068/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.2606 - mse: 33.2606 - val_loss: 19.0841 - val_mse: 19.0841\n",
            "Epoch 1069/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.4985 - mse: 32.4985 - val_loss: 16.3355 - val_mse: 16.3355\n",
            "Epoch 1070/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.5006 - mse: 33.5006 - val_loss: 14.9714 - val_mse: 14.9714\n",
            "Epoch 1071/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.7778 - mse: 33.7778 - val_loss: 15.7914 - val_mse: 15.7914\n",
            "Epoch 1072/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.6221 - mse: 32.6221 - val_loss: 17.3255 - val_mse: 17.3255\n",
            "Epoch 1073/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.2371 - mse: 37.2371 - val_loss: 14.9019 - val_mse: 14.9019\n",
            "Epoch 1074/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.2049 - mse: 33.2049 - val_loss: 16.1123 - val_mse: 16.1123\n",
            "Epoch 1075/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.1640 - mse: 35.1640 - val_loss: 16.1809 - val_mse: 16.1809\n",
            "Epoch 1076/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.4999 - mse: 32.4999 - val_loss: 19.8697 - val_mse: 19.8697\n",
            "Epoch 1077/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.4195 - mse: 30.4195 - val_loss: 20.1444 - val_mse: 20.1444\n",
            "Epoch 1078/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 29.9039 - mse: 29.9039 - val_loss: 19.6920 - val_mse: 19.6920\n",
            "Epoch 1079/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.8124 - mse: 32.8124 - val_loss: 17.6806 - val_mse: 17.6806\n",
            "Epoch 1080/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.5334 - mse: 34.5334 - val_loss: 15.9941 - val_mse: 15.9941\n",
            "Epoch 1081/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.7765 - mse: 32.7765 - val_loss: 15.2011 - val_mse: 15.2011\n",
            "Epoch 1082/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.4108 - mse: 35.4108 - val_loss: 17.1360 - val_mse: 17.1360\n",
            "Epoch 1083/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.3122 - mse: 30.3122 - val_loss: 15.8510 - val_mse: 15.8510\n",
            "Epoch 1084/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.9440 - mse: 33.9440 - val_loss: 17.6446 - val_mse: 17.6446\n",
            "Epoch 1085/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.8656 - mse: 31.8656 - val_loss: 15.8633 - val_mse: 15.8633\n",
            "Epoch 1086/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.6700 - mse: 33.6700 - val_loss: 17.1457 - val_mse: 17.1457\n",
            "Epoch 1087/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.5304 - mse: 30.5304 - val_loss: 21.2608 - val_mse: 21.2608\n",
            "Epoch 1088/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 41.7682 - mse: 41.7682 - val_loss: 21.8841 - val_mse: 21.8841\n",
            "Epoch 1089/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.7502 - mse: 36.7502 - val_loss: 16.4784 - val_mse: 16.4784\n",
            "Epoch 1090/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 38.1435 - mse: 38.1435 - val_loss: 14.9083 - val_mse: 14.9083\n",
            "Epoch 1091/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.7586 - mse: 32.7586 - val_loss: 15.5425 - val_mse: 15.5425\n",
            "Epoch 1092/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.1448 - mse: 33.1448 - val_loss: 15.3950 - val_mse: 15.3950\n",
            "Epoch 1093/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.3748 - mse: 35.3748 - val_loss: 16.5372 - val_mse: 16.5372\n",
            "Epoch 1094/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.1553 - mse: 34.1553 - val_loss: 19.6769 - val_mse: 19.6769\n",
            "Epoch 1095/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.0868 - mse: 33.0868 - val_loss: 22.7428 - val_mse: 22.7428\n",
            "Epoch 1096/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.6640 - mse: 31.6640 - val_loss: 21.6522 - val_mse: 21.6522\n",
            "Epoch 1097/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.7870 - mse: 30.7870 - val_loss: 18.6894 - val_mse: 18.6894\n",
            "Epoch 1098/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.1600 - mse: 38.1600 - val_loss: 17.4420 - val_mse: 17.4420\n",
            "Epoch 1099/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.4467 - mse: 31.4467 - val_loss: 15.7058 - val_mse: 15.7058\n",
            "Epoch 1100/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.0269 - mse: 27.0269 - val_loss: 14.5895 - val_mse: 14.5895\n",
            "Epoch 1101/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.4498 - mse: 33.4498 - val_loss: 17.3302 - val_mse: 17.3302\n",
            "Epoch 1102/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.3858 - mse: 31.3858 - val_loss: 15.8793 - val_mse: 15.8793\n",
            "Epoch 1103/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.2798 - mse: 32.2798 - val_loss: 14.9331 - val_mse: 14.9331\n",
            "Epoch 1104/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.6125 - mse: 31.6125 - val_loss: 16.2458 - val_mse: 16.2458\n",
            "Epoch 1105/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.5970 - mse: 31.5970 - val_loss: 15.7049 - val_mse: 15.7049\n",
            "Epoch 1106/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.5248 - mse: 28.5248 - val_loss: 14.9261 - val_mse: 14.9261\n",
            "Epoch 1107/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.9483 - mse: 31.9483 - val_loss: 16.7951 - val_mse: 16.7951\n",
            "Epoch 1108/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 36.6233 - mse: 36.6233 - val_loss: 30.2839 - val_mse: 30.2839\n",
            "Epoch 1109/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.4927 - mse: 33.4927 - val_loss: 19.3737 - val_mse: 19.3737\n",
            "Epoch 1110/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.2105 - mse: 35.2105 - val_loss: 17.9416 - val_mse: 17.9416\n",
            "Epoch 1111/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.5097 - mse: 32.5097 - val_loss: 15.1812 - val_mse: 15.1812\n",
            "Epoch 1112/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.9893 - mse: 27.9893 - val_loss: 15.3643 - val_mse: 15.3643\n",
            "Epoch 1113/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.7810 - mse: 30.7810 - val_loss: 14.7060 - val_mse: 14.7060\n",
            "Epoch 1114/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.5363 - mse: 34.5363 - val_loss: 16.2764 - val_mse: 16.2764\n",
            "Epoch 1115/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.8158 - mse: 29.8158 - val_loss: 17.4513 - val_mse: 17.4513\n",
            "Epoch 1116/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.2865 - mse: 26.2865 - val_loss: 16.2003 - val_mse: 16.2003\n",
            "Epoch 1117/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.7428 - mse: 26.7428 - val_loss: 18.9217 - val_mse: 18.9217\n",
            "Epoch 1118/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.8791 - mse: 31.8791 - val_loss: 23.8601 - val_mse: 23.8601\n",
            "Epoch 1119/10000\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 31.8783 - mse: 31.8783 - val_loss: 17.9473 - val_mse: 17.9473\n",
            "Epoch 1120/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.5971 - mse: 32.5971 - val_loss: 21.2955 - val_mse: 21.2955\n",
            "Epoch 1121/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.3603 - mse: 28.3603 - val_loss: 15.0187 - val_mse: 15.0187\n",
            "Epoch 1122/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.6668 - mse: 29.6668 - val_loss: 16.9438 - val_mse: 16.9438\n",
            "Epoch 1123/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.8283 - mse: 32.8283 - val_loss: 15.0436 - val_mse: 15.0436\n",
            "Epoch 1124/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.0480 - mse: 23.0480 - val_loss: 14.5035 - val_mse: 14.5035\n",
            "Epoch 1125/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.7131 - mse: 27.7131 - val_loss: 14.4345 - val_mse: 14.4345\n",
            "Epoch 1126/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.8936 - mse: 31.8936 - val_loss: 15.7368 - val_mse: 15.7368\n",
            "Epoch 1127/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.1585 - mse: 29.1585 - val_loss: 14.5640 - val_mse: 14.5640\n",
            "Epoch 1128/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.0537 - mse: 32.0537 - val_loss: 15.6238 - val_mse: 15.6238\n",
            "Epoch 1129/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.5706 - mse: 34.5706 - val_loss: 18.5248 - val_mse: 18.5248\n",
            "Epoch 1130/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.3223 - mse: 34.3223 - val_loss: 14.2951 - val_mse: 14.2951\n",
            "Epoch 1131/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.6323 - mse: 29.6323 - val_loss: 14.9230 - val_mse: 14.9230\n",
            "Epoch 1132/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.2142 - mse: 27.2142 - val_loss: 15.3092 - val_mse: 15.3092\n",
            "Epoch 1133/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.1269 - mse: 28.1269 - val_loss: 14.4449 - val_mse: 14.4449\n",
            "Epoch 1134/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.5448 - mse: 33.5448 - val_loss: 15.1942 - val_mse: 15.1942\n",
            "Epoch 1135/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.2616 - mse: 33.2616 - val_loss: 17.3108 - val_mse: 17.3108\n",
            "Epoch 1136/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.1681 - mse: 28.1681 - val_loss: 18.1890 - val_mse: 18.1890\n",
            "Epoch 1137/10000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 31.8608 - mse: 31.8608 - val_loss: 17.6740 - val_mse: 17.6740\n",
            "Epoch 1138/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.7900 - mse: 32.7900 - val_loss: 16.6187 - val_mse: 16.6187\n",
            "Epoch 1139/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 34.8334 - mse: 34.8334 - val_loss: 14.6124 - val_mse: 14.6124\n",
            "Epoch 1140/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.9871 - mse: 31.9871 - val_loss: 15.3077 - val_mse: 15.3077\n",
            "Epoch 1141/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.7996 - mse: 25.7996 - val_loss: 16.7303 - val_mse: 16.7303\n",
            "Epoch 1142/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.8330 - mse: 30.8330 - val_loss: 16.5548 - val_mse: 16.5548\n",
            "Epoch 1143/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.4124 - mse: 28.4124 - val_loss: 17.4491 - val_mse: 17.4491\n",
            "Epoch 1144/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.3108 - mse: 29.3108 - val_loss: 15.1073 - val_mse: 15.1073\n",
            "Epoch 1145/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 37.0605 - mse: 37.0605 - val_loss: 18.6680 - val_mse: 18.6680\n",
            "Epoch 1146/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.1311 - mse: 31.1311 - val_loss: 15.1735 - val_mse: 15.1735\n",
            "Epoch 1147/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.7119 - mse: 32.7119 - val_loss: 17.2174 - val_mse: 17.2174\n",
            "Epoch 1148/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.6492 - mse: 35.6492 - val_loss: 17.2284 - val_mse: 17.2284\n",
            "Epoch 1149/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.2047 - mse: 28.2047 - val_loss: 20.9223 - val_mse: 20.9223\n",
            "Epoch 1150/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.4523 - mse: 30.4523 - val_loss: 27.3030 - val_mse: 27.3030\n",
            "Epoch 1151/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 37.3742 - mse: 37.3742 - val_loss: 16.4131 - val_mse: 16.4131\n",
            "Epoch 1152/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.9989 - mse: 29.9989 - val_loss: 15.7147 - val_mse: 15.7147\n",
            "Epoch 1153/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.9511 - mse: 30.9511 - val_loss: 14.9521 - val_mse: 14.9521\n",
            "Epoch 1154/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.9676 - mse: 29.9676 - val_loss: 16.8936 - val_mse: 16.8936\n",
            "Epoch 1155/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.2052 - mse: 28.2052 - val_loss: 18.1944 - val_mse: 18.1944\n",
            "Epoch 1156/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.7280 - mse: 30.7280 - val_loss: 16.2389 - val_mse: 16.2389\n",
            "Epoch 1157/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.0352 - mse: 30.0352 - val_loss: 23.6594 - val_mse: 23.6594\n",
            "Epoch 1158/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 36.9380 - mse: 36.9380 - val_loss: 21.3315 - val_mse: 21.3315\n",
            "Epoch 1159/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.4163 - mse: 26.4163 - val_loss: 17.2073 - val_mse: 17.2073\n",
            "Epoch 1160/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.1302 - mse: 30.1302 - val_loss: 16.4541 - val_mse: 16.4541\n",
            "Epoch 1161/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.4968 - mse: 26.4968 - val_loss: 15.5947 - val_mse: 15.5947\n",
            "Epoch 1162/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.5138 - mse: 30.5138 - val_loss: 16.9636 - val_mse: 16.9636\n",
            "Epoch 1163/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.0295 - mse: 31.0295 - val_loss: 17.1209 - val_mse: 17.1209\n",
            "Epoch 1164/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.0533 - mse: 27.0533 - val_loss: 14.5820 - val_mse: 14.5820\n",
            "Epoch 1165/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.6317 - mse: 28.6317 - val_loss: 15.1101 - val_mse: 15.1101\n",
            "Epoch 1166/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.8440 - mse: 29.8440 - val_loss: 21.3372 - val_mse: 21.3372\n",
            "Epoch 1167/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 40.0311 - mse: 40.0311 - val_loss: 18.2509 - val_mse: 18.2509\n",
            "Epoch 1168/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.3727 - mse: 31.3727 - val_loss: 14.5704 - val_mse: 14.5704\n",
            "Epoch 1169/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.3570 - mse: 28.3570 - val_loss: 16.9292 - val_mse: 16.9292\n",
            "Epoch 1170/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.5168 - mse: 29.5168 - val_loss: 18.6979 - val_mse: 18.6979\n",
            "Epoch 1171/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.7853 - mse: 28.7853 - val_loss: 16.8089 - val_mse: 16.8089\n",
            "Epoch 1172/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.5591 - mse: 30.5591 - val_loss: 16.4496 - val_mse: 16.4496\n",
            "Epoch 1173/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.6648 - mse: 26.6648 - val_loss: 16.8643 - val_mse: 16.8643\n",
            "Epoch 1174/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.1700 - mse: 26.1700 - val_loss: 18.8982 - val_mse: 18.8982\n",
            "Epoch 1175/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.2052 - mse: 35.2052 - val_loss: 14.9314 - val_mse: 14.9314\n",
            "Epoch 1176/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.3357 - mse: 29.3357 - val_loss: 14.2150 - val_mse: 14.2150\n",
            "Epoch 1177/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.6151 - mse: 31.6151 - val_loss: 16.7958 - val_mse: 16.7958\n",
            "Epoch 1178/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.6007 - mse: 31.6007 - val_loss: 19.9001 - val_mse: 19.9001\n",
            "Epoch 1179/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.4996 - mse: 31.4996 - val_loss: 15.8138 - val_mse: 15.8138\n",
            "Epoch 1180/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.8201 - mse: 35.8201 - val_loss: 15.3080 - val_mse: 15.3080\n",
            "Epoch 1181/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.2819 - mse: 28.2819 - val_loss: 15.4358 - val_mse: 15.4358\n",
            "Epoch 1182/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.5941 - mse: 33.5941 - val_loss: 15.7413 - val_mse: 15.7413\n",
            "Epoch 1183/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.3977 - mse: 24.3977 - val_loss: 16.9891 - val_mse: 16.9891\n",
            "Epoch 1184/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 38.4932 - mse: 38.4932 - val_loss: 14.9548 - val_mse: 14.9548\n",
            "Epoch 1185/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.4340 - mse: 31.4340 - val_loss: 17.5373 - val_mse: 17.5373\n",
            "Epoch 1186/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.1999 - mse: 32.1999 - val_loss: 14.1126 - val_mse: 14.1126\n",
            "Epoch 1187/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.4031 - mse: 31.4031 - val_loss: 17.9233 - val_mse: 17.9233\n",
            "Epoch 1188/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.8315 - mse: 29.8315 - val_loss: 14.5119 - val_mse: 14.5119\n",
            "Epoch 1189/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.1940 - mse: 27.1940 - val_loss: 16.1922 - val_mse: 16.1922\n",
            "Epoch 1190/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.2546 - mse: 29.2546 - val_loss: 15.5823 - val_mse: 15.5823\n",
            "Epoch 1191/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.2173 - mse: 33.2173 - val_loss: 17.3543 - val_mse: 17.3543\n",
            "Epoch 1192/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.9371 - mse: 33.9371 - val_loss: 16.9975 - val_mse: 16.9975\n",
            "Epoch 1193/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.3959 - mse: 27.3959 - val_loss: 14.8675 - val_mse: 14.8675\n",
            "Epoch 1194/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.6689 - mse: 29.6689 - val_loss: 18.6514 - val_mse: 18.6514\n",
            "Epoch 1195/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.0118 - mse: 29.0118 - val_loss: 18.2258 - val_mse: 18.2258\n",
            "Epoch 1196/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.2301 - mse: 30.2301 - val_loss: 20.0684 - val_mse: 20.0684\n",
            "Epoch 1197/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.8517 - mse: 26.8517 - val_loss: 14.8655 - val_mse: 14.8655\n",
            "Epoch 1198/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.8279 - mse: 34.8279 - val_loss: 16.8700 - val_mse: 16.8700\n",
            "Epoch 1199/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.4228 - mse: 32.4228 - val_loss: 17.9811 - val_mse: 17.9811\n",
            "Epoch 1200/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.3279 - mse: 27.3279 - val_loss: 14.9982 - val_mse: 14.9982\n",
            "Epoch 1201/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.2289 - mse: 27.2289 - val_loss: 19.0076 - val_mse: 19.0076\n",
            "Epoch 1202/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.7952 - mse: 29.7952 - val_loss: 16.5919 - val_mse: 16.5919\n",
            "Epoch 1203/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.4760 - mse: 26.4760 - val_loss: 19.2968 - val_mse: 19.2968\n",
            "Epoch 1204/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.5036 - mse: 26.5036 - val_loss: 14.5461 - val_mse: 14.5461\n",
            "Epoch 1205/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.3680 - mse: 32.3680 - val_loss: 14.8748 - val_mse: 14.8748\n",
            "Epoch 1206/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.5758 - mse: 30.5758 - val_loss: 15.6586 - val_mse: 15.6586\n",
            "Epoch 1207/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.4381 - mse: 28.4381 - val_loss: 16.9855 - val_mse: 16.9855\n",
            "Epoch 1208/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.5189 - mse: 33.5189 - val_loss: 14.1590 - val_mse: 14.1590\n",
            "Epoch 1209/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.3963 - mse: 31.3963 - val_loss: 14.2508 - val_mse: 14.2508\n",
            "Epoch 1210/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.0505 - mse: 25.0505 - val_loss: 24.1533 - val_mse: 24.1533\n",
            "Epoch 1211/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.9534 - mse: 34.9534 - val_loss: 20.9153 - val_mse: 20.9153\n",
            "Epoch 1212/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.0851 - mse: 29.0851 - val_loss: 14.7536 - val_mse: 14.7536\n",
            "Epoch 1213/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.1866 - mse: 28.1866 - val_loss: 17.5562 - val_mse: 17.5562\n",
            "Epoch 1214/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.5272 - mse: 26.5272 - val_loss: 16.7742 - val_mse: 16.7742\n",
            "Epoch 1215/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.8088 - mse: 25.8088 - val_loss: 13.8647 - val_mse: 13.8647\n",
            "Epoch 1216/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.9716 - mse: 30.9716 - val_loss: 14.4580 - val_mse: 14.4580\n",
            "Epoch 1217/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.5551 - mse: 31.5551 - val_loss: 19.7778 - val_mse: 19.7778\n",
            "Epoch 1218/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.9718 - mse: 33.9718 - val_loss: 17.6253 - val_mse: 17.6253\n",
            "Epoch 1219/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.3266 - mse: 25.3266 - val_loss: 20.8838 - val_mse: 20.8838\n",
            "Epoch 1220/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.6608 - mse: 27.6608 - val_loss: 15.9253 - val_mse: 15.9253\n",
            "Epoch 1221/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.3301 - mse: 27.3301 - val_loss: 15.3855 - val_mse: 15.3855\n",
            "Epoch 1222/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.7128 - mse: 28.7128 - val_loss: 18.0496 - val_mse: 18.0496\n",
            "Epoch 1223/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.6641 - mse: 28.6641 - val_loss: 15.1610 - val_mse: 15.1610\n",
            "Epoch 1224/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.7584 - mse: 25.7584 - val_loss: 16.2576 - val_mse: 16.2576\n",
            "Epoch 1225/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.8733 - mse: 30.8733 - val_loss: 15.7572 - val_mse: 15.7572\n",
            "Epoch 1226/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.9139 - mse: 30.9139 - val_loss: 14.7153 - val_mse: 14.7153\n",
            "Epoch 1227/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 32.4811 - mse: 32.4811 - val_loss: 17.3330 - val_mse: 17.3330\n",
            "Epoch 1228/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 35.1616 - mse: 35.1616 - val_loss: 13.9522 - val_mse: 13.9522\n",
            "Epoch 1229/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.2746 - mse: 31.2746 - val_loss: 22.0787 - val_mse: 22.0787\n",
            "Epoch 1230/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.6678 - mse: 30.6678 - val_loss: 16.3442 - val_mse: 16.3442\n",
            "Epoch 1231/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.8381 - mse: 26.8381 - val_loss: 14.8124 - val_mse: 14.8124\n",
            "Epoch 1232/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.9596 - mse: 25.9596 - val_loss: 14.6004 - val_mse: 14.6004\n",
            "Epoch 1233/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.7947 - mse: 26.7947 - val_loss: 14.7904 - val_mse: 14.7904\n",
            "Epoch 1234/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.5933 - mse: 26.5933 - val_loss: 15.6596 - val_mse: 15.6596\n",
            "Epoch 1235/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.4993 - mse: 30.4993 - val_loss: 16.5434 - val_mse: 16.5434\n",
            "Epoch 1236/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.2110 - mse: 30.2110 - val_loss: 17.2063 - val_mse: 17.2063\n",
            "Epoch 1237/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.1571 - mse: 29.1571 - val_loss: 14.0168 - val_mse: 14.0168\n",
            "Epoch 1238/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.9190 - mse: 24.9190 - val_loss: 14.8384 - val_mse: 14.8384\n",
            "Epoch 1239/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.9667 - mse: 21.9667 - val_loss: 14.3091 - val_mse: 14.3091\n",
            "Epoch 1240/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.2451 - mse: 25.2451 - val_loss: 15.1484 - val_mse: 15.1484\n",
            "Epoch 1241/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.8239 - mse: 30.8239 - val_loss: 18.5048 - val_mse: 18.5048\n",
            "Epoch 1242/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.5467 - mse: 28.5467 - val_loss: 16.4020 - val_mse: 16.4020\n",
            "Epoch 1243/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.2211 - mse: 28.2211 - val_loss: 18.2915 - val_mse: 18.2915\n",
            "Epoch 1244/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.3826 - mse: 30.3826 - val_loss: 14.7503 - val_mse: 14.7503\n",
            "Epoch 1245/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.5356 - mse: 26.5356 - val_loss: 16.2603 - val_mse: 16.2603\n",
            "Epoch 1246/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.4510 - mse: 28.4510 - val_loss: 19.1892 - val_mse: 19.1892\n",
            "Epoch 1247/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.2652 - mse: 29.2652 - val_loss: 17.3195 - val_mse: 17.3195\n",
            "Epoch 1248/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.0421 - mse: 25.0421 - val_loss: 14.6908 - val_mse: 14.6908\n",
            "Epoch 1249/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.1081 - mse: 27.1081 - val_loss: 15.0988 - val_mse: 15.0988\n",
            "Epoch 1250/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.2155 - mse: 32.2155 - val_loss: 13.8898 - val_mse: 13.8898\n",
            "Epoch 1251/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.8087 - mse: 27.8087 - val_loss: 14.0303 - val_mse: 14.0303\n",
            "Epoch 1252/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.3886 - mse: 25.3886 - val_loss: 13.5716 - val_mse: 13.5716\n",
            "Epoch 1253/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.6326 - mse: 32.6326 - val_loss: 15.7712 - val_mse: 15.7712\n",
            "Epoch 1254/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.6834 - mse: 25.6834 - val_loss: 15.6453 - val_mse: 15.6453\n",
            "Epoch 1255/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.2230 - mse: 25.2230 - val_loss: 13.5636 - val_mse: 13.5636\n",
            "Epoch 1256/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.0279 - mse: 22.0279 - val_loss: 15.2278 - val_mse: 15.2278\n",
            "Epoch 1257/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.2298 - mse: 26.2298 - val_loss: 14.7366 - val_mse: 14.7366\n",
            "Epoch 1258/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.8515 - mse: 28.8515 - val_loss: 17.0074 - val_mse: 17.0074\n",
            "Epoch 1259/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.1594 - mse: 25.1594 - val_loss: 19.1289 - val_mse: 19.1289\n",
            "Epoch 1260/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.1317 - mse: 28.1317 - val_loss: 13.5562 - val_mse: 13.5562\n",
            "Epoch 1261/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.5118 - mse: 22.5118 - val_loss: 15.7285 - val_mse: 15.7285\n",
            "Epoch 1262/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.2892 - mse: 29.2892 - val_loss: 13.4675 - val_mse: 13.4675\n",
            "Epoch 1263/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.9322 - mse: 25.9322 - val_loss: 17.6771 - val_mse: 17.6771\n",
            "Epoch 1264/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.6708 - mse: 28.6708 - val_loss: 14.4191 - val_mse: 14.4191\n",
            "Epoch 1265/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.5554 - mse: 29.5554 - val_loss: 17.2178 - val_mse: 17.2178\n",
            "Epoch 1266/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.3812 - mse: 24.3812 - val_loss: 13.6553 - val_mse: 13.6553\n",
            "Epoch 1267/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.2402 - mse: 29.2402 - val_loss: 17.2448 - val_mse: 17.2448\n",
            "Epoch 1268/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.2931 - mse: 31.2931 - val_loss: 14.3842 - val_mse: 14.3842\n",
            "Epoch 1269/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.2758 - mse: 24.2758 - val_loss: 17.6659 - val_mse: 17.6659\n",
            "Epoch 1270/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.0376 - mse: 26.0376 - val_loss: 13.4921 - val_mse: 13.4921\n",
            "Epoch 1271/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.6081 - mse: 28.6081 - val_loss: 14.0166 - val_mse: 14.0166\n",
            "Epoch 1272/10000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 28.0141 - mse: 28.0141 - val_loss: 14.4262 - val_mse: 14.4262\n",
            "Epoch 1273/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.3515 - mse: 34.3515 - val_loss: 14.7983 - val_mse: 14.7983\n",
            "Epoch 1274/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.6627 - mse: 30.6627 - val_loss: 15.4825 - val_mse: 15.4825\n",
            "Epoch 1275/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 35.0765 - mse: 35.0765 - val_loss: 16.5548 - val_mse: 16.5548\n",
            "Epoch 1276/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.0239 - mse: 27.0239 - val_loss: 13.2185 - val_mse: 13.2185\n",
            "Epoch 1277/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 33.1905 - mse: 33.1905 - val_loss: 15.0950 - val_mse: 15.0950\n",
            "Epoch 1278/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.2673 - mse: 30.2673 - val_loss: 17.5763 - val_mse: 17.5763\n",
            "Epoch 1279/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.0545 - mse: 29.0545 - val_loss: 16.3719 - val_mse: 16.3719\n",
            "Epoch 1280/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.4544 - mse: 30.4544 - val_loss: 18.1205 - val_mse: 18.1205\n",
            "Epoch 1281/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.8633 - mse: 27.8633 - val_loss: 13.9845 - val_mse: 13.9845\n",
            "Epoch 1282/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.2082 - mse: 25.2082 - val_loss: 16.4192 - val_mse: 16.4192\n",
            "Epoch 1283/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.3748 - mse: 33.3748 - val_loss: 14.2227 - val_mse: 14.2227\n",
            "Epoch 1284/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.1985 - mse: 26.1985 - val_loss: 16.0261 - val_mse: 16.0261\n",
            "Epoch 1285/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.0734 - mse: 26.0734 - val_loss: 17.1764 - val_mse: 17.1764\n",
            "Epoch 1286/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.2760 - mse: 24.2760 - val_loss: 15.4680 - val_mse: 15.4680\n",
            "Epoch 1287/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.2481 - mse: 23.2481 - val_loss: 15.6226 - val_mse: 15.6226\n",
            "Epoch 1288/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.7098 - mse: 26.7098 - val_loss: 14.1614 - val_mse: 14.1614\n",
            "Epoch 1289/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.9105 - mse: 27.9105 - val_loss: 15.4531 - val_mse: 15.4531\n",
            "Epoch 1290/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.6136 - mse: 26.6136 - val_loss: 15.9516 - val_mse: 15.9516\n",
            "Epoch 1291/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.1644 - mse: 30.1644 - val_loss: 16.3780 - val_mse: 16.3780\n",
            "Epoch 1292/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.1626 - mse: 27.1626 - val_loss: 13.4947 - val_mse: 13.4947\n",
            "Epoch 1293/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.5065 - mse: 33.5065 - val_loss: 18.9141 - val_mse: 18.9141\n",
            "Epoch 1294/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.3710 - mse: 28.3710 - val_loss: 17.0238 - val_mse: 17.0238\n",
            "Epoch 1295/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.3917 - mse: 29.3917 - val_loss: 15.2580 - val_mse: 15.2580\n",
            "Epoch 1296/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.6673 - mse: 30.6673 - val_loss: 14.6044 - val_mse: 14.6044\n",
            "Epoch 1297/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.0891 - mse: 25.0891 - val_loss: 20.3756 - val_mse: 20.3756\n",
            "Epoch 1298/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.6263 - mse: 30.6263 - val_loss: 17.1603 - val_mse: 17.1603\n",
            "Epoch 1299/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.6028 - mse: 30.6028 - val_loss: 13.7958 - val_mse: 13.7958\n",
            "Epoch 1300/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.9744 - mse: 28.9744 - val_loss: 15.6922 - val_mse: 15.6922\n",
            "Epoch 1301/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.0179 - mse: 27.0179 - val_loss: 14.6730 - val_mse: 14.6730\n",
            "Epoch 1302/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.9690 - mse: 33.9690 - val_loss: 14.3698 - val_mse: 14.3698\n",
            "Epoch 1303/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.6703 - mse: 26.6703 - val_loss: 13.7656 - val_mse: 13.7656\n",
            "Epoch 1304/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.7358 - mse: 25.7358 - val_loss: 14.0385 - val_mse: 14.0385\n",
            "Epoch 1305/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.9738 - mse: 24.9738 - val_loss: 13.5460 - val_mse: 13.5460\n",
            "Epoch 1306/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.7122 - mse: 27.7122 - val_loss: 14.7778 - val_mse: 14.7778\n",
            "Epoch 1307/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.0488 - mse: 26.0488 - val_loss: 14.2205 - val_mse: 14.2205\n",
            "Epoch 1308/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.4698 - mse: 24.4698 - val_loss: 18.4170 - val_mse: 18.4170\n",
            "Epoch 1309/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.2029 - mse: 25.2029 - val_loss: 13.9561 - val_mse: 13.9561\n",
            "Epoch 1310/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.5906 - mse: 24.5906 - val_loss: 16.9364 - val_mse: 16.9364\n",
            "Epoch 1311/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.9669 - mse: 30.9669 - val_loss: 17.7848 - val_mse: 17.7848\n",
            "Epoch 1312/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.5383 - mse: 29.5383 - val_loss: 19.6272 - val_mse: 19.6272\n",
            "Epoch 1313/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.8981 - mse: 22.8981 - val_loss: 16.0378 - val_mse: 16.0378\n",
            "Epoch 1314/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.0780 - mse: 28.0780 - val_loss: 13.0192 - val_mse: 13.0192\n",
            "Epoch 1315/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.9285 - mse: 28.9285 - val_loss: 15.0754 - val_mse: 15.0754\n",
            "Epoch 1316/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.9285 - mse: 25.9285 - val_loss: 19.2190 - val_mse: 19.2190\n",
            "Epoch 1317/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.5850 - mse: 26.5850 - val_loss: 19.3916 - val_mse: 19.3916\n",
            "Epoch 1318/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.9618 - mse: 31.9618 - val_loss: 13.8731 - val_mse: 13.8731\n",
            "Epoch 1319/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.6989 - mse: 26.6989 - val_loss: 17.9938 - val_mse: 17.9938\n",
            "Epoch 1320/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.5964 - mse: 31.5964 - val_loss: 19.2631 - val_mse: 19.2631\n",
            "Epoch 1321/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.4879 - mse: 26.4879 - val_loss: 17.6374 - val_mse: 17.6374\n",
            "Epoch 1322/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.6000 - mse: 28.6000 - val_loss: 13.8965 - val_mse: 13.8965\n",
            "Epoch 1323/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.6186 - mse: 28.6186 - val_loss: 13.8663 - val_mse: 13.8663\n",
            "Epoch 1324/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.8452 - mse: 30.8452 - val_loss: 15.3316 - val_mse: 15.3316\n",
            "Epoch 1325/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.0508 - mse: 30.0508 - val_loss: 16.3863 - val_mse: 16.3863\n",
            "Epoch 1326/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.2122 - mse: 28.2122 - val_loss: 15.3996 - val_mse: 15.3996\n",
            "Epoch 1327/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.1221 - mse: 28.1221 - val_loss: 15.1905 - val_mse: 15.1905\n",
            "Epoch 1328/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.8112 - mse: 30.8112 - val_loss: 15.8175 - val_mse: 15.8175\n",
            "Epoch 1329/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.8410 - mse: 24.8410 - val_loss: 15.2133 - val_mse: 15.2133\n",
            "Epoch 1330/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.2110 - mse: 27.2110 - val_loss: 17.1982 - val_mse: 17.1982\n",
            "Epoch 1331/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.3943 - mse: 22.3943 - val_loss: 19.8231 - val_mse: 19.8231\n",
            "Epoch 1332/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.2520 - mse: 25.2520 - val_loss: 13.6117 - val_mse: 13.6117\n",
            "Epoch 1333/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.4180 - mse: 28.4180 - val_loss: 13.1502 - val_mse: 13.1502\n",
            "Epoch 1334/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.2402 - mse: 28.2402 - val_loss: 14.2378 - val_mse: 14.2378\n",
            "Epoch 1335/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.2486 - mse: 22.2486 - val_loss: 18.4944 - val_mse: 18.4944\n",
            "Epoch 1336/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.6640 - mse: 24.6640 - val_loss: 14.1947 - val_mse: 14.1947\n",
            "Epoch 1337/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.8850 - mse: 26.8850 - val_loss: 14.1121 - val_mse: 14.1121\n",
            "Epoch 1338/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.3767 - mse: 28.3767 - val_loss: 17.8816 - val_mse: 17.8816\n",
            "Epoch 1339/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.0415 - mse: 25.0415 - val_loss: 15.7119 - val_mse: 15.7119\n",
            "Epoch 1340/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.3795 - mse: 26.3795 - val_loss: 15.2019 - val_mse: 15.2019\n",
            "Epoch 1341/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.0544 - mse: 31.0544 - val_loss: 18.4744 - val_mse: 18.4744\n",
            "Epoch 1342/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.1067 - mse: 24.1067 - val_loss: 15.6917 - val_mse: 15.6917\n",
            "Epoch 1343/10000\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 23.8287 - mse: 23.8287 - val_loss: 17.3273 - val_mse: 17.3273\n",
            "Epoch 1344/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.9370 - mse: 25.9370 - val_loss: 15.7383 - val_mse: 15.7383\n",
            "Epoch 1345/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.4344 - mse: 28.4344 - val_loss: 17.6556 - val_mse: 17.6556\n",
            "Epoch 1346/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.9382 - mse: 24.9382 - val_loss: 17.0334 - val_mse: 17.0334\n",
            "Epoch 1347/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.4622 - mse: 28.4622 - val_loss: 14.8887 - val_mse: 14.8887\n",
            "Epoch 1348/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.2475 - mse: 26.2475 - val_loss: 16.9215 - val_mse: 16.9215\n",
            "Epoch 1349/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.2487 - mse: 23.2487 - val_loss: 14.9291 - val_mse: 14.9291\n",
            "Epoch 1350/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.6123 - mse: 30.6123 - val_loss: 14.3481 - val_mse: 14.3481\n",
            "Epoch 1351/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.7296 - mse: 28.7296 - val_loss: 15.2088 - val_mse: 15.2088\n",
            "Epoch 1352/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.1101 - mse: 28.1101 - val_loss: 15.5621 - val_mse: 15.5621\n",
            "Epoch 1353/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.2797 - mse: 28.2797 - val_loss: 19.9646 - val_mse: 19.9646\n",
            "Epoch 1354/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 34.3004 - mse: 34.3004 - val_loss: 20.8636 - val_mse: 20.8636\n",
            "Epoch 1355/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.6552 - mse: 27.6552 - val_loss: 16.1465 - val_mse: 16.1465\n",
            "Epoch 1356/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 23.2609 - mse: 23.2609 - val_loss: 17.7163 - val_mse: 17.7163\n",
            "Epoch 1357/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.7674 - mse: 26.7674 - val_loss: 20.8711 - val_mse: 20.8711\n",
            "Epoch 1358/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.5516 - mse: 25.5516 - val_loss: 14.9561 - val_mse: 14.9561\n",
            "Epoch 1359/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.9562 - mse: 24.9562 - val_loss: 14.7899 - val_mse: 14.7899\n",
            "Epoch 1360/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.2766 - mse: 27.2766 - val_loss: 17.4253 - val_mse: 17.4253\n",
            "Epoch 1361/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.5228 - mse: 24.5228 - val_loss: 16.1522 - val_mse: 16.1522\n",
            "Epoch 1362/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.7007 - mse: 21.7007 - val_loss: 16.2790 - val_mse: 16.2790\n",
            "Epoch 1363/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.9126 - mse: 25.9126 - val_loss: 13.4342 - val_mse: 13.4342\n",
            "Epoch 1364/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.6365 - mse: 31.6365 - val_loss: 17.9202 - val_mse: 17.9202\n",
            "Epoch 1365/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.8748 - mse: 25.8748 - val_loss: 14.7139 - val_mse: 14.7139\n",
            "Epoch 1366/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.6043 - mse: 25.6043 - val_loss: 16.7480 - val_mse: 16.7480\n",
            "Epoch 1367/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.4945 - mse: 25.4945 - val_loss: 15.7768 - val_mse: 15.7768\n",
            "Epoch 1368/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.0724 - mse: 28.0724 - val_loss: 13.8653 - val_mse: 13.8653\n",
            "Epoch 1369/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.7310 - mse: 23.7310 - val_loss: 16.7209 - val_mse: 16.7209\n",
            "Epoch 1370/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 23.9086 - mse: 23.9086 - val_loss: 14.3336 - val_mse: 14.3336\n",
            "Epoch 1371/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.0112 - mse: 23.0112 - val_loss: 16.3017 - val_mse: 16.3017\n",
            "Epoch 1372/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.7669 - mse: 27.7669 - val_loss: 17.1294 - val_mse: 17.1294\n",
            "Epoch 1373/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.3887 - mse: 24.3887 - val_loss: 15.2149 - val_mse: 15.2149\n",
            "Epoch 1374/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.6225 - mse: 26.6225 - val_loss: 15.6904 - val_mse: 15.6904\n",
            "Epoch 1375/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.8855 - mse: 22.8855 - val_loss: 15.9613 - val_mse: 15.9613\n",
            "Epoch 1376/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.4321 - mse: 28.4321 - val_loss: 20.8933 - val_mse: 20.8933\n",
            "Epoch 1377/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.0263 - mse: 30.0263 - val_loss: 23.0389 - val_mse: 23.0389\n",
            "Epoch 1378/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.2504 - mse: 29.2504 - val_loss: 17.5592 - val_mse: 17.5592\n",
            "Epoch 1379/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.4879 - mse: 23.4879 - val_loss: 15.7656 - val_mse: 15.7656\n",
            "Epoch 1380/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.5687 - mse: 27.5687 - val_loss: 18.6672 - val_mse: 18.6672\n",
            "Epoch 1381/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.6039 - mse: 32.6039 - val_loss: 19.2715 - val_mse: 19.2715\n",
            "Epoch 1382/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.3195 - mse: 27.3195 - val_loss: 20.2832 - val_mse: 20.2832\n",
            "Epoch 1383/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.3786 - mse: 25.3786 - val_loss: 21.0404 - val_mse: 21.0404\n",
            "Epoch 1384/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.7679 - mse: 30.7679 - val_loss: 15.9691 - val_mse: 15.9691\n",
            "Epoch 1385/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.9244 - mse: 24.9244 - val_loss: 14.6717 - val_mse: 14.6717\n",
            "Epoch 1386/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.1341 - mse: 29.1341 - val_loss: 16.2414 - val_mse: 16.2414\n",
            "Epoch 1387/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.0545 - mse: 26.0545 - val_loss: 15.2629 - val_mse: 15.2629\n",
            "Epoch 1388/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.9533 - mse: 25.9533 - val_loss: 15.7460 - val_mse: 15.7460\n",
            "Epoch 1389/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.8761 - mse: 30.8761 - val_loss: 14.6442 - val_mse: 14.6442\n",
            "Epoch 1390/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.2503 - mse: 26.2503 - val_loss: 19.6826 - val_mse: 19.6826\n",
            "Epoch 1391/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.5300 - mse: 31.5300 - val_loss: 17.6963 - val_mse: 17.6963\n",
            "Epoch 1392/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.3687 - mse: 26.3687 - val_loss: 14.3154 - val_mse: 14.3154\n",
            "Epoch 1393/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 23.0973 - mse: 23.0973 - val_loss: 17.6643 - val_mse: 17.6643\n",
            "Epoch 1394/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.3986 - mse: 28.3986 - val_loss: 14.8805 - val_mse: 14.8805\n",
            "Epoch 1395/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.0812 - mse: 24.0812 - val_loss: 15.5464 - val_mse: 15.5464\n",
            "Epoch 1396/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 20.5687 - mse: 20.5687 - val_loss: 14.8885 - val_mse: 14.8885\n",
            "Epoch 1397/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.0134 - mse: 23.0134 - val_loss: 13.3823 - val_mse: 13.3823\n",
            "Epoch 1398/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.0539 - mse: 31.0539 - val_loss: 17.4667 - val_mse: 17.4667\n",
            "Epoch 1399/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.7906 - mse: 25.7906 - val_loss: 16.1739 - val_mse: 16.1739\n",
            "Epoch 1400/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.4328 - mse: 21.4328 - val_loss: 14.8634 - val_mse: 14.8634\n",
            "Epoch 1401/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.8937 - mse: 27.8937 - val_loss: 14.4255 - val_mse: 14.4255\n",
            "Epoch 1402/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.8044 - mse: 24.8044 - val_loss: 16.5703 - val_mse: 16.5703\n",
            "Epoch 1403/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.2479 - mse: 24.2479 - val_loss: 20.1119 - val_mse: 20.1119\n",
            "Epoch 1404/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.2700 - mse: 30.2700 - val_loss: 15.5202 - val_mse: 15.5202\n",
            "Epoch 1405/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.1441 - mse: 24.1441 - val_loss: 15.3744 - val_mse: 15.3744\n",
            "Epoch 1406/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.6719 - mse: 29.6719 - val_loss: 16.0967 - val_mse: 16.0967\n",
            "Epoch 1407/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.0487 - mse: 29.0487 - val_loss: 15.4843 - val_mse: 15.4843\n",
            "Epoch 1408/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.7228 - mse: 25.7228 - val_loss: 18.0366 - val_mse: 18.0366\n",
            "Epoch 1409/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.4960 - mse: 27.4960 - val_loss: 15.0833 - val_mse: 15.0833\n",
            "Epoch 1410/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.2436 - mse: 29.2436 - val_loss: 15.6189 - val_mse: 15.6189\n",
            "Epoch 1411/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.4138 - mse: 26.4138 - val_loss: 13.1560 - val_mse: 13.1560\n",
            "Epoch 1412/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.3727 - mse: 29.3727 - val_loss: 14.9025 - val_mse: 14.9025\n",
            "Epoch 1413/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.0970 - mse: 27.0970 - val_loss: 17.7575 - val_mse: 17.7575\n",
            "Epoch 1414/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.9708 - mse: 25.9708 - val_loss: 12.9561 - val_mse: 12.9561\n",
            "Epoch 1415/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.1188 - mse: 26.1188 - val_loss: 20.0197 - val_mse: 20.0197\n",
            "Epoch 1416/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.8029 - mse: 22.8029 - val_loss: 20.0441 - val_mse: 20.0441\n",
            "Epoch 1417/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.2974 - mse: 27.2974 - val_loss: 20.1580 - val_mse: 20.1580\n",
            "Epoch 1418/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.5204 - mse: 30.5204 - val_loss: 13.8558 - val_mse: 13.8558\n",
            "Epoch 1419/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.6272 - mse: 26.6272 - val_loss: 15.8768 - val_mse: 15.8768\n",
            "Epoch 1420/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.7117 - mse: 26.7117 - val_loss: 16.7173 - val_mse: 16.7173\n",
            "Epoch 1421/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.9969 - mse: 30.9969 - val_loss: 14.1839 - val_mse: 14.1839\n",
            "Epoch 1422/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.4000 - mse: 27.4000 - val_loss: 15.0796 - val_mse: 15.0796\n",
            "Epoch 1423/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.6546 - mse: 29.6546 - val_loss: 15.2196 - val_mse: 15.2196\n",
            "Epoch 1424/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.8152 - mse: 28.8152 - val_loss: 15.3087 - val_mse: 15.3087\n",
            "Epoch 1425/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.1849 - mse: 25.1849 - val_loss: 15.9100 - val_mse: 15.9100\n",
            "Epoch 1426/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.4835 - mse: 28.4835 - val_loss: 19.5714 - val_mse: 19.5714\n",
            "Epoch 1427/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.4995 - mse: 29.4995 - val_loss: 13.9826 - val_mse: 13.9826\n",
            "Epoch 1428/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 31.6969 - mse: 31.6969 - val_loss: 17.2787 - val_mse: 17.2787\n",
            "Epoch 1429/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.8392 - mse: 30.8392 - val_loss: 27.4568 - val_mse: 27.4568\n",
            "Epoch 1430/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 30.8476 - mse: 30.8476 - val_loss: 14.8258 - val_mse: 14.8258\n",
            "Epoch 1431/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.9512 - mse: 25.9512 - val_loss: 17.4957 - val_mse: 17.4957\n",
            "Epoch 1432/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.2700 - mse: 29.2700 - val_loss: 17.6986 - val_mse: 17.6986\n",
            "Epoch 1433/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.2865 - mse: 27.2865 - val_loss: 15.5149 - val_mse: 15.5149\n",
            "Epoch 1434/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.7701 - mse: 26.7701 - val_loss: 15.6773 - val_mse: 15.6773\n",
            "Epoch 1435/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.1912 - mse: 28.1912 - val_loss: 16.3281 - val_mse: 16.3281\n",
            "Epoch 1436/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 23.5081 - mse: 23.5081 - val_loss: 15.8451 - val_mse: 15.8451\n",
            "Epoch 1437/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.7368 - mse: 25.7368 - val_loss: 14.4163 - val_mse: 14.4163\n",
            "Epoch 1438/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.1433 - mse: 22.1433 - val_loss: 13.6014 - val_mse: 13.6014\n",
            "Epoch 1439/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.9526 - mse: 22.9526 - val_loss: 17.2884 - val_mse: 17.2884\n",
            "Epoch 1440/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.6961 - mse: 29.6961 - val_loss: 14.9478 - val_mse: 14.9478\n",
            "Epoch 1441/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.9933 - mse: 25.9933 - val_loss: 16.6699 - val_mse: 16.6699\n",
            "Epoch 1442/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 30.8430 - mse: 30.8430 - val_loss: 14.1977 - val_mse: 14.1977\n",
            "Epoch 1443/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 20.4492 - mse: 20.4492 - val_loss: 13.8344 - val_mse: 13.8344\n",
            "Epoch 1444/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.0097 - mse: 22.0097 - val_loss: 16.3526 - val_mse: 16.3526\n",
            "Epoch 1445/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.8874 - mse: 24.8874 - val_loss: 13.8684 - val_mse: 13.8684\n",
            "Epoch 1446/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 21.9017 - mse: 21.9017 - val_loss: 13.9265 - val_mse: 13.9265\n",
            "Epoch 1447/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.7964 - mse: 25.7964 - val_loss: 14.2116 - val_mse: 14.2116\n",
            "Epoch 1448/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 21.5936 - mse: 21.5936 - val_loss: 13.0239 - val_mse: 13.0239\n",
            "Epoch 1449/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.0928 - mse: 28.0928 - val_loss: 16.7966 - val_mse: 16.7966\n",
            "Epoch 1450/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.5853 - mse: 24.5853 - val_loss: 22.1245 - val_mse: 22.1245\n",
            "Epoch 1451/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.8808 - mse: 22.8808 - val_loss: 15.0206 - val_mse: 15.0206\n",
            "Epoch 1452/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 21.4532 - mse: 21.4532 - val_loss: 14.0097 - val_mse: 14.0097\n",
            "Epoch 1453/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.2722 - mse: 25.2722 - val_loss: 14.3964 - val_mse: 14.3964\n",
            "Epoch 1454/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.0058 - mse: 22.0058 - val_loss: 13.7575 - val_mse: 13.7575\n",
            "Epoch 1455/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.8339 - mse: 27.8339 - val_loss: 14.8644 - val_mse: 14.8644\n",
            "Epoch 1456/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.9015 - mse: 26.9015 - val_loss: 13.2419 - val_mse: 13.2419\n",
            "Epoch 1457/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.4464 - mse: 22.4464 - val_loss: 14.2751 - val_mse: 14.2751\n",
            "Epoch 1458/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.5374 - mse: 27.5374 - val_loss: 13.5899 - val_mse: 13.5899\n",
            "Epoch 1459/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 20.8103 - mse: 20.8103 - val_loss: 14.2712 - val_mse: 14.2712\n",
            "Epoch 1460/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.7812 - mse: 24.7812 - val_loss: 14.8896 - val_mse: 14.8896\n",
            "Epoch 1461/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.8690 - mse: 22.8690 - val_loss: 15.5691 - val_mse: 15.5691\n",
            "Epoch 1462/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.4172 - mse: 22.4172 - val_loss: 14.1034 - val_mse: 14.1034\n",
            "Epoch 1463/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.0780 - mse: 29.0780 - val_loss: 16.4286 - val_mse: 16.4286\n",
            "Epoch 1464/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.2670 - mse: 26.2670 - val_loss: 20.3329 - val_mse: 20.3329\n",
            "Epoch 1465/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.5090 - mse: 26.5090 - val_loss: 15.6047 - val_mse: 15.6047\n",
            "Epoch 1466/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.7204 - mse: 26.7204 - val_loss: 15.2755 - val_mse: 15.2755\n",
            "Epoch 1467/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.9723 - mse: 25.9723 - val_loss: 14.8070 - val_mse: 14.8070\n",
            "Epoch 1468/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.0475 - mse: 24.0475 - val_loss: 18.5673 - val_mse: 18.5673\n",
            "Epoch 1469/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.3132 - mse: 26.3132 - val_loss: 14.1953 - val_mse: 14.1953\n",
            "Epoch 1470/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.1709 - mse: 24.1709 - val_loss: 17.1504 - val_mse: 17.1504\n",
            "Epoch 1471/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.2441 - mse: 33.2441 - val_loss: 19.9948 - val_mse: 19.9948\n",
            "Epoch 1472/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.3822 - mse: 28.3822 - val_loss: 16.0899 - val_mse: 16.0899\n",
            "Epoch 1473/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.4602 - mse: 25.4602 - val_loss: 14.4151 - val_mse: 14.4151\n",
            "Epoch 1474/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.6099 - mse: 24.6099 - val_loss: 17.2036 - val_mse: 17.2036\n",
            "Epoch 1475/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.6126 - mse: 26.6126 - val_loss: 15.1322 - val_mse: 15.1322\n",
            "Epoch 1476/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.0099 - mse: 23.0099 - val_loss: 13.8515 - val_mse: 13.8515\n",
            "Epoch 1477/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.1070 - mse: 25.1070 - val_loss: 17.8320 - val_mse: 17.8320\n",
            "Epoch 1478/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.3092 - mse: 25.3092 - val_loss: 14.0139 - val_mse: 14.0139\n",
            "Epoch 1479/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.5667 - mse: 28.5667 - val_loss: 18.3138 - val_mse: 18.3138\n",
            "Epoch 1480/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.2216 - mse: 29.2216 - val_loss: 17.4195 - val_mse: 17.4195\n",
            "Epoch 1481/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.4846 - mse: 24.4846 - val_loss: 14.1088 - val_mse: 14.1088\n",
            "Epoch 1482/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.5531 - mse: 26.5531 - val_loss: 14.2274 - val_mse: 14.2274\n",
            "Epoch 1483/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.8091 - mse: 27.8091 - val_loss: 14.5602 - val_mse: 14.5602\n",
            "Epoch 1484/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 18.8281 - mse: 18.8281 - val_loss: 19.7534 - val_mse: 19.7534\n",
            "Epoch 1485/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.5241 - mse: 29.5241 - val_loss: 14.4980 - val_mse: 14.4980\n",
            "Epoch 1486/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 21.8385 - mse: 21.8385 - val_loss: 17.3255 - val_mse: 17.3255\n",
            "Epoch 1487/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.6096 - mse: 28.6096 - val_loss: 17.3158 - val_mse: 17.3158\n",
            "Epoch 1488/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.4744 - mse: 24.4744 - val_loss: 13.8984 - val_mse: 13.8984\n",
            "Epoch 1489/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 20.0418 - mse: 20.0418 - val_loss: 14.8206 - val_mse: 14.8206\n",
            "Epoch 1490/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 20.9159 - mse: 20.9159 - val_loss: 12.8326 - val_mse: 12.8326\n",
            "Epoch 1491/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.0641 - mse: 24.0641 - val_loss: 14.8425 - val_mse: 14.8425\n",
            "Epoch 1492/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 23.2875 - mse: 23.2875 - val_loss: 15.0224 - val_mse: 15.0224\n",
            "Epoch 1493/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 21.6428 - mse: 21.6428 - val_loss: 12.0111 - val_mse: 12.0111\n",
            "Epoch 1494/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 31.3968 - mse: 31.3968 - val_loss: 19.2384 - val_mse: 19.2384\n",
            "Epoch 1495/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 33.3653 - mse: 33.3653 - val_loss: 17.9542 - val_mse: 17.9542\n",
            "Epoch 1496/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.6084 - mse: 29.6084 - val_loss: 17.0275 - val_mse: 17.0275\n",
            "Epoch 1497/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.4067 - mse: 23.4067 - val_loss: 14.6676 - val_mse: 14.6676\n",
            "Epoch 1498/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 20.1647 - mse: 20.1647 - val_loss: 13.8866 - val_mse: 13.8866\n",
            "Epoch 1499/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.3797 - mse: 22.3797 - val_loss: 16.7465 - val_mse: 16.7465\n",
            "Epoch 1500/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.7243 - mse: 29.7243 - val_loss: 17.4583 - val_mse: 17.4583\n",
            "Epoch 1501/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.3071 - mse: 21.3071 - val_loss: 14.7685 - val_mse: 14.7685\n",
            "Epoch 1502/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.6287 - mse: 27.6287 - val_loss: 12.6782 - val_mse: 12.6782\n",
            "Epoch 1503/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.6005 - mse: 21.6005 - val_loss: 14.8009 - val_mse: 14.8009\n",
            "Epoch 1504/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.8532 - mse: 25.8532 - val_loss: 13.9090 - val_mse: 13.9090\n",
            "Epoch 1505/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.6504 - mse: 27.6504 - val_loss: 27.1391 - val_mse: 27.1391\n",
            "Epoch 1506/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 29.6192 - mse: 29.6192 - val_loss: 17.1183 - val_mse: 17.1183\n",
            "Epoch 1507/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.1400 - mse: 22.1400 - val_loss: 13.8367 - val_mse: 13.8367\n",
            "Epoch 1508/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.4300 - mse: 28.4300 - val_loss: 15.7321 - val_mse: 15.7321\n",
            "Epoch 1509/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.9324 - mse: 22.9324 - val_loss: 15.5414 - val_mse: 15.5414\n",
            "Epoch 1510/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.7650 - mse: 24.7650 - val_loss: 15.1149 - val_mse: 15.1149\n",
            "Epoch 1511/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 19.3942 - mse: 19.3942 - val_loss: 14.1185 - val_mse: 14.1185\n",
            "Epoch 1512/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.4834 - mse: 25.4834 - val_loss: 17.3795 - val_mse: 17.3795\n",
            "Epoch 1513/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.2097 - mse: 26.2097 - val_loss: 14.2925 - val_mse: 14.2925\n",
            "Epoch 1514/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.7960 - mse: 29.7960 - val_loss: 16.5642 - val_mse: 16.5642\n",
            "Epoch 1515/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.4589 - mse: 21.4589 - val_loss: 12.7697 - val_mse: 12.7697\n",
            "Epoch 1516/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.1093 - mse: 23.1093 - val_loss: 12.8280 - val_mse: 12.8280\n",
            "Epoch 1517/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.7823 - mse: 27.7823 - val_loss: 13.9708 - val_mse: 13.9708\n",
            "Epoch 1518/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.4139 - mse: 25.4139 - val_loss: 14.9775 - val_mse: 14.9775\n",
            "Epoch 1519/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.3736 - mse: 24.3736 - val_loss: 14.4168 - val_mse: 14.4168\n",
            "Epoch 1520/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.2893 - mse: 22.2893 - val_loss: 15.7330 - val_mse: 15.7330\n",
            "Epoch 1521/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.0535 - mse: 25.0535 - val_loss: 14.4988 - val_mse: 14.4988\n",
            "Epoch 1522/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.4662 - mse: 25.4662 - val_loss: 16.0016 - val_mse: 16.0016\n",
            "Epoch 1523/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 23.3443 - mse: 23.3443 - val_loss: 13.6173 - val_mse: 13.6173\n",
            "Epoch 1524/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.5234 - mse: 25.5234 - val_loss: 15.1894 - val_mse: 15.1894\n",
            "Epoch 1525/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.9512 - mse: 26.9512 - val_loss: 16.6405 - val_mse: 16.6405\n",
            "Epoch 1526/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 23.8995 - mse: 23.8995 - val_loss: 15.0500 - val_mse: 15.0500\n",
            "Epoch 1527/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.4092 - mse: 26.4092 - val_loss: 13.4526 - val_mse: 13.4526\n",
            "Epoch 1528/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.1893 - mse: 27.1893 - val_loss: 13.7057 - val_mse: 13.7057\n",
            "Epoch 1529/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.9628 - mse: 22.9628 - val_loss: 15.2080 - val_mse: 15.2080\n",
            "Epoch 1530/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 21.6475 - mse: 21.6475 - val_loss: 13.8745 - val_mse: 13.8745\n",
            "Epoch 1531/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.1281 - mse: 26.1281 - val_loss: 15.0571 - val_mse: 15.0571\n",
            "Epoch 1532/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.6000 - mse: 24.6000 - val_loss: 15.4128 - val_mse: 15.4128\n",
            "Epoch 1533/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 24.5018 - mse: 24.5018 - val_loss: 15.8892 - val_mse: 15.8892\n",
            "Epoch 1534/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 20.8040 - mse: 20.8040 - val_loss: 19.2222 - val_mse: 19.2222\n",
            "Epoch 1535/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.0073 - mse: 22.0073 - val_loss: 15.1578 - val_mse: 15.1578\n",
            "Epoch 1536/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.0080 - mse: 22.0080 - val_loss: 14.1589 - val_mse: 14.1589\n",
            "Epoch 1537/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.4878 - mse: 25.4878 - val_loss: 20.1680 - val_mse: 20.1680\n",
            "Epoch 1538/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.9262 - mse: 26.9262 - val_loss: 16.5078 - val_mse: 16.5078\n",
            "Epoch 1539/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 25.5690 - mse: 25.5690 - val_loss: 15.5016 - val_mse: 15.5016\n",
            "Epoch 1540/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 19.6190 - mse: 19.6190 - val_loss: 12.4635 - val_mse: 12.4635\n",
            "Epoch 1541/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.9819 - mse: 22.9819 - val_loss: 15.9772 - val_mse: 15.9772\n",
            "Epoch 1542/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.7062 - mse: 23.7062 - val_loss: 20.6695 - val_mse: 20.6695\n",
            "Epoch 1543/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.5780 - mse: 26.5780 - val_loss: 17.9711 - val_mse: 17.9711\n",
            "Epoch 1544/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 23.7400 - mse: 23.7400 - val_loss: 16.6451 - val_mse: 16.6451\n",
            "Epoch 1545/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.2384 - mse: 21.2384 - val_loss: 13.4441 - val_mse: 13.4441\n",
            "Epoch 1546/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.8743 - mse: 23.8743 - val_loss: 19.2116 - val_mse: 19.2116\n",
            "Epoch 1547/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.8073 - mse: 28.8073 - val_loss: 26.3256 - val_mse: 26.3256\n",
            "Epoch 1548/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.7643 - mse: 23.7643 - val_loss: 16.4265 - val_mse: 16.4265\n",
            "Epoch 1549/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.5162 - mse: 27.5162 - val_loss: 14.2510 - val_mse: 14.2510\n",
            "Epoch 1550/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.6103 - mse: 25.6103 - val_loss: 15.2471 - val_mse: 15.2471\n",
            "Epoch 1551/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.5818 - mse: 26.5818 - val_loss: 14.6773 - val_mse: 14.6773\n",
            "Epoch 1552/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.2902 - mse: 22.2902 - val_loss: 14.9787 - val_mse: 14.9787\n",
            "Epoch 1553/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 20.5638 - mse: 20.5638 - val_loss: 15.1476 - val_mse: 15.1476\n",
            "Epoch 1554/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 21.2327 - mse: 21.2327 - val_loss: 15.7922 - val_mse: 15.7922\n",
            "Epoch 1555/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.0625 - mse: 28.0625 - val_loss: 19.5141 - val_mse: 19.5141\n",
            "Epoch 1556/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 28.3178 - mse: 28.3178 - val_loss: 16.6283 - val_mse: 16.6283\n",
            "Epoch 1557/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.5474 - mse: 21.5474 - val_loss: 14.5903 - val_mse: 14.5903\n",
            "Epoch 1558/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 19.5145 - mse: 19.5145 - val_loss: 14.9516 - val_mse: 14.9516\n",
            "Epoch 1559/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.5427 - mse: 22.5427 - val_loss: 13.0483 - val_mse: 13.0483\n",
            "Epoch 1560/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 23.6626 - mse: 23.6626 - val_loss: 13.3166 - val_mse: 13.3166\n",
            "Epoch 1561/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.9283 - mse: 25.9283 - val_loss: 15.0723 - val_mse: 15.0723\n",
            "Epoch 1562/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.6960 - mse: 27.6960 - val_loss: 15.8242 - val_mse: 15.8242\n",
            "Epoch 1563/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.2831 - mse: 25.2831 - val_loss: 15.9746 - val_mse: 15.9746\n",
            "Epoch 1564/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.7582 - mse: 21.7582 - val_loss: 14.8490 - val_mse: 14.8490\n",
            "Epoch 1565/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.4254 - mse: 22.4254 - val_loss: 14.9634 - val_mse: 14.9634\n",
            "Epoch 1566/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 23.6001 - mse: 23.6001 - val_loss: 15.7939 - val_mse: 15.7939\n",
            "Epoch 1567/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 20.7170 - mse: 20.7170 - val_loss: 16.6473 - val_mse: 16.6473\n",
            "Epoch 1568/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.8290 - mse: 26.8290 - val_loss: 20.9015 - val_mse: 20.9015\n",
            "Epoch 1569/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 26.3398 - mse: 26.3398 - val_loss: 17.8139 - val_mse: 17.8139\n",
            "Epoch 1570/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 29.2300 - mse: 29.2300 - val_loss: 15.0850 - val_mse: 15.0850\n",
            "Epoch 1571/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 20.4608 - mse: 20.4608 - val_loss: 13.9439 - val_mse: 13.9439\n",
            "Epoch 1572/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 21.9934 - mse: 21.9934 - val_loss: 15.0695 - val_mse: 15.0695\n",
            "Epoch 1573/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.4279 - mse: 21.4279 - val_loss: 16.9898 - val_mse: 16.9898\n",
            "Epoch 1574/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 23.9079 - mse: 23.9079 - val_loss: 15.0815 - val_mse: 15.0815\n",
            "Epoch 1575/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 28.4641 - mse: 28.4641 - val_loss: 17.3651 - val_mse: 17.3651\n",
            "Epoch 1576/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.7343 - mse: 21.7343 - val_loss: 15.6515 - val_mse: 15.6515\n",
            "Epoch 1577/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 21.2908 - mse: 21.2908 - val_loss: 16.1089 - val_mse: 16.1089\n",
            "Epoch 1578/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.2846 - mse: 22.2846 - val_loss: 19.6171 - val_mse: 19.6171\n",
            "Epoch 1579/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.9049 - mse: 25.9049 - val_loss: 13.6440 - val_mse: 13.6440\n",
            "Epoch 1580/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 27.0179 - mse: 27.0179 - val_loss: 15.3640 - val_mse: 15.3640\n",
            "Epoch 1581/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.8446 - mse: 25.8446 - val_loss: 13.5295 - val_mse: 13.5295\n",
            "Epoch 1582/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 23.2532 - mse: 23.2532 - val_loss: 16.0385 - val_mse: 16.0385\n",
            "Epoch 1583/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 27.5819 - mse: 27.5819 - val_loss: 15.7841 - val_mse: 15.7841\n",
            "Epoch 1584/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.2909 - mse: 26.2909 - val_loss: 17.1118 - val_mse: 17.1118\n",
            "Epoch 1585/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.1304 - mse: 25.1304 - val_loss: 15.1220 - val_mse: 15.1220\n",
            "Epoch 1586/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 22.2540 - mse: 22.2540 - val_loss: 12.8409 - val_mse: 12.8409\n",
            "Epoch 1587/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 26.5077 - mse: 26.5077 - val_loss: 16.3188 - val_mse: 16.3188\n",
            "Epoch 1588/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 24.7473 - mse: 24.7473 - val_loss: 13.5516 - val_mse: 13.5516\n",
            "Epoch 1589/10000\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 22.9770 - mse: 22.9770 - val_loss: 15.3315 - val_mse: 15.3315\n",
            "Epoch 1590/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 32.6980 - mse: 32.6980 - val_loss: 15.4280 - val_mse: 15.4280\n",
            "Epoch 1591/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 21.8938 - mse: 21.8938 - val_loss: 12.1702 - val_mse: 12.1702\n",
            "Epoch 1592/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.2742 - mse: 25.2742 - val_loss: 15.2736 - val_mse: 15.2736\n",
            "Epoch 1593/10000\n",
            "23/23 [==============================] - 0s 5ms/step - loss: 25.9833 - mse: 25.9833 - val_loss: 17.2922 - val_mse: 17.2922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAFAdpgVFCMM",
        "outputId": "1550070c-4280-4156-f1a1-2a606cc847de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "nvjcO7PyEo5D",
        "outputId": "68a14fbe-60e1-4d1a-d564-874a7b2fe82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nb d'epoch enregistrés : 1593\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAGDCAYAAABqY9oIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde7TlZX3n+fd33845daMKKBCrSIokeEFjIJaIozPtaIJALpAYRZeJjO2SZAUnuiadFbSnGzvRGbO6O3aTVhNsiZCxRQbjSBIMouJkbAUpDHIRDCVCqJJLURfqei577+/8sX+n6lRxqiio3/7tX9V5v9Y6q/Z59u/y7FNF1Yfv8/yeJzITSZIkSZIkqWyNUXdAkiRJkiRJxyYLT5IkSZIkSRoKC0+SJEmSJEkaCgtPkiRJkiRJGgoLT5IkSZIkSRoKC0+SJEmSJEkaCgtPko5qEfGZiPjwYR77cET8wpFeR5IkaSErK39JWhgsPEmSJEmSJGkoLDxJkiRJkiRpKCw8SRq6Yor1H0TE3RGxKyI+HREnR8SXI2JHRHw1IlbMOf5XI+K+iNgWEd+IiJfOee+siPhucd7ngfED7vXLEXFXce63IuIVz7PP74mI9RGxJSJujIgXFu0RER+LiCcjYntE3BMRLy/euyAivl/0bWNE/Kvn9QOTJEk6QkdD/ioe2ftE0aedEfHfI+IFEfGfImJrRDwQEWfNOf4Pi4y1IyJ+EBFvLNobEXF5RPwwIjZHxPURcfwR/xAllcLCk6SqvBn4ReBFwK8AXwY+CKxk8HfR7wFExIuAzwHvL967CfibiOhERAf4f4C/Ao4H/u/iuhTnngVcDfw2cALwF8CNETH2XDoaEW8A/k/grcApwCPAdcXb5wL/U/E5jiuO2Vy892ngtzNzKfBy4OvP5b6SJEklOxry11uB/x04EZgCvg18t/j+BuBPi/u8GHgv8Koia70JeLi4xv8KXAT8C+CFwFbg44d5f0lDZuFJUlX+LDOfyMyNwP8H3J6Z/5iZk8AXgdnRrIuBv8vMWzJzBvgPwATwPwDnAG3gP2XmTGbeANwx5x6XAn+RmbdnZi8zr2EQYM55jn19B3B1Zn43M6eADwCviYg1wAywFHgJEJl5f2Y+Vpw3A5wREcsyc2tmfvc53leSJKlMR0P++mJm3jmnT5OZeW1m9oDPz+ljDxhjkLXamflwZv6weO93gH+dmRuK7PYh4DciovVcfliShsPCk6SqPDHn9Z55vl9SvH4hgxlGAGRmH3gUWFW8tzEzc865j8x5/ZPA7xfTvLdFxDbg1OK85+LAPuxkMKtpVWZ+HfgvDEbRnoyIqyJiWXHom4ELgEci4v+NiNc8x/tKkiSV6WjIX4fVx8xcz2BG1ocYZLDrZpdCKPrwxTn3v59Boerkw+yDpCGy8CSpbn7MIDwAgzWVGISXjcBjwKqibdZPzHn9KPCRzFw+52tRZn7uCPuwmMHU8Y0AmXllZr4SOIPB1PU/KNrvyMwLgZMYTEm//jneV5IkaRTqkL+eVWb+t8x8XdHXBP5kTh/OP6AP48VML0kjZuFJUt1cD/xSRLwxItrA7zOYrv0tBs/8d4Hfi4h2RPw6cPaccz8F/E5EvLpYBHxxRPxSRCx9jn34HPCuiDizWJ/g/2AwNf3hiHhVcf02sAuYBPrFGgjviIjjiinq24H+EfwcJEmSqlKH/HVIEfHiiHhDkc0mGcyGms1afw58JCJ+sjh2ZURcWOb9JT1/Fp4k1Upm/gD4TeDPgKcYLIT5K5k5nZnTwK8D/wuwhcF6BH8959x1wHsYPAq3FVhfHPtc+/BV4N8AX2AwyvfTwNuKt5cxCFhbGUwz3wz8++K93wIejojtDNYaeMdzvbckSVLV6pC/DsMY8NGif48zmGH+geK9/wzcCHwlInYAtwGvHkIfJD0Psf+jupIkSZIkSVI5nPEkSZIkSZKkobDwJEmSJEmSpKGw8CRJkiRJkqShsPAkSZIkSZKkobDwJEmSJEmSpKFojboDVTvxxBNzzZo1o+6GJEkakjvvvPOpzFw56n5oH/OXJEnHvoNlsAVXeFqzZg3r1q0bdTckSdKQRMQjo+6D9mf+kiTp2HewDOajdpIkSZIkSRoKC0+SJEmSJEkaCgtPkiRJkiRJGooFt8bTfGZmZtiwYQOTk5Oj7spQjY+Ps3r1atrt9qi7IkmSFriFkr/ADCZJWtgsPAEbNmxg6dKlrFmzhogYdXeGIjPZvHkzGzZs4LTTTht1dyRJ0gK3EPIXmMEkSfJRO2BycpITTjjhmA49EcEJJ5ywIEYVJUlS/S2E/AVmMEmSLDwVjvXQAwvjM0qSpKPHQskmC+VzSpI0HwtPNbBt2zY+8YlPPOfzLrjgArZt2zaEHkmSJB37zGCSJA2fhacaOFjo6Xa7hzzvpptuYvny5cPqliRJ0jHNDCZJ0vC5uHgNXH755fzwhz/kzDPPpN1uMz4+zooVK3jggQf4p3/6Jy666CIeffRRJicned/73sell14KwJo1a1i3bh07d+7k/PPP53Wvex3f+ta3WLVqFV/60peYmJgY8SeTJEmqLzOYJEnDZ+HpAP/ub+7j+z/eXuo1z3jhMq74lZcd9P2PfvSj3Hvvvdx111184xvf4Jd+6Ze499579+58cvXVV3P88cezZ88eXvWqV/HmN7+ZE044Yb9rPPjgg3zuc5/jU5/6FG9961v5whe+wG/+5m+W+jkkSZKGYRT5C8xgkiRVwcJTSXr9pBHlLB559tln77fd7pVXXskXv/hFAB599FEefPDBZ4Se0047jTPPPBOAV77ylTz88MNH3A9JkqQ6S6DfTxqNoIzlu81gkiSVz8LTAZ5tZOxg7t6wjZOWjvOC48aPuA+LFy/e+/ob3/gGX/3qV/n2t7/NokWLeP3rXz/vdrxjY2N7XzebTfbs2XPE/ZAkSarC881fOyZn+NFTu/jplUtYPHbksdYMJklS+VxcvAaWLl3Kjh075n3v6aefZsWKFSxatIgHHniA2267reLeSZIkHZvMYJIkDZ8znkpyJNO7TzjhBF772tfy8pe/nImJCU4++eS975133nn8+Z//OS996Ut58YtfzDnnnHPknZUkSToGHOnjdWYwSZKGLzJz1H2o1Nq1a3PdunX7td1///289KUvPaLr3rNhGytLetRumMr4rJIk1VlE3JmZa0fdD+0zrPy1c3KGh57axU+tXMKSEh61GyYzmCTpWHewDOajdqUpY0lLSZIkSZKkY4eFJ0mSJEmSJA2FhadSLazHFiVJkiRJkg7FwpMkSZIkSZKGwsKTJEmSjm5OOpckqbYsPJXFtcUlSZIqZgCTJKnuLDyVqKrBtiVLllR0J0mSpKNBNSnMDCZJ0nNn4UmSJElHJyc8SZJUe61Rd0Bw+eWXc+qpp3LZZZcB8KEPfYhWq8Wtt97K1q1bmZmZ4cMf/jAXXnjhiHsqSZJ07DCDSZI0fBaeDvTly+Hxe57zaadNd2k3A5rNZ775gp+F8z960HMvvvhi3v/+9+8NPddffz0333wzv/d7v8eyZct46qmnOOecc/jVX/1VIhzakyRJx5jnmb8mMvmp6R4T7QY0DpjI/yz5C8xgkiRVYWiP2kXEeER8JyK+FxH3RcS/K9o/ExE/ioi7iq8zi/aIiCsjYn1E3B0RPz/nWpdExIPF1yVz2l8ZEfcU51wZR2kiOOuss3jyySf58Y9/zPe+9z1WrFjBC17wAj74wQ/yile8gl/4hV9g48aNPPHEE6PuqiRJqjkz2OEzg0mSNHzDnPE0BbwhM3dGRBv4ZkR8uXjvDzLzhgOOPx84vfh6NfBJ4NURcTxwBbCWwcqRd0bEjZm5tTjmPcDtwE3AecCXORLPMjJ2MA9vfJoTlnQ45biJ53X+W97yFm644QYef/xxLr74Yj772c+yadMm7rzzTtrtNmvWrGFycvJ5XVuSJC0oR18Ge575a3Kqy0ObdvJTJy5myXj7eV3DDCZJ0nANbcZTDuwsvm0XX4facuRC4NrivNuA5RFxCvAm4JbM3FIEnVuA84r3lmXmbZmZwLXARcP6PMN28cUXc91113HDDTfwlre8haeffpqTTjqJdrvNrbfeyiOPPDLqLkqSpKPAQsxgR7KnnRlMkqThGuqudhHRjIi7gCcZBJfbi7c+Ukzl/lhEjBVtq4BH55y+oWg7VPuGedqPSi972cvYsWMHq1at4pRTTuEd73gH69at42d/9me59tpreclLXjLqLkqSpKOEGezwmcEkSRquoS4unpk94MyIWA58MSJeDnwAeBzoAFcBfwj80TD7ERGXApcC/MRP/MQwb3VE7rln36KaJ554It/+9rfnPW7nzp3ztkuSJEE9MtjRkr/ADCZJ0jANdcbTrMzcBtwKnJeZjxVTuaeAvwTOLg7bCJw657TVRduh2lfP0z7f/a/KzLWZuXblypVlfCRJkqTaG2UGM39JkiQY7q52K4tRNiJiAvhF4IFiXQCK3U8uAu4tTrkReGexs8o5wNOZ+RhwM3BuRKyIiBXAucDNxXvbI+Kc4lrvBL40rM9zWI5kgQFJkqQSLMgMJkmSamuYj9qdAlwTEU0GBa7rM/NvI+LrEbESCOAu4HeK428CLgDWA7uBdwFk5paI+GPgjuK4P8rMLcXr3wU+A0ww2EnlyHa0kyRJOvqZwSRJUm0MrfCUmXcDZ83T/oaDHJ/AZQd572rg6nna1wEvP7Ke7r0Wg0G7Y9fgRyxJko5lR1MGWwj5C8xgkqSFrZI1nupufHyczZs3H3EoqHOkyEw2b97M+Pj4qLsiSZJUWv6qOzOYJGmhG+qudkeL1atXs2HDBjZt2vS8r/HEtj3sHGvx9ES7xJ6Va3x8nNWrVz/7gZIkSUNWRv6a7vZ5cscUvS0dxtvNEntXLjOYJGkhs/AEtNttTjvttCO6xm/827/nbWf/BP/ml19aUq8kSZKOXWXkrzsf2cp7PvstPvOuV3HWi08qqWeSJKlMPmpXkoWwPoEkSVKdGL8kSao/C08lOsaXKJAkSaolI5gkSfVl4akkDrhJkiRVy/wlSVL9WXgqUTreJkmSVD0jmCRJtWXhqSwOuUmSJFXKNTYlSao/C08lco0nSZKk6jnrXJKk+rLwVBLH2yRJkqpl/pIkqf4sPEmSJOmo5qxzSZLqy8JTSVxjQJIkqVrGL0mS6s/CU4nS4TZJkqTKGcEkSaovC08lccRNkiSpWuEqT5Ik1Z6FpxI52CZJklQ9M5gkSfVl4akkjrdJkiRVyxnnkiTVn4WnErm+gCRJUvVcZ1OSpPqy8FQSd7WTJEmSJEnan4UnSZIkSZIkDYWFpxKlS1tKkiRVzgQmSVJ9WXgqiQ/aSZIkVcuVDiRJqj8LTyVyXUtJkqTqmcEkSaovC08lccRNkiSpWuGcc0mSas/CU4kcbJMkSRoFU5gkSXVl4ak0jrhJkiRVyRnnkiTVn4WnErm+gCRJUvXMYJIk1ZeFp5I44iZJklQt85ckSfVn4alUDrdJkiRVzQQmSVJ9WXgqiQNukiRJ1XJXO0mS6s/CU4lcX0CSJKl6ZjBJkurLwlNJXGNAkiSpWuYvSZLqb2iFp4gYj4jvRMT3IuK+iPh3RftpEXF7RKyPiM9HRKdoHyu+X1+8v2bOtT5QtP8gIt40p/28om19RFw+rM9yuBxtkyRJo7YgM5irPEmSVFvDnPE0BbwhM38OOBM4LyLOAf4E+Fhm/gywFXh3cfy7ga1F+8eK44iIM4C3AS8DzgM+ERHNiGgCHwfOB84A3l4cOxKuMSBJkmpiwWQw05ckSfU3tMJTDuwsvm0XXwm8AbihaL8GuKh4fWHxPcX7b4yIKNqvy8ypzPwRsB44u/han5kPZeY0cF1x7Mg42iZJkkZtQWYwI5gkSbU11DWeilGxu4AngVuAHwLbMrNbHLIBWFW8XgU8ClC8/zRwwtz2A845WPtIuMaAJEmqi4WSwcxfkiTV31ALT5nZy8wzgdUMRsdeMsz7HUxEXBoR6yJi3aZNm0bRBUmSpMrUIYOZvyRJElS0q11mbgNuBV4DLI+IVvHWamBj8XojcCpA8f5xwOa57Qecc7D2+e5/VWauzcy1K1euLOUzzX+foV1akiTpORtlBqsqfwEudiBJUo0Nc1e7lRGxvHg9AfwicD+D8PMbxWGXAF8qXt9YfE/x/tczM4v2txU7rpwGnA58B7gDOL3YoaXDYPHLG4f1eZ6NM70lSVIdLKwMZgKTJKnuWs9+yPN2CnBNsfNJA7g+M/82Ir4PXBcRHwb+Efh0cfyngb+KiPXAFgYhhsy8LyKuB74PdIHLMrMHEBHvBW4GmsDVmXnfED/Ps3K0TZIk1cDCy2BOO5ckqbaGVnjKzLuBs+Zpf4jBWgMHtk8CbznItT4CfGSe9puAm464syUIV7eUJEk1sJAymPFLkqT6q2SNp4XCwTZJkiRJkqR9LDxJkiTpqOSEJ0mS6s/CU4nSVZ4kSZIq56xzSZLqy8JTSVxjQJIkqVqusSlJUv1ZeCqTo22SJEmVc9a5JEn1ZeGpJA64SZIkVcv4JUlS/Vl4KpFjbZIkSdVzjSdJkurLwlNJwjE3SZKkSjnjXJKk+rPwVKJ0uE2SJKlyRjBJkurLwlNJHHGTJEmqljPOJUmqPwtPJXKwTZIkqXpmMEmS6svCU0kcb5MkSaqWM84lSao/C08lcn0BSZKk6rnOpiRJ9WXhqSThkJskSZIkSdJ+LDxJkiRJkiRpKCw8lchJ3pIkSdUzg0mSVF8Wnkrig3aSJEnVcqUDSZLqz8JTiVzYUpIkaQSMYJIk1ZaFp7I44iZJklQpN3eRJKn+LDyVyME2SZKk6qUpTJKk2rLwVBLH2yRJkqpl/pIkqf4sPJXJwTZJkqTKucymJEn1ZeGpJK4xIEmSVC3jlyRJ9WfhqUSuLyBJklQ9E5gkSfVl4akkDrhJkiRVK0xgkiTVnoWnErm+gCRJUvXMYJIk1ZeFp5K4xoAkSVK1zF+SJNWfhacSOdomSZJUPdfZlCSpviw8lcQ1BiRJkqpl+pIkqf4sPJXI0TZJkqTqOetckqT6svBUEtcYkCRJqpj5S5Kk2hta4SkiTo2IWyPi+xFxX0S8r2j/UERsjIi7iq8L5pzzgYhYHxE/iIg3zWk/r2hbHxGXz2k/LSJuL9o/HxGdYX0eSZKko4EZTJIk1ckwZzx1gd/PzDOAc4DLIuKM4r2PZeaZxddNAMV7bwNeBpwHfCIimhHRBD4OnA+cAbx9znX+pLjWzwBbgXcP8fM8K6d5S5KkGlh4GWyUN5ckSYc0tMJTZj6Wmd8tXu8A7gdWHeKUC4HrMnMqM38ErAfOLr7WZ+ZDmTkNXAdcGBEBvAG4oTj/GuCi4XwaSZKko8NCymBu7iJJUv1VssZTRKwBzgJuL5reGxF3R8TVEbGiaFsFPDrntA1F28HaTwC2ZWb3gPaRcbRNkiTVyULJYE47lySpvoZeeIqIJcAXgPdn5nbgk8BPA2cCjwH/sYI+XBoR6yJi3aZNm4Z1j6FcV5Ik6fkYdQarJn8N5bKSJKlEQy08RUSbQeD5bGb+NUBmPpGZvczsA59iMI0bYCNw6pzTVxdtB2vfDCyPiNYB7c+QmVdl5trMXLty5cpyPty89xnapSVJkg5bHTJYVfkLnHUuSVKdDXNXuwA+DdyfmX86p/2UOYf9GnBv8fpG4G0RMRYRpwGnA98B7gBOL3ZP6TBY/PLGzEzgVuA3ivMvAb40rM/zbBxwkyRJdbCQMpj5S5Kk+ms9+yHP22uB3wLuiYi7irYPMtgR5UwGg1MPA78NkJn3RcT1wPcZ7MZyWWb2ACLivcDNQBO4OjPvK673h8B1EfFh4B8ZhKwRcrxNkiSN3ILLYM46lySpvoZWeMrMbzL/QNRNhzjnI8BH5mm/ab7zMvMh9k0THynXGJAkSXWwkDKYa2xKklR/lexqt1A42iZJklS9NIRJklRbFp5K4oCbJElStYxfkiTVn4WnEjnWJkmSVD0zmCRJ9WXhqSThmJskSVKlnHEuSVL9WXgqkesLSJIkVc8IJklSfVl4KokjbpIkSdVyxrkkSfVn4alEDrZJkiRVzwwmSVJ9WXgqieNtkiRJFTOASZJUexaeJEmSJEmSNBQWnkrkwpaSJEnVc4MXSZLqy8JTWVxdXJIkqVLGL0mS6s/CU4kca5MkSZIkSdrHwlNJHHCTJEmqlvlLkqT6s/BUItcXkCRJqp4RTJKk+rLwVBLXGJAkSapWGMAkSao9C0+SJEk6qqUrbUqSVFsWnkrieJskSVK1zF+SJNWfhacSub6AJElS9cxgkiTVl4WnkrjGgCRJUrWMX5Ik1Z+FpxK5voAkSVL1TGCSJNWXhaeSOOAmSZJUrTCBSZJUexaeSuT6ApIkSdUzg0mSVF8WnkriGgOSJEnVMn9JklR/Fp5K5GibJElS9VxnU5Kk+rLwVBLXGJAkSZIkSdqfhacSOdomSZJUPWedS5JUXxaeyuKEJ0mSpEq5xpMkSfVn4UmSJEmSJElDYeGpRE7zliRJkiRJ2sfCU0mc6S1JklQtN3eRJKn+LDyVyAlPkiRJ1UunnUuSVFtDKzxFxKkRcWtEfD8i7ouI9xXtx0fELRHxYPHriqI9IuLKiFgfEXdHxM/PudYlxfEPRsQlc9pfGRH3FOdcGTG6JSZd3FKSJNXBQspg5i9JkupvmDOeusDvZ+YZwDnAZRFxBnA58LXMPB34WvE9wPnA6cXXpcAnYRCSgCuAVwNnA1fMBqXimPfMOe+8IX6eZ+dgmyRJGr0Fl8Gc8CRJUn0NrfCUmY9l5neL1zuA+4FVwIXANcVh1wAXFa8vBK7NgduA5RFxCvAm4JbM3JKZW4FbgPOK95Zl5m05mF997ZxrVc41BiRJUh0spAxm+pIkqf4qWeMpItYAZwG3Aydn5mPFW48DJxevVwGPzjltQ9F2qPYN87TPd/9LI2JdRKzbtGnTEX2WQ0mnPEmSpBoZZQarKn+Bk84lSaqzoReeImIJ8AXg/Zm5fe57xSjZ0LNCZl6VmWszc+3KlSuHcg/XGJAkSXUy6gxWTf4ygEmSVHdDLTxFRJtB4PlsZv510fxEMUWb4tcni/aNwKlzTl9dtB2qffU87SPj+gKSJKkOzGCSJKkuhrmrXQCfBu7PzD+d89aNwOyuKJcAX5rT/s5iZ5VzgKeL6eA3A+dGxIpiQctzgZuL97ZHxDnFvd4551qVc8BNkiTVwULKYMYvSZLqrzXEa78W+C3gnoi4q2j7IPBR4PqIeDfwCPDW4r2bgAuA9cBu4F0AmbklIv4YuKM47o8yc0vx+neBzwATwJeLr5FxsE2SJJUtIl4HnJ6ZfxkRK4ElmfmjQ5yyADOYKUySpLo6rMJTRLwP+EtgB/BfGSxSeXlmfuVg52TmNzn4QNQb5zk+gcsOcq2rgavnaV8HvPzZ+l8Fd7WTJElli4grgLXAixlksTbwfzEoLs1rIWUwZ5xLklR/h/uo3b8sFqU8F1jBYBTto0Pr1VEqXWBAkiSV69eAXwV2AWTmj4GlI+1RDRnBJEmqr8MtPM2OJ10A/FVm3oeP1e/HETdJkjQE03N3oIuIxSPuT624q50kSfV3uIWnOyPiKwwKTzdHxFKgP7xuHZ0cbJMkSSW7PiL+AlgeEe8Bvgp8asR9qh0zmCRJ9XW4i4u/GzgTeCgzd0fE8RQLT0qSJGk4MvM/RMQvAtsZrPP0bzPzlhF3S5Ik6bAdbuHpNcBdmbkrIn4T+HngPw+vW5IkSSoerft6Zt4SES8GXhwR7cycGXXfJEmSDsfhPmr3SWB3RPwc8PvAD4Frh9aro5QLW0qSpJL9AzAWEauAv2ewwctnRtqjOjKESZJUW4dbeOoWC1teCPyXzPw47qiyHxe3lCRJQxCZuRv4deCTmfkW4GUj7lOtGMEkSaq3wy087YiIDzAYZfu7iGgA7eF16+jkWJskSSpZRMRrgHcAf1e0NUfYn1oyg0mSVF+HW3i6GJgC/mVmPg6sBv790Hp1FHKwTZIkDcH7gMuBv87M+yLiNODrI+5TrZjBJEmqt8NaXDwzH4+IzwKviohfBr6Tma7xdCDXF5AkSeXaDfSBtxcbvARO8HkGI5gkSfV1WIWniHgrgxlO32AQeP4sIv4gM28YYt+OKq4vIEmShuCzwL8C7mVQgNIBXGdTkqR6O6zCE/CvgVdl5pMAEbES+Cpg4WkOB9skSVLJNmXm34y6E3WXpjBJkmrrcAtPjdmiU2Ezh78+1ILgWJskSRqCKyLivwJfY7DeJgCZ+dej61K9mMEkSaq3wy08/X1E3Ax8rvj+YuCm4XTp6OX6ApIkqWTvAl7CYDfh2UftErDwNIcZTJKk+jrcxcX/ICLeDLy2aLoqM784vG4dfSLCad6SJKlsr8rMF4+6E3UW4XIHkiTV2eHOeCIzvwB8YYh9OaoFjrZJkqTSfSsizsjM74+6I3UVhBlMkqQaO2ThKSJ2MP8g0qDOkrlsKL06CkVYeJIkSaU7B7grIn7EYI2n2Qz2itF2q0bCxcUlSaqzQxaeMnNpVR05+oWRR5Ikle28UXeg7gJ81k6SpBo77EftdGiDGU+mHkmSVJ7MfGTUfag713iSJKneGqPuwLHCrXwlSZKqF6YwSZJqzcKTJEmSjlrOOpckqd4sPJXExcUlSZJGwwwmSVJ9WXgqSRDuqCJJklSxwDWeJEmqMwtPJXHGkyRJUvUiwgwmSVKNWXgqiTuqSJIkVW8w48kUJklSXVl4KkkQLmwpSZJUNWedS5JUaxaeyuKMJ0mSpMrFqDsgSZIOycJTSQKsPEmSJFVssMaTIUySpLqy8FSSiLDuJEmSVDHX2ZQkqd4sPJUkwNE2SZKkig0y2Kh7IUmSDsbCU0nCBQYkSZIqF4YwSZJqbWiFp4i4OiKejIh757R9KCI2RsRdxdcFc977QESsj4gfRMSb5rSfV7Stj4jL57SfFhG3F+2fj4jOsD7L4XKwTZIkjdpCy2ABpClMkqTaGuaMp88A583T/rHMPLP4ugkgIs4A3ga8rDjnExHRjIgm8HHgfOAM4O3FsQB/UlzrZ4CtwLuH+FmeldO8JUlSTXyGhZTBwgwmSehjg9MAACAASURBVFKdDa3wlJn/AGw5zMMvBK7LzKnM/BGwHji7+FqfmQ9l5jRwHXBhDOZUvwG4oTj/GuCiUj/AczRYXNzUI0mSRmuhZTBwgxdJkupsFGs8vTci7i6mga8o2lYBj845ZkPRdrD2E4Btmdk9oH1eEXFpRKyLiHWbNm0q63Psfw8cbZMkSbVWaQarIn/NMoNJklRfVReePgn8NHAm8BjwH6u4aWZelZlrM3PtypUrh3MTp3lLkqT6qjyDVZK/mN3gxRAmSVJdtaq8WWY+Mfs6Ij4F/G3x7Ubg1DmHri7aOEj7ZmB5RLSKEbe5x49E4I4qkiSpno7tDObgnyRJdVbpjKeIOGXOt78GzO62ciPwtogYi4jTgNOB7wB3AKcXu6d0GCx+eWNmJnAr8BvF+ZcAX6riMxzMYGFLU48kSaqfYz+DjbIHkiTpUIY24ykiPge8HjgxIjYAVwCvj4gzGcyHfhj4bYDMvC8irge+D3SByzKzV1znvcDNQBO4OjPvK27xh8B1EfFh4B+BTw/rsxyOwVa+kiRJo7XwMpgbvEiSVGdDKzxl5tvnaT5oMMnMjwAfmaf9JuCmedofYrDjSi042iZJkurADCZJkupkFLvaHZMcbZMkSaqes84lSao3C08lcbRNkiSpehFhBpMkqcYsPJUk3NROkiRJkiRpPxaeSuRgmyRJUrUicLkDSZJqzMJTaZzmLUmSVLVwkSdJkmrNwlNJBo/amXokSZKqNNjgRZIk1ZWFp5IELi4uSZJUtcEGL4YwSZLqysJTSQbrC0iSJKlKPmknSVK9WXgqSRCOtkmSJI2AEUySpPqy8FQSZzxJkiRVL8I1niRJqjMLTyVxjSdJkqTqDTKYIUySpLqy8FSSCB+1kyRJqpyzziVJqjULTyUy9EiSJFUrwBAmSVKNWXgqSbiliiRJUuUGazwZwiRJqisLT5IkSTpqxag7IEmSDsnCU0kCd1SRJEmqWoQbvEiSVGcWnkoyCD2mHkmSpCoFYeFJkqQas/BUEpd4kiRJql4ErvEkSVKNWXgqidO8JUmSRsMMJklSfVl4Kok7qkiSJFVvkMEkSVJdWXgqSeBomyRJUtXMYJIk1ZuFp7KEazxJkiSNhilMkqS6svBUkrDyJEmSVDnX2ZQkqd4sPJXEHVUkSZKqF479SZJUaxaeSuL6ApIkSdULgjSESZJUWxaeShIx6h5IkiQtPGYwSZLqzcJTiRxrkyRJqlZgBpMkqc4sPJXEad6SJEkjEOFyB5Ik1ZiFp5K4sKUkSVL1nPEkSVK9WXgqiYuLS5IkVS8CZ51LklRjFp7K4sqWkiRJlTOBSZJUb0MrPEXE1RHxZETcO6ft+Ii4JSIeLH5dUbRHRFwZEesj4u6I+Pk551xSHP9gRFwyp/2VEXFPcc6VEaOt/Mze3BE3SZI0Sgsug7nGkyRJtTbMGU+fAc47oO1y4GuZeTrwteJ7gPOB04uvS4FPwiAkAVcArwbOBq6YDUrFMe+Zc96B96rUbOQy+EiSpBH7DAspgwHpKk+SJNXW0ApPmfkPwJYDmi8ErileXwNcNKf92hy4DVgeEacAbwJuycwtmbkVuAU4r3hvWWbeloMpRtfOudZIRDHnydgjSZJGacFlsHDgT5KkOqt6jaeTM/Ox4vXjwMnF61XAo3OO21C0Hap9wzzt84qISyNiXUSs27Rp05F9goPeY/Crj9pJkqQaqjyDVZG/YDD4Z/ySJKm+Rra4eDFKVklMyMyrMnNtZq5duXLlUO6xd42noVxdkiSpHFVlsCry1957mcAkSaqtqgtPTxRTtCl+fbJo3wicOue41UXbodpXz9M+Mq7xJEmSauyYzWD4qJ0kSbVWdeHpRmB2V5RLgC/NaX9nsbPKOcDTxXTwm4FzI2JFsaDlucDNxXvbI+KcYieVd8651kiMeEMXSZKkQzl2M9goby5Jkp5Va1gXjojPAa8HToyIDQx2RvkocH1EvBt4BHhrcfhNwAXAemA38C6AzNwSEX8M3FEc90eZObtY5u8y2LVlAvhy8TVyTvWWJEmjtNAyWAT0jV+SJNXW0ApPmfn2g7z1xnmOTeCyg1znauDqedrXAS8/kj4Og1O9JUnSKC20DBY+aydJUq2NbHHxY41P2kmSJFUvwhnnkiTVmYWnkkSxwoADbpIkSdUJJzxJklRrFp5KsndXO0fcJEmSKhOE6UuSpBqz8FSS2SftHHGTJEmqzmDGkwFMkqS6svBUkn0zniRJklQl85ckSfVl4akk+9Z4MvpIkiRVJSKccS5JUo1ZeCqJM54kSZKqF5i/JEmqMwtPJXPETZIkqToRGMAkSaoxC08lidkpT5IkSZIkSQIsPJXPATdJkqTK+KidJEn1ZuGpJLPzndLoI0mSVBkXF5ckqd4sPJVk7+LiBh9JkqTKDGY8GcAkSaorC08l2TfjSZIkSVWJcOBPkqQ6s/BUktnFxdPkI0mSVCEftZMkqc4sPJWkUUx56ht8JEmSKtMI6Ft5kiSptiw8laTRcMaTJElS1ZoNZzxJklRnFp5K0ioKT12nPEmSJFWm2Qi6/f6ouyFJkg7CwlNJGsUaTz0LT5IkSZVpNsL8JUlSjVl4KkmraeFJkiSpas1G0PNZO0mSasvCU0majcGP0kftJEmSqtOMoNczf0mSVFcWnkrS9FE7SZKkyrWa4cCfJEk1ZuGpJM2GhSdJkqSqNRtB30ftJEmqLQtPJWlZeJIkSapcM5zxJElSnVl4KsnsjCe385UkSapOs9FwjSdJkmrMwlNJZgtPTvWWJEmqTqvprnaSJNWZhaeSzD5q13XETZIkqTINH7WTJKnWLDyVpOEaT5IkSZVrNcL8JUlSjVl4KsnexcWd6i1JklSZRlF4SjOYJEm1ZOGpJI29i4sbeiRJkqrS2rvO5og7IkmS5mXhqSR7Zzy5xpMkSVJl3FlYkqR6s/BUkqaP2kmSJFVu787C1p0kSaqlkRSeIuLhiLgnIu6KiHVF2/ERcUtEPFj8uqJoj4i4MiLWR8TdEfHzc65zSXH8gxFxySg+y6ymi4tLkqSaOxYzWMsZT5Ik1dooZzz9z5l5ZmauLb6/HPhaZp4OfK34HuB84PTi61LgkzAIScAVwKuBs4ErZoPSKLRc40mSJB0djqkM5uCfJEn1VqdH7S4ErileXwNcNKf92hy4DVgeEacAbwJuycwtmbkVuAU4r+pOz2o2Bj/KvqFHkiQdXY7yDGbhSZKkOhtV4SmBr0TEnRFxadF2cmY+Vrx+HDi5eL0KeHTOuRuKtoO1P0NEXBoR6yJi3aZNm8r6DPtphjOeJElS7VWWwarIX2DhSZKkumuN6L6vy8yNEXEScEtEPDD3zczMiCgtPWTmVcBVAGvXrh1KKmk2Zxe2NPRIkqTaqiyDVZG/YN/gnxu8SJJUTyOZ8ZSZG4tfnwS+yGB9gCeK6dsUvz5ZHL4ROHXO6auLtoO1j4RrPEmSpLo7FjPY7Iynbs8MJklSHVVeeIqIxRGxdPY1cC5wL3AjMLsryiXAl4rXNwLvLHZWOQd4upgOfjNwbkSsKBa0PLdoG4nG7GibO6pIkqQaOlYzWKvpo3aSJNXZKB61Oxn4YgwKNS3gv2Xm30fEHcD1EfFu4BHgrcXxNwEXAOuB3cC7ADJzS0T8MXBHcdwfZeaW6j7G/lquLyBJkurtmMxgDR+1kySp1iovPGXmQ8DPzdO+GXjjPO0JXHaQa10NXF12H5+P2TWefNROkiTV0bGawVrFzsIO/kmSVE+j2tXumLN3YUtDjyRJUmVc40mSpHqz8FSSvVv5Os1bkiSpMrMZrG8GkySpliw8lWTvGk+OtkmSJFXGnYUlSao3C08laRp6JEmSKrd31rk7C0uSVEsWnkoSETTCad6SJElV2ld4GnFHJEnSvCw8lajVaDjjSZIkqUL7Zp1beZIkqY4sPJWo0XBXO0mSpCrtm/FkBpMkqY4sPJWo1WgYeiRJkipk4UmSpHqz8FSiRhh6JEmSqtSy8CRJUq1ZeCpRq9lwfQFJkqQKNcKdhSVJqjMLTyVqNsIdVSRJkirUag4KT30LT5Ik1ZKFpxI1I+g540mSJKkyrYYzniRJqjMLTyVqNsLQI0mSVKHZR+1c40mSpHqy8FSiVjOc5i1JklShVmMQZy08SZJUTxaeStQMZzxJkiRVqag7WXiSJKmmLDyVqN1sMOPq4pIkSZXpNAdxdtoMJklSLVl4KtFEp8nu6d6ouyFJkrRgTHSaAOwxg0mSVEsWnkq0eMzCkyRJUpUWdVoA7JrujrgnkiRpPhaeSrSo02LXlKFHkiSpKs1GMN5uOPgnSVJNWXgq0eJOkz0zhh5JkqQqLe602O2MJ0mSasnCU4kmOi12TVl4kiRJqtJEp8luM5gkSbXUGnUHjgn9PjzwN6zpNdg93R51byRJkhaGnU/Cw9/klFabXdPLRt0bSZI0D2c8lSECrr+En3v66+ye7tHv56h7JEmSdOx74j644V2c3vyxazxJklRTFp7KEAGdJSxiEoDJrsFHkiRp6MaWArC8OW3hSZKkmrLwVJaxJUzkHgDXeZIkSapCZzEAxzWn3FlYkqSasvBUls5ixnMw48ldVSRJkipQFJ6WNaac8SRJUk1ZeCpLZzFj/d2AM54kSZIq0VkCwJLGlAN/kiTVlIWnsnSW0ukPHrXbtmd6xJ2RJElaAGYLTzHJ9skuPTd4kSSpdiw8lWViOUu622g1gm8++NSoeyNJknTsa3WgvYg143uY7vb5zo+2jLpHkiTpABaeynLSGbS2/pAXndDih5t2jro3kiRJC8NJZ/DCqfUArDeDSZJUOxaeynLKKyB7vG7pk3z3n7cx1XWdJ0mSpKE75RV0Nt3Lkk5w+0ObR90bSZJ0gKO+8BQR50XEDyJifURcPrKOvPAsAN5y8kY27ZjiD2+4m8/f8c/8wz9tYuuuabbtnubp3TPM9PpkJnvceUWSJB3F6pTBYmo7l728z9/f+zgfu+WfuP6OR7l7wza2T86weecUkzM9Znp9ur0+093+yLoqSdJC1Bp1B45ERDSBjwO/CGwA7oiIGzPz+5V35rjVcNLL+Jl7/pSvnPxKvnbfSu6/Zxnfo811uZitLGUxk2zJpUxFh8lsEyTd6LB6+TitVocGXf55d4dxpuiMjbO4E0y0GjB+HKfMPEo/+zRX/CQ0mrRaLZ7a3WPpzFNs7byQlUvHmJyaYfnSRXSaDfYUAWum1+f4xWP0+n0aEXT7SbvZYKbXZ3Kmx3i7SQDLJtpMz3Q5rjnJThaTmcz0kolOk14/OWnpGDPdGXbummQm2rR7u4j2OE9PBROdJu3uTrb1xgmSn1zeYdFYm21TyVS3z3i7wUS7yVM7p2j0uyybaNHtJbun+yxZNM7O3ZOMd5qMj43R6/UY77RoRLBzqkun2aDdTJIG3V7SagZBMjmTLB5rsbjVY+tkn/F2m5l+sqjdZPvkDBPtJtO9Ppkw3mrQzGn6zTECGG832TPdZbqXHDfRZs9Mj4lGl2Z7nB2TXZqNYOl4i0WdJpt3TvPkjkleuHyCdrNBp9WgU/z8ZnpJr590WsGT26dYNNZiyViTTrNJP3POF+yZ7tGIoBGwaKzF9EyPZjPo9aHb79PtJWOtBmNNaLVadPu5t69Jsnu6x1gzmOr26bSag2s1IAgiIIAIgME9IoJ+Jrunekx0mrQaMee/m31/bIN4Rtus4xd32D3do9dPGgEJ9DPJ3PfrVLfPok6TCGhEMN5qMtUbnNPrJ2OtJu1m0E+Y6vZoNxuQfRqN5t4+B4MPEAH9/uC64+0me2Z6LOo0AWg1gslun3YzyGRfHw7o89yPMfuZgvk/++H8PHr9pNUI8hnXnnPNg11rvpuVaPb35fncJzOH3j9Jx7ZaZbCffiNEk0sf+d945XEv4uvfWMV9jHEHY2zLJexinA4zbM2lzNBiN2NMMM2e9nLWLIVsjdPvzfDYniZLWn2iNcbxY9DqjNFsdzhh90O0l55Ib/wEotkiCbbtnmFRbzv9Zacy3uyTNFi6qEOr0WDXVJfpXp9Wo8HS8VbxOpgp/q2f7PaY6SZj7UGmGG83obuH8VaD3dmh2xv8/d5qNoiAk5aOsXP3HnZNzUCjTUzvoDl+HE9Pdlky1iCndrFpqs0Llo9zfKdHe2wR2ye7THX7LBtvkyRbds2wqNlj0XiHqck9TDLGRKfJzNQk7c4Y7VaTXq/PsokOU91Bhmw3oN1s0gemZvosHW+xc3KaRqPJ4rEW4zHN1ukGY60mjSKD7JrqsqjTZLL4N79Dl2y0SBqMtQfj3bsnu3SK/NlsBtGdotWZYOfUDO1mg+WLOgBs2jHFzqkuL1g2PshfRQbbOdUlYl8W2LxzmmUTLY6baBMM8k8vkywy2M6pLmOtwb0XdVrMdHs0GkG3N8ip7WaDVgPGm0E/BsdFDLJIP5Mde2ZYNt4e5NJWk1YzaMQz89e+74Ppbp9ePxlvN2jEM3PIobJJsxGsWNTh6T0zzMa3fkIy+DyZSbeXREC7+DPSbAStRoNuf5BPG8He35fpXh8SGo0gsk8cJINNzfRZPNYkEyZneiybaO/N3jO9Ps3G4WWw+XLVfJ/z2X4eg5yz7+e67/16ZLBm4/ndwwymheqoLjwBZwPrM/MhgIi4DrgQqD70AFz8V8Q3/5QXPXATL2r+d2ge5nm752nrArvmaX/imU39DHo0IGBXDoo/g7/CZ/9JGPy69x+Evd/v396iSyd6bM8J9v+rfGARk7SiTzcbtKLPTDbZQwcIlsVuprJNjwZN+vQJdjI++IeNLL4G1xiLwXbHu3OMJn3adGlEsi0XM8EUk3Ro0WOSDg2SpexmM8uKa/U5nh1sYwlBspxdTNJhN2O06dKkT5cmPRo0SGZosZg9LI4pHsvjadGjTZcl7GE7i2jRJ4HjYjdP5HJmiv8kZvt8IvCSQ/zMprLNWMzwU3RZxBRbWbL3ZxXAdhaxiClmaLEnOyRBn+CFsZndjLGbcTIHV21E8gK28DjHkzm4w6KYZJo2p7F97+/DFG22s5jl7GQ7i+jQpUGfbbmEPg2SfWFgKX0mYordOSi6NejTiKTD4PdgMtvM0Nrv96gdXbrZ5PHiD3Cfxt73G/Rp0SciyQxaNJjceybsKF43GIwmz9Df789AFzg5tvAkK5gsfh6toi+DvkfxJ3bw69H2wESLLsfFLnblBHvo0KZLnwZdmrToMRYz9GjQzwZ9Bv/d9ouvuZ7x5y32bwcGyQ/2htID/2zubZsNwnOOydx3nXGm2cHiQWGRxt4i2+x1cr/rDX5vouj1vj81wRJ2s4txIgafLWmQEfT6c4/KvX8OI5JmcccuDXo06eeggJlFHwd/iw0C2tzPlXP6M7dtvim8g57G/j+/OOCDMQjRs4XbLH6uecDNDrzfnB/j3v/ZgNzXPnvOPNfY++kO+DCZRdDmmaGe3HepDl1adNnFxOB3bb+g/4zT9v5e5Zx35/7OjDG998/AeO5hdyxi8L+bg8J/zjln9vdy8Ddt0mGaXbFo373n/GCy+J+x2Z9lFt9E9nhy5Wv4Hy/78wM/pY4e9clgx62Cd1xP8/arOPvBr3B2+znsbHdg1uoBU/O0H+QfpKls06bLVLSZKgYVn08GG/w3GOxmjGf+VwzLYhAWZzPYVLaZpE2HLhMxzVS2mKTDUvawkwmmaD0jg62IfetfPZ2LWFzkutnvO3Tp0iSBGVp0ily1gwmi6OM4M+xknAbJitjJ07mIaVosZooZmnRp0qbHFG2SYAU7aEePx/J4OszQpscY0+xhbPZfChYxyWOc8Iyfzwvn+ZkFcGLxU53t4xlM0aLPVpYwxgwTTDFFmyk6TDDFbsaYzva+68ZmtrKUaVpkDnLZREyzlN08lccNjotkMZNM0uElsYU9Ofg3fTbntemxmzHGmaZHk+25qOjd4O/uFrCILu3osic7e//ta0af8eLzd/P/b+9egyU56zqOf3/dc9mzuwm7mwvEBEjCxTJWKWCMUcRCQgEiRXgRy5SIAbWowkuJUiqIl9JXgpaoVZSBAiyQyC0GTFFY3KRi8YIEiCQkXNeEy8bEJGSzl3N2bt1/XzzPnDPn7Nlkb7Nzpuf3qZo908/06e3/PD3dv3m6Z07ai46fhwCWNGAluvzvRM9NZrC2KiKUj8Ral8HG84+Pwr3JYy7QUsU5HGBfnEeVM16HYe7ztIwDrGWwefua/iX16TJgOZbo02Y7/fw+hdVtuVJB5PdNVc6dkxlsMmcdK4NNHruPlcEm88ZkBkvHwpzfCFpUHGZp3fF5s7wzbk97l3Haz/2toBsDekr7jgpRqKAKEVGv/l+TWaxQWsb4vVJFsZo/Ig9ySlodJDtWJuQxpoHV53hdfh3Pr7W4UG/4fyYfG09PGmew8XMyHviOiaCkjVnv8TJYrrsUR69tjH+k3+zSp6JgSAdp/dwbn4d6w2t0s59dBtQ5X7ViyEBdRA35db5x6eN3DyU1LSpWtO2o/DVZ9Lg5JKIOWozYf8UbuOKlr95Y6VTN+8DThcD3J6b3AT+1cSZJrwVeC/CUpzxlemtzztPg6ren2NU/BIPldBsegSOPQDWAagT1EAYrUJTQPwhlB6JO89WjtemyDXWVlrXziekV1DsIUaX2qOCRe9C23bSLEgh2DVaoxm9cJIarV2ykHQn5RV0UaQczqIJOq2BYQykYrOyn29lJXcfq1SUSrAxG9KkZtjoUrTb9zi5agwNsrwcMB31Gyw9QnXURg94RyuoIVRQsdZdolyUVMBgF7VZBPVxhWSX1aEBRFAyLNtW2szkyGFDUIwZFenH01GYpelQBy1HTrtJgVrssOKSSroJhFOzvHURlmyg6DCSGo3RGplX3qcs2LWA4XObgcJnutj2M6qBfbqPX2sZosEJZFLRiwMGD30PbL2BHWTCo0qtzVKezjYOqzme9oAqo6rWrTdoxYIUWS6OD9LtnUwdQV/RHhxgsnU9Z9egXLeoQS3UfgKoa8WhnO7XatOseUdfUEbQUHDzyMNp2HnWdrvJZVhui4gdUDNVBRUFrcAgVLR7u7KIcHuZw0QZEd5DiArDa32VZ0lOHVtUjVACiUsGyWhT1ECLoxDDt+FQQAaO8g6qGQ9ql8uBcAfksYFW00rLqCkWdI2o6BTas6rSLVIGKMv1/6VWYT62J76pNq1qhjICoqdVKB42oV3fDdd6G67ynrGP8xp68tNU7mx4MNzm+TTQ99huSo/bZjzXvhukhcF/rLMpqQKvuMVDaoEXNUCU9tXOddTrrGPmgkYPBZoeu1YPT6oEwTdeRBjqqidd1ndc/Jp6Vow9yUOa+ADhAQadaSf1f1xRFChrjD6JMhoAqQBFEUeSwXEOkoc5H1UJRU0c6xBKBoqJdFIwHrEIiQgw1jkBpcEpRU8QIAaN8UC+Uahyvc+SAsHpmedwe42AoRkd1XuTnexzQJp+b9cZnLsd/in28HhuXNxkp1rbHtK5pu2XdPEEOcmg11IxHkIK0Lx4vehyeBuPCN57V1VrlhymRRFEPc6hZv/3ExD+F1t58pg08h89Ye1Z66qaKY0RVtFCMGFCkbSsmtx5Wt42Byhx7RKfu5TmECuU3RutD9rphr6LF9j0XbNoXNjceN4OdsfwF8PQXplsE9B5NOWtwOGWrlfzXhqthylm9g9BegsMPwtJuGC7DqA9KWQqU/lreILfvuQSO7IfRIP1+VGlZ+++ls3QOanVZGiyzVI+oIihzxhpFOpbXMR6AFQWgoqDKV+OUEqMAqj5VNaJbpDfKZd41pKumRxwpaqLsULSXGHV3wsoj7GDE4MhhqmqZfmsPVANWevupurvZ0Sopy4JhFYxqaLdKeoODKUsMlym7O+kFaGk3vd4RinpIVYpBtCgVbIs+dRQM6xHFqKbdblEF9MuSjsSggkeWH4Qd5xJVzaGiTTXs02mVDOohZZEyR2/lAZY7Z9NROw20tDqslG1Ggz7tAoqqRyzfT3f7kyjzVUjV+M0ooijSra6hivTYsIJuGZT1kJVoofogK93dUI0YVT0OA6POWZQxole0YDRkhyrqnLce6pyNqOnUfaoqnyCLAcuDZYrObqpIx4QDRZey6nN/WdCvC9oFMOpTlx2Wiy5l1eNg2UX1iG3DQwCrWRtAZZt+FLTqwWoGG6nkoFqUVS+9cYxIA/L53WFPJVQj6rqmU5DeD6gglI6lw6KdjgKRMljaTIKIOmfgvJ0V6dRBjRhRpKuwJA6rpDU6Qisfv4dFG9VVWo6grtPb37IgX1WlfHXP2kvtMTPYMTLUyWSw9Ufcx1pmskzBI62dtEfLtGLIYbVpRRrQPaJ2eg5zJlNUKIKCilbEhuWtP0ED6zNYkN4jlEXKHVKx+vt1nfLI0cfktaPo+H3ZuN529BGRT76xuv2P12T1CrAICqBWSYUg55uKYKAWRE3UFYVgWKehxVaRs1ceckrbRLGavyJEEcPV4/Mo/3+FtPppmSqP8qRNe21wbJzBNs3huWV9Blufn8aDTQCtnPnXct/GDHb01jA+QShI+S/W1m/dMrT2XK8myvHA4UQGG2/P/TrYLH8BeZg4OKJOev8UdX4eN1a+9k+pcdYar0TqR3IGC+CIuvn5qhgWZRowVLH6/E5msABGiFol4yHrdj3I7+WUBwzJFwmsFr7aAxIM1OKss3cd1WPTNu8DT8clIt4JvBPg8ssvP4G3laege1a6nQEbd8qTF1q1jzHPuK2b73c2+d1JZ2+Y7k7cb0383H6M3z9W+/E+fiZsrNHMzMxO3kzyl5QGk5Z2n5n/bsN0OdHe3tC2cb5x++NlsM4x2mEtgx0rw3Q3TG/bZJ4dE/d3Psb/ZWZmdrLm/cvF7wOePDF9UW4zMzMzs+lxBjMzM7PjMu8DT18EniHpEkkd4Frg5hmvk5mZmVnTOYOZmZnZcZnrj9pFxEjS7wCfJF2h/J6IuHvGq2VmZmbWaM5giKhp+gAACjVJREFUZmZmdrzmeuAJICI+AXxi1uthZmZmtkicwczMzOx4zPtH7czMzMzMzMzMbIvywJOZmZmZmZmZmU2FB57MzMzMzMzMzGwqPPBkZmZmZmZmZmZT4YEnMzMzMzMzMzObCg88mZmZmZmZmZnZVHjgyczMzMzMzMzMpsIDT2ZmZmZmZmZmNhUeeDIzMzMzMzMzs6lQRMx6Hc4oSQ8B353S4s8FHp7SsrcS19ksi1DnItQIrrNpXOfJe2pEnHeal2mnwPnrtHCdzeI6m8V1NovrPHmbZrCFG3iaJklfiojLZ70e0+Y6m2UR6lyEGsF1No3rNDs+i7INuc5mcZ3N4jqbxXWefv6onZmZmZmZmZmZTYUHnszMzMzMzMzMbCo88HR6vXPWK3CGuM5mWYQ6F6FGcJ1N4zrNjs+ibEOus1lcZ7O4zmZxnaeZv+PJzMzMzMzMzMymwlc8mZmZmZmZmZnZVHjg6TSQ9BJJ35S0V9IbZ70+p0LSkyV9TtLXJN0t6fdy+x5Jn5b07fxzd26XpH/Mtd8p6TmzreDESCol/bekj+fpSyTdmuv5kKRObu/m6b358Ytnud4nQtIuSTdK+oakr0v66Sb2p6Tfz9vsXZI+IGlbE/pT0nskPSjprom2E+4/Sdfl+b8t6bpZ1PJYjlHn3+Tt9k5JH5W0a+KxN+U6vynpxRPtW3p/vFmdE4+9QVJIOjdPN6o/c/vv5j69W9JbJ9rnsj9t9pq0jWiBMpgWIH/BYmQwNTR/gTOYnMHmrj+PVaO2Qv6KCN9O4QaUwP8AlwId4A7gslmv1ynUcwHwnHz/LOBbwGXAW4E35vY3Am/J918K/Acg4Erg1lnXcIL1/gHwr8DH8/SHgWvz/euB1+X7vwVcn+9fC3xo1ut+AjW+F/jNfL8D7GpafwIXAvcCSxP9+Oom9Cfwc8BzgLsm2k6o/4A9wD355+58f/esazuOOl8EtPL9t0zUeVne13aBS/I+uJyH/fFmdeb2JwOfBL4LnNvQ/vx54DNAN0+fP+/96dtsb03bRligDMYC5K+8zo3OYDQ4f+V1dAYLZ7B56s9j9OWWyF++4unUXQHsjYh7ImIAfBC4esbrdNIi4v6IuD3fPwR8nXRQuZp08CT/fEW+fzXwvki+AOySdMEZXu2TIuki4BeBd+VpAS8AbsyzbKxzXP+NwFV5/i1N0hNIO6B3A0TEICIepYH9CbSAJUktYDtwPw3oz4j4L+CRDc0n2n8vBj4dEY9ExH7g08BLpr/2x2+zOiPiUxExypNfAC7K968GPhgR/Yi4F9hL2hdv+f3xMfoT4G3AHwGTX7zYqP4EXgf8dUT08zwP5va57U+buUZtI4uSwRYhf8FCZbBG5i9wBnMGm7/+3Mr5ywNPp+5C4PsT0/ty29zLl78+G7gVeGJE3J8fegB4Yr4/z/X/PWknU+fpc4BHJ3ayk7Ws1pkfP5Dn3+ouAR4C/lnpkvZ3SdpBw/ozIu4D/hb4HinwHAC+TPP6c+xE+28u+3WDXyedeYKG1SnpauC+iLhjw0ONqhN4JvC8/PGKWyT9ZG5vWp125jR2G2l4BluE/AULkMEWMH+BM1ij6lyQDLYl8pcHnmxTknYC/wa8PiIOTj4WEcH6EeG5I+llwIMR8eVZr8uUtUiXW/5TRDwbWCZdFryqIf25mzQSfwnwQ8AOttDZh2lqQv89HklvBkbADbNel9NN0nbgT4A/n/W6nAEt0qXpVwJ/CHx4K5/pNpuVJmewBcpfsAAZbJHzF8x//x0PZ7BG2BL5ywNPp+4+0udCxy7KbXNLUpsUeG6IiJty8/+NL/fNP8eX6M1r/c8FXi7pO6TLB18A/APpMspWnmeyltU68+NPAH5wJlf4JO0D9kXErXn6RlIIalp/vhC4NyIeioghcBOpj5vWn2Mn2n/z2q9IejXwMuCVOeBBs+p8Gimw35H3RxcBt0t6Es2qE9L+6KZ82fptpKsdzqV5ddqZ07htZAEy2KLkL1iMDLZo+QucwZpU56JksC2RvzzwdOq+CDxD6a83dEhflHfzjNfppOXRz3cDX4+Iv5t46GZg/K391wH/PtH+a/mb/68EDkxcfrplRcSbIuKiiLiY1Gf/GRGvBD4HXJNn21jnuP5r8vxb/gxHRDwAfF/SD+emq4Cv0bD+JF3ifaWk7XkbHtfZqP6ccKL990ngRZJ257OTL8ptW5qkl5A+jvHyiFiZeOhm4Fqlv45zCfAM4DbmcH8cEV+NiPMj4uK8P9pH+nLhB2hYfwIfI33BJZKeSfrCyodpUH/aGdeobWQRMtii5C9YmAy2aPkLnMEac8xeoAy2NfJXbIFvX5/3G+lb779F+vb3N896fU6xlp8lXTJ6J/CVfHsp6fPXnwW+TfpW/D15fgFvz7V/Fbh81jWcRM3PZ+2vqlyaX3B7gY+w9u3/2/L03vz4pbNe7xOo71nAl3Kffoz0Fxga15/AXwLfAO4C/oX0Fxrmvj+BD5C+N2FIOiD+xsn0H+nz+Xvz7TWzrus469xL+oz5eF90/cT8b851fhP4hYn2Lb0/3qzODY9/h7W/qNK0/uwA78+v0duBF8x7f/o2+1uTthEWLIPR8PyV17/xGYyG5q+8vs5gzmBz1Z/H6Mstkb+UF2xmZmZmZmZmZnZa+aN2ZmZmZmZmZmY2FR54MjMzMzMzMzOzqfDAk5mZmZmZmZmZTYUHnszMzMzMzMzMbCo88GRmZmZmZmZmZlPhgSczW3iSni/p47NeDzMzM7NF4fxltjg88GRmZmZmZmZmZlPhgSczmxuSflXSbZK+IukdkkpJhyW9TdLdkj4r6bw877MkfUHSnZI+Kml3bn+6pM9IukPS7ZKelhe/U9KNkr4h6QZJmlmhZmZmZluE85eZnSoPPJnZXJD0I8AvA8+NiGcBFfBKYAfwpYj4UeAW4C/yr7wP+OOI+DHgqxPtNwBvj4gfB34GuD+3Pxt4PXAZcCnw3KkXZWZmZraFOX+Z2enQmvUKmJkdp6uAnwC+mE+GLQEPAjXwoTzP+4GbJD0B2BURt+T29wIfkXQWcGFEfBQgInoAeXm3RcS+PP0V4GLg89Mvy8zMzGzLcv4ys1PmgSczmxcC3hsRb1rXKP3ZhvniJJffn7hf4f2jmZmZmfOXmZ0yf9TOzObFZ4FrJJ0PIGmPpKeS9mPX5Hl+Bfh8RBwA9kt6Xm5/FXBLRBwC9kl6RV5GV9L2M1qFmZmZ2fxw/jKzU+YRZTObCxHxNUl/CnxKUgEMgd8GloEr8mMPkr6HAOA64PocbO4BXpPbXwW8Q9Jf5WX80hksw8zMzGxuOH+Z2emgiJO9KtLMbPYkHY6InbNeDzMzM7NF4fxlZifCH7UzMzMzMzMzM7Op8BVPZmZmZmZmZmY2Fb7iyczMzMzMzMzMpsIDT2ZmZmZmZmZmNhUeeDIzMzMzMzMzs6nwwJOZmZmZmZmZmU2FB57MzMzMzMzMzGwqPPBkZmZmZmZmZmZT8f+3ah0M8YquCQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpoEor58Hriw",
        "outputId": "a13ebf0b-1d09-40a4-a2b5-09602ede2f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 6ms/step - loss: 0.0000e+00 - mse: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0, 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(model.predict(X_test)-y_test.reshape(-1,1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGRm41Z-H1Qr",
        "outputId": "4d35ab06-4307-4bfb-90b7-eb7729bd7a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2880181219063553"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ax.plot(trX, model.predict(X_test), color=\"red\")"
      ],
      "metadata": {
        "id": "W1QQdlpMJpZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}